{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import ppi_py as ppi\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to simulate data\n",
    "def simulate_ols_data(beta, ppi_corr, bias, n, N):\n",
    "    \"\"\"\n",
    "    Simulate data for the OLS example\n",
    "\n",
    "    Args:\n",
    "        beta (ndarray): regression coefficients, shape d + 1\n",
    "        ppi_corr (float): PPI correlation\n",
    "        bias (float): bias to appl to all coefficients\n",
    "        n (int): number of labeled samples\n",
    "        N (int): number of unlabeled samples\n",
    "\n",
    "    Returns:\n",
    "        X (ndarray): covariates for labeled data , shape n x (d + 1)\n",
    "        Y (ndarray): labels, shape n\n",
    "        Yhat (ndarray): predictions on labeled data, shape n\n",
    "        X_unlabeled (ndarray): covariates for unlabeled data, shape N x (d + 1)\n",
    "        Yhat_unlabeled (ndarray): predictions on unlabeled data set shape N\n",
    "    \"\"\"\n",
    "\n",
    "    d = len(beta) - 1\n",
    "\n",
    "    X = np.random.normal(size=(n, d+1))/(d+1)**0.5\n",
    "    X[:, 0] = 1\n",
    "\n",
    "    X_unlabeled = np.random.normal(size=(N, d+1))/(d+1)**0.5\n",
    "    X_unlabeled[:, 0] = 1\n",
    "\n",
    "    max_ppi_corr = 1/(2*bias**2+1)**0.5\n",
    "    max_bias = (0.5*((1/ppi_corr)**2 - 1))**0.5\n",
    "    assert ppi_corr <= max_ppi_corr, f\"ppi_corr must be less than {max_ppi_corr} or bias must be less than {max_bias}\"\n",
    "\n",
    "    rho = ppi_corr * (2*bias**2+1)**0.5\n",
    "\n",
    "    Z = np.random.normal(size=n)\n",
    "    Y = X @ beta + rho**0.5 * Z + (1 - rho) ** 0.5 * np.random.normal(size=n)\n",
    "    Yhat = X @ (beta + bias) + rho**0.5 * Z + (1 - rho) ** 0.5 * np.random.normal(size=n)\n",
    "    Yhat_unlabeled = X_unlabeled @ (beta + bias) + np.random.normal(size=N)\n",
    "\n",
    "    return X, Y, Yhat, X_unlabeled, Yhat_unlabeled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true intercept and slope\n",
    "param = 1.\n",
    "beta = np.array([param, param])\n",
    "\n",
    "# bias \n",
    "bias = 0.1\n",
    "\n",
    "# PPI correlation\n",
    "ppi_corr = 0.9\n",
    "\n",
    "# sample size human subjects\n",
    "ns = [1000]\n",
    "\n",
    "# multiples of human subjects sample size\n",
    "ks = list([0.1, 0.25, 0.5, 0.75]) + list(np.arange(1, 10.5, 0.5))\n",
    "\n",
    "# sample size silicon subjects \n",
    "Ns = [int(n * k) for n in ns for k in ks]\n",
    "\n",
    "# number of repetitions for combinations of n and N\n",
    "reps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation over N\n",
    "\n",
    "In this simulation, we examine how bias, standard error, coverage, and RMSE vary as a function of the number of silicon subjects $N$. We hold the number of human subjects and the PPI correlation fixed at $n=1000$ and $\\tilde \\rho = 0.5$, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 1 out of 1000\n",
      "Repetition 25 out of 1000\n",
      "Repetition 50 out of 1000\n",
      "Repetition 75 out of 1000\n",
      "Repetition 100 out of 1000\n",
      "Repetition 125 out of 1000\n",
      "Repetition 150 out of 1000\n",
      "Repetition 175 out of 1000\n",
      "Repetition 200 out of 1000\n",
      "Repetition 225 out of 1000\n",
      "Repetition 250 out of 1000\n",
      "Repetition 275 out of 1000\n",
      "Repetition 300 out of 1000\n",
      "Repetition 325 out of 1000\n",
      "Repetition 350 out of 1000\n",
      "Repetition 375 out of 1000\n",
      "Repetition 400 out of 1000\n",
      "Repetition 425 out of 1000\n",
      "Repetition 450 out of 1000\n",
      "Repetition 475 out of 1000\n",
      "Repetition 500 out of 1000\n",
      "Repetition 525 out of 1000\n",
      "Repetition 550 out of 1000\n",
      "Repetition 575 out of 1000\n",
      "Repetition 600 out of 1000\n",
      "Repetition 625 out of 1000\n",
      "Repetition 650 out of 1000\n",
      "Repetition 675 out of 1000\n",
      "Repetition 700 out of 1000\n",
      "Repetition 725 out of 1000\n",
      "Repetition 750 out of 1000\n",
      "Repetition 775 out of 1000\n",
      "Repetition 800 out of 1000\n",
      "Repetition 825 out of 1000\n",
      "Repetition 850 out of 1000\n",
      "Repetition 875 out of 1000\n",
      "Repetition 900 out of 1000\n",
      "Repetition 925 out of 1000\n",
      "Repetition 950 out of 1000\n",
      "Repetition 975 out of 1000\n",
      "Repetition 1000 out of 1000\n"
     ]
    }
   ],
   "source": [
    "# initialize list to collect results\n",
    "results = []\n",
    "\n",
    "# loop over repetitions\n",
    "for r in range(reps):\n",
    "\n",
    "  # keep track of progress\n",
    "  if (r+1) % 25 == 0 or r == 0:\n",
    "    print(f\"Repetition {r+1} out of {reps}\")\n",
    "\n",
    "  # loop over the combinations of n and N\n",
    "  for n in ns:      \n",
    "\n",
    "    for N in Ns:\n",
    "      # simulate data\n",
    "      X, Y, Yhat, X_unlabeled, Yhat_unlabeled = simulate_ols_data(beta, ppi_corr, bias, n, N)\n",
    "\n",
    "      # get silicon sampling estimates\n",
    "      mod_sil = sm.regression.linear_model.OLS(Yhat_unlabeled, X_unlabeled).fit()\n",
    "      beta_sil = mod_sil.params[1]\n",
    "      se_sil = mod_sil.bse[1]\n",
    "\n",
    "      beta_ci_sil = mod_sil.conf_int(alpha=0.05)[1]\n",
    "      lower_sil = beta_ci_sil[0]\n",
    "      upper_sil = beta_ci_sil[1]\n",
    "\n",
    "      # get coefficient estimate from mixed-subjects PPI\n",
    "      mod_ppi = ppi.ppi_ols_pointestimate(\n",
    "        X, \n",
    "        Y, \n",
    "        Yhat, \n",
    "        X_unlabeled, \n",
    "        Yhat_unlabeled)\n",
    "      beta_ppi = mod_ppi[1]\n",
    "\n",
    "      # compute PPI standard error from CIs\n",
    "      beta_ci_ppi = ppi.ppi_ols_ci(X=X, Y=Y, Yhat=Yhat, X_unlabeled=X_unlabeled, Yhat_unlabeled=Yhat_unlabeled, alpha=0.05)\n",
    "      lower_ppi = beta_ci_ppi[0][1]\n",
    "      upper_ppi = beta_ci_ppi[1][1]\n",
    "      se_ppi = (upper_ppi - lower_ppi) / (2 * 1.96)\n",
    "\n",
    "      # report whether the true value is within the confidence interval\n",
    "      ci_ppi_covers_param = int(lower_ppi <= param <= upper_ppi)\n",
    "      ci_sil_covers_param = int(lower_sil <= param <= upper_sil)\n",
    "\n",
    "      # append the results to the list as a dictionary\n",
    "      results.append({\n",
    "          'beta_ppi': beta_ppi,\n",
    "          'se_ppi': se_ppi,\n",
    "          'lower_ppi': lower_ppi,\n",
    "          'upper_ppi': upper_ppi,\n",
    "          'coverage_ppi': ci_ppi_covers_param,\n",
    "          'beta_sil': beta_sil,\n",
    "          'se_sil': se_sil,\n",
    "          'lower_sil': lower_sil,\n",
    "          'upper_sil': upper_sil,\n",
    "          'coverage_sil': ci_sil_covers_param,\n",
    "          'n': n,\n",
    "          'N': N,\n",
    "          'param': param,\n",
    "          'bias': bias,\n",
    "          'ppi_corr': ppi_corr,\n",
    "          'rep': r+1\n",
    "      })\n",
    "  \n",
    "# convert list of dictionaries to a pandas DataFrame\n",
    "result_df = pd.DataFrame(results).sort_values(by=['rep','n','N','ppi_corr'], ascending=True)\n",
    "result_df['repetitions'] = reps\n",
    "\n",
    "# Group by 'n' and 'N', then calculate mean across repetitions\n",
    "stats = ['beta_ppi','se_ppi','lower_ppi','upper_ppi','coverage_ppi',\n",
    "         'beta_sil','se_sil','lower_sil','upper_sil','coverage_sil']\n",
    "df = result_df.groupby(['n','N','ppi_corr','repetitions'])[stats].mean().reset_index()\n",
    "\n",
    "# Calculate bias columns\n",
    "df['bias_ppi'] = df['beta_ppi'] - 1\n",
    "df['bias_sil'] = df['beta_sil'] - 1\n",
    "df['rmse_ppi'] = np.sqrt(df['bias_ppi']**2 + df['se_ppi']**2)\n",
    "df['rmse_sil'] = np.sqrt(df['bias_sil']**2 + df['se_sil']**2)\n",
    "\n",
    "# Save averaged simulation results to compressed csv file\n",
    "df.to_csv(\"../Data/simulation_study_N.csv.gz\", compression=\"gzip\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
