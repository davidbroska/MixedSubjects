{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import ppi_py as ppi\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to simulate data\n",
    "def simulate_ols_data(beta, ppi_corr, bias, n, N):\n",
    "    \"\"\"\n",
    "    Simulate data for the OLS example\n",
    "\n",
    "    Args:\n",
    "        beta (ndarray): regression coefficients, shape d + 1\n",
    "        ppi_corr (float): PPI correlation\n",
    "        bias (float): bias to appl to all coefficients\n",
    "        n (int): number of labeled samples\n",
    "        N (int): number of unlabeled samples\n",
    "\n",
    "    Returns:\n",
    "        X (ndarray): covariates for labeled data , shape n x (d + 1)\n",
    "        Y (ndarray): labels, shape n\n",
    "        Yhat (ndarray): predictions on labeled data, shape n\n",
    "        X_unlabeled (ndarray): covariates for unlabeled data, shape N x (d + 1)\n",
    "        Yhat_unlabeled (ndarray): predictions on unlabeled data set shape N\n",
    "    \"\"\"\n",
    "\n",
    "    d = len(beta) - 1\n",
    "\n",
    "    X = np.random.normal(size=(n, d+1))/(d+1)**0.5\n",
    "    X[:, 0] = 1\n",
    "\n",
    "    X_unlabeled = np.random.normal(size=(N, d+1))/(d+1)**0.5\n",
    "    X_unlabeled[:, 0] = 1\n",
    "\n",
    "    max_ppi_corr = 1/(2*bias**2+1)**0.5\n",
    "    max_bias = (0.5*((1/ppi_corr)**2 - 1))**0.5\n",
    "    assert ppi_corr <= max_ppi_corr, f\"ppi_corr must be less than {max_ppi_corr} or bias must be less than {max_bias}\"\n",
    "\n",
    "    rho = ppi_corr * (2*bias**2+1)**0.5\n",
    "\n",
    "    Z = np.random.normal(size=n)\n",
    "    Y = X @ beta + rho**0.5 * Z + (1 - rho) ** 0.5 * np.random.normal(size=n)\n",
    "    Yhat = X @ (beta + bias) + rho**0.5 * Z + (1 - rho) ** 0.5 * np.random.normal(size=n)\n",
    "    Yhat_unlabeled = X_unlabeled @ (beta + bias) + np.random.normal(size=N)\n",
    "\n",
    "    return X, Y, Yhat, X_unlabeled, Yhat_unlabeled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true intercept and slope\n",
    "param = 1.\n",
    "beta = np.array([param, param])\n",
    "\n",
    "# bias \n",
    "bias = 0.1\n",
    "\n",
    "# PPI correlation\n",
    "ppi_corr = 0.5\n",
    "\n",
    "# sample size human subjects\n",
    "ns = [1000]\n",
    "\n",
    "# multiples of human subjects sample size\n",
    "ks = list([0.25, 0.5, 0.75]) + list(np.arange(1, 10.5, 0.5))\n",
    "\n",
    "# sample size silicon subjects \n",
    "Ns = [int(n * k) for n in ns for k in ks]\n",
    "\n",
    "# number of repetitions for combinations of n and N\n",
    "reps = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation over N\n",
    "\n",
    "In this simulation, we examine how bias, standard error, coverage, and RMSE vary as a function of the number of silicon subjects $N$. We hold the number of human subjects and the PPI correlation fixed at $n=1000$ and $\\tilde \\rho = 0.5$, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list to collect results\n",
    "results = []\n",
    "\n",
    "# loop over repetitions\n",
    "for r in range(reps):\n",
    "\n",
    "  # keep track of progress\n",
    "  if (r+1) % 25 == 0 or r == 0:\n",
    "    print(f\"Repetition {r+1} out of {reps}\")\n",
    "\n",
    "  # loop over the combinations of n and N\n",
    "  for n in ns:      \n",
    "\n",
    "    for N in Ns:\n",
    "      # simulate data\n",
    "      X, Y, Yhat, X_unlabeled, Yhat_unlabeled = simulate_ols_data(beta, ppi_corr, bias, n, N)\n",
    "\n",
    "      # get silicon sampling estimates\n",
    "      mod_sil = sm.regression.linear_model.OLS(Yhat_unlabeled, X_unlabeled).fit()\n",
    "      beta_sil = mod_sil.params[1]\n",
    "      se_sil = mod_sil.bse[1]\n",
    "\n",
    "      beta_ci_sil = mod_sil.conf_int(alpha=0.05)[1]\n",
    "      lower_sil = beta_ci_sil[0]\n",
    "      upper_sil = beta_ci_sil[1]\n",
    "\n",
    "      # get coefficient estimate from mixed-subjects PPI\n",
    "      mod_ppi = ppi.ppi_ols_pointestimate(\n",
    "        X, \n",
    "        Y, \n",
    "        Yhat, \n",
    "        X_unlabeled, \n",
    "        Yhat_unlabeled)\n",
    "      beta_ppi = mod_ppi[1]\n",
    "\n",
    "      # compute PPI standard error from CIs\n",
    "      beta_ci_ppi = ppi.ppi_ols_ci(X=X, Y=Y, Yhat=Yhat, X_unlabeled=X_unlabeled, Yhat_unlabeled=Yhat_unlabeled, alpha=0.05)\n",
    "      lower_ppi = beta_ci_ppi[0][1]\n",
    "      upper_ppi = beta_ci_ppi[1][1]\n",
    "      se_ppi = (upper_ppi - lower_ppi) / (2 * 1.96)\n",
    "\n",
    "      # report whether the true value is within the confidence interval\n",
    "      ci_ppi_covers_param = int(lower_ppi <= param <= upper_ppi)\n",
    "      ci_sil_covers_param = int(lower_sil <= param <= upper_sil)\n",
    "\n",
    "      # append the results to the list as a dictionary\n",
    "      results.append({\n",
    "          'beta_ppi': beta_ppi,\n",
    "          'se_ppi': se_ppi,\n",
    "          'lower_ppi': lower_ppi,\n",
    "          'upper_ppi': upper_ppi,\n",
    "          'coverage_ppi': ci_ppi_covers_param,\n",
    "          'beta_sil': beta_sil,\n",
    "          'se_sil': se_sil,\n",
    "          'lower_sil': lower_sil,\n",
    "          'upper_sil': upper_sil,\n",
    "          'coverage_sil': ci_sil_covers_param,\n",
    "          'n': n,\n",
    "          'N': N,\n",
    "          'param': param,\n",
    "          'bias': bias,\n",
    "          'ppi_corr': ppi_corr,\n",
    "          'rep': r+1\n",
    "      })\n",
    "  \n",
    "# convert list of dictionaries to a pandas DataFrame\n",
    "result_df = pd.DataFrame(results).sort_values(by=['rep','n','N','ppi_corr'], ascending=True)\n",
    "result_df['repetitions'] = reps\n",
    "\n",
    "# Group by 'n' and 'N', then calculate mean across repetitions\n",
    "stats = ['beta_ppi','se_ppi','lower_ppi','upper_ppi','coverage_ppi',\n",
    "         'beta_sil','se_sil','lower_sil','upper_sil','coverage_sil']\n",
    "df = result_df.groupby(['n','N','ppi_corr','repetitions'])[stats].mean().reset_index()\n",
    "\n",
    "# Calculate bias columns\n",
    "df['bias_ppi'] = df['beta_ppi'] - 1\n",
    "df['bias_sil'] = df['beta_sil'] - 1\n",
    "df['rmse_ppi'] = np.sqrt(df['bias_ppi']**2 + df['se_ppi']**2)\n",
    "df['rmse_sil'] = np.sqrt(df['bias_sil']**2 + df['se_sil']**2)\n",
    "\n",
    "# Save averaged simulation results to compressed csv file\n",
    "df.to_csv(\"../Data/simulation_study_N.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation over PPI Correlation\n",
    "\n",
    "In this simulation, we examine how bias, standard error, coverage, and RMSE vary as a function of the PPI correlation $\\tilde \\rho$. We hold the number of human subjects and the number of silicon subjects fixed at $n=1000$ and $N=1000$, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list to collect results\n",
    "results = []\n",
    "\n",
    "# fix sample sizes\n",
    "n = 1000 \n",
    "N = 1000\n",
    "\n",
    "# PPI correlation values\n",
    "rhos = list(np.arange(0, 1., 0.05)) + list([0.99])\n",
    "\n",
    "# loop over repetitions\n",
    "for r in range(reps):\n",
    "\n",
    "  # keep track of progress\n",
    "  if (r+1) % 25 == 0 or r == 0:\n",
    "    print(f\"Repetition {r+1} out of {reps}\")\n",
    "\n",
    "  # loop values of the PPI correlation\n",
    "  for rho in rhos:      \n",
    "\n",
    "    # simulate data\n",
    "    X, Y, Yhat, X_unlabeled, Yhat_unlabeled = simulate_ols_data(beta, rho, bias, n, N)\n",
    "\n",
    "    # get silicon sampling estimates\n",
    "    mod_sil = sm.regression.linear_model.OLS(Yhat_unlabeled, X_unlabeled).fit()\n",
    "    beta_sil = mod_sil.params[1]\n",
    "    se_sil = mod_sil.bse[1]\n",
    "\n",
    "    beta_ci_sil = mod_sil.conf_int(alpha=0.05)[1]\n",
    "    lower_sil = beta_ci_sil[0]\n",
    "    upper_sil = beta_ci_sil[1]\n",
    "\n",
    "    # get coefficient estimate from mixed-subjects PPI\n",
    "    mod_ppi = ppi.ppi_ols_pointestimate(\n",
    "      X, \n",
    "      Y, \n",
    "      Yhat, \n",
    "      X_unlabeled, \n",
    "      Yhat_unlabeled)\n",
    "    beta_ppi = mod_ppi[1]\n",
    "\n",
    "    # compute PPI standard error from CIs\n",
    "    beta_ci_ppi = ppi.ppi_ols_ci(X=X, Y=Y, Yhat=Yhat, X_unlabeled=X_unlabeled, Yhat_unlabeled=Yhat_unlabeled, alpha=0.05)\n",
    "    lower_ppi = beta_ci_ppi[0][1]\n",
    "    upper_ppi = beta_ci_ppi[1][1]\n",
    "    se_ppi = (upper_ppi - lower_ppi) / (2 * 1.96)\n",
    "\n",
    "    # report whether the true value is within the confidence interval\n",
    "    ci_ppi_covers_param = int(lower_ppi <= param <= upper_ppi)\n",
    "    ci_sil_covers_param = int(lower_sil <= param <= upper_sil)\n",
    "\n",
    "    # append the results to the list as a dictionary\n",
    "    results.append({\n",
    "        'beta_ppi': beta_ppi,\n",
    "        'se_ppi': se_ppi,\n",
    "        'lower_ppi': lower_ppi,\n",
    "        'upper_ppi': upper_ppi,\n",
    "        'coverage_ppi': ci_ppi_covers_param,\n",
    "        'beta_sil': beta_sil,\n",
    "        'se_sil': se_sil,\n",
    "        'lower_sil': lower_sil,\n",
    "        'upper_sil': upper_sil,\n",
    "        'coverage_sil': ci_sil_covers_param,\n",
    "        'n': n,\n",
    "        'N': N,\n",
    "        'param': param,\n",
    "        'bias': bias,\n",
    "        'ppi_corr': rho,\n",
    "        'rep': r+1\n",
    "    })\n",
    "  \n",
    "# convert list of dictionaries to a pandas DataFrame\n",
    "result_df = pd.DataFrame(results).sort_values(by=['rep','n','N','ppi_corr'], ascending=True)\n",
    "result_df['repetitions'] = reps\n",
    "\n",
    "# Group by 'n' and 'N', then calculate mean across repetitions\n",
    "stats = ['beta_ppi','se_ppi','lower_ppi','upper_ppi','coverage_ppi',\n",
    "         'beta_sil','se_sil','lower_sil','upper_sil','coverage_sil']\n",
    "df = result_df.groupby(['n','N','ppi_corr','repetitions'])[stats].mean().reset_index()\n",
    "\n",
    "# Calculate bias columns\n",
    "df['bias_ppi'] = df['beta_ppi'] - 1\n",
    "df['bias_sil'] = df['beta_sil'] - 1\n",
    "df['rmse_ppi'] = np.sqrt(df['bias_ppi']**2 + df['se_ppi']**2)\n",
    "df['rmse_sil'] = np.sqrt(df['bias_sil']**2 + df['se_sil']**2)\n",
    "\n",
    "# Save averaged simulation results to compressed csv file\n",
    "df.to_csv(\"../Data/simulation_study_rho.csv.gz\", compression=\"gzip\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
