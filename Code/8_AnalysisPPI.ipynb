{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/md/0h30crhs7xb714g8zy7dzj1m0000gn/T/ipykernel_26094/781864472.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "from ppi_py import ppi_ols_ci, classical_ols_ci, ppi_ols_pointestimate\n",
    "\n",
    "df = pd.read_csv(\"../Data/5_SurveySampleLLM.csv.gz\")\n",
    "\n",
    "Covs = ['PedPed', 'Barrier', 'CrossingSignal', 'NumberOfCharacters',\n",
    "        'DiffNumberOFCharacters', 'LeftHand', 'Man', 'Woman', 'Pregnant',\n",
    "        'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
    "        'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive',\n",
    "        'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor',\n",
    "        'MaleDoctor', 'Dog', 'Cat', \n",
    "        'Intervention'\n",
    "        ]\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAs Saved:  0\n",
      "Number of NAs gpt4turbo_wp_Saved:  0\n",
      "Number of NAs gpt4o_wp_Saved:  6\n",
      "Number of NAs gpt35turbo0125_wp_Saved:  2\n"
     ]
    }
   ],
   "source": [
    "# very few missing predicted values for the dependent variable\n",
    "print(\"Number of NAs Saved: \",df[\"Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt4turbo_wp_Saved: \",df[\"gpt4turbo_wp_Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt4o_wp_Saved: \",df[\"gpt4o_wp_Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt35turbo0125_wp_Saved: \",df[\"gpt35turbo0125_wp_Saved\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate weights for conjoint experiment\n",
    "def CalcTheoreticalInt(r):\n",
    "    # this function is applied to each row (r)\n",
    "    if r[\"Intervention\"]==0:\n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: p = 0.48\n",
    "            else: p = 0.32\n",
    "            \n",
    "            if r[\"CrossingSignal\"]==0:   p = p * 0.48\n",
    "            elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "            else: p = p * 0.32\n",
    "        else: p = 0.2\n",
    "\n",
    "    else: \n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: \n",
    "                p = 0.48\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.32\n",
    "                else: p = p * 0.2\n",
    "            else: \n",
    "                p = 0.2\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "                else: p = p * 0.32\n",
    "        else: p = 0.32  \n",
    "    \n",
    "    return(p)  \n",
    "        \n",
    "def calcWeightsTheoretical(profiles):\n",
    "    \n",
    "    p = profiles.apply(CalcTheoreticalInt, axis=1)\n",
    "\n",
    "    weight = 1/p \n",
    "\n",
    "    return(weight) \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate amce for intervention\n",
    "def compute_amce(data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        data.loc[:,\"weights\"] = calcWeightsTheoretical(data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        X = dd[\"Intervention\"]\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]==0) & (data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        X = dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        X = 1 - X\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]!=0) & (data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        X = dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        X = 2 - X \n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Utilitarian\") & (data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        X = (dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Species\") & (data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        X = (dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Gender\") & (data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        X = (dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Fitness\") & (data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        X = (dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Age\") & (data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        X = (dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Social Status\") & (data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        X = (dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "\n",
    "    # fit model and extract estimates\n",
    "    fit = model.fit(cov_type = 'cluster', cov_kwds = {'groups': dd[\"UserID\"]})\n",
    "    coef = fit.params[x]\n",
    "    ci = fit.conf_int(alpha=alpha).loc[x]\n",
    "\n",
    "    # store results\n",
    "    res = pd.DataFrame({\n",
    "        'x': [x],\n",
    "        'y': [y],\n",
    "        'pointest_pooled': [coef],\n",
    "        'conf_low_pooled': [ci[0]],\n",
    "        'conf_high_pooled': [ci[1]]\n",
    "    })\n",
    "\n",
    "    return(res)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate amce with ppi \n",
    "def compute_amce_ppi(n_data, N_data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        n_data.loc[:,\"weights\"] = calcWeightsTheoretical(n_data)\n",
    "        N_data.loc[:,\"weights\"] = calcWeightsTheoretical(N_data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data.dropna(subset=y)\n",
    "        N_dd = N_data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        n_X = n_dd[\"Intervention\"]               \n",
    "        N_X = N_dd[\"Intervention\"]\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]==0) & (n_data[\"PedPed\"]==0), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]==0) & (N_data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        n_X = n_dd[\"Barrier\"]\n",
    "        N_X = N_dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        n_X = 1 - n_X\n",
    "        N_X = 1 - N_X\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]!=0) & (n_data[\"PedPed\"]==1), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]!=0) & (N_data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        n_X = n_dd[\"CrossingSignal\"]\n",
    "        N_X = N_dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        n_X = 2 - n_X \n",
    "        N_X = 2 - N_X \n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "    \n",
    "\n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Utilitarian\") & (n_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Utilitarian\") & (N_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        n_X = (n_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Species\") & (n_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Species\") & (N_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        n_X = (n_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Gender\") & (n_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Gender\") & (N_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        n_X = (n_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Fitness\") & (n_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Fitness\") & (N_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        n_X = (n_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Age\") & (n_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Age\") & (N_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        n_X = (n_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Social Status\") & (n_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Social Status\") & (N_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        n_X = (n_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # calculate point estimate\n",
    "    pointest_ppi = ppi_ols_pointestimate(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                         X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                         w=n_weights, w_unlabeled=N_weights)\n",
    "\n",
    "    # calculate PPI confidence intervals\n",
    "    lower_CI_ppi, upper_CI_ppi = ppi_ols_ci(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                            X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                            w=n_weights, w_unlabeled=N_weights, alpha=alpha)\n",
    "\n",
    "    # calculate OLS confidence intervals\n",
    "    lower_CI_ols, upper_CI_ols = classical_ols_ci(X=n_X, Y=n_Y_human, w=n_weights, alpha=alpha)\n",
    "\n",
    "    # create and return the output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        \"y\": y,                              # Dependent variable (Saved)\n",
    "        \"x\": x,                              # Predictor variable (scenario attribute)\n",
    "        \"pointest_ppi\": pointest_ppi[1],     # PPI point estimate\n",
    "        \"conf_low_ppi\": lower_CI_ppi[1],     # The lower bound of the PPI confidence interval\n",
    "        \"conf_high_ppi\": upper_CI_ppi[1],    # The upper bound of the PPI confidence interval\n",
    "        \"conf_low_ols\": lower_CI_ols[1],     # The lower bound of the OLS confidence interval\n",
    "        \"conf_high_ols\": upper_CI_ols[1]},   # The upper bound of the OLS confidence interval\n",
    "        index=[0])\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>pointest_pooled</th>\n",
       "      <th>conf_low_pooled</th>\n",
       "      <th>conf_high_pooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intervention</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.068216</td>\n",
       "      <td>0.052464</td>\n",
       "      <td>0.083969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrier</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.164845</td>\n",
       "      <td>0.136728</td>\n",
       "      <td>0.192962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gender</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.159646</td>\n",
       "      <td>0.126552</td>\n",
       "      <td>0.192741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fitness</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.120809</td>\n",
       "      <td>0.085368</td>\n",
       "      <td>0.156249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Social Status</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.170991</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>0.262948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.336349</td>\n",
       "      <td>0.306916</td>\n",
       "      <td>0.365781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.481846</td>\n",
       "      <td>0.450556</td>\n",
       "      <td>0.513135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.573304</td>\n",
       "      <td>0.545087</td>\n",
       "      <td>0.601520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Species</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.674794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x      y  pointest_pooled  conf_low_pooled  conf_high_pooled\n",
       "0    Intervention  Saved         0.068216         0.052464          0.083969\n",
       "0         Barrier  Saved         0.164845         0.136728          0.192962\n",
       "0          Gender  Saved         0.159646         0.126552          0.192741\n",
       "0         Fitness  Saved         0.120809         0.085368          0.156249\n",
       "0   Social Status  Saved         0.170991         0.079034          0.262948\n",
       "0  CrossingSignal  Saved         0.336349         0.306916          0.365781\n",
       "0             Age  Saved         0.481846         0.450556          0.513135\n",
       "0     Utilitarian  Saved         0.573304         0.545087          0.601520\n",
       "0         Species  Saved         0.645940         0.617086          0.674794"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([compute_amce(df, x=\"Intervention\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Barrier\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Gender\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Fitness\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Social Status\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"CrossingSignal\",y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Age\", y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Utilitarian\", y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Species\", y=\"Saved\")\n",
    "           ])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AMCE estimates computed above are the same as those calculated with the functions by Awad et al. (2018), see R script `7_CalculateAMCE.R`.\n",
    "\n",
    "\n",
    "|           label            |    dv  |  amce |   se  | conf.low | conf.high |\n",
    "|----------------------------|--------|-------|-------|----------|-----------|\n",
    "|   Intervention             | Saved  | 0.068 | 0.008 |    0.052 |     0.084 |\n",
    "|        Barrier             | Saved  | 0.165 | 0.014 |    0.137 |     0.193 |\n",
    "|            Law             | Saved  | 0.336 | 0.015 |    0.307 |     0.366 |\n",
    "|         Gender             | Saved  | 0.160 | 0.017 |    0.127 |     0.193 |\n",
    "|        Fitness             | Saved  | 0.121 | 0.018 |    0.085 |     0.156 |\n",
    "|  Social Status             | Saved  | 0.171 | 0.047 |    0.079 |     0.263 |\n",
    "|            Age             | Saved  | 0.482 | 0.016 |    0.451 |     0.513 |\n",
    "| No. Characters             | Saved  | 0.573 | 0.014 |    0.545 |     0.602 |\n",
    "|        Species             | Saved  | 0.646 | 0.015 |    0.617 |     0.675 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the PPI point estimates with a large sample size of human responses which we expect to be very close to the AMCE estimates obtained by applying classical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>pointest_ppi</th>\n",
       "      <th>conf_low_ppi</th>\n",
       "      <th>conf_high_ppi</th>\n",
       "      <th>conf_low_ols</th>\n",
       "      <th>conf_high_ols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.068236</td>\n",
       "      <td>0.057351</td>\n",
       "      <td>0.079119</td>\n",
       "      <td>0.036889</td>\n",
       "      <td>0.082952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Barrier</td>\n",
       "      <td>0.165336</td>\n",
       "      <td>0.149060</td>\n",
       "      <td>0.181591</td>\n",
       "      <td>0.102490</td>\n",
       "      <td>0.170932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Gender</td>\n",
       "      <td>0.159563</td>\n",
       "      <td>0.134662</td>\n",
       "      <td>0.184486</td>\n",
       "      <td>0.122257</td>\n",
       "      <td>0.227644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>0.121818</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.148291</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.122181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Social Status</td>\n",
       "      <td>0.177492</td>\n",
       "      <td>0.105193</td>\n",
       "      <td>0.250504</td>\n",
       "      <td>0.119301</td>\n",
       "      <td>0.404078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saved</td>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>0.336415</td>\n",
       "      <td>0.317788</td>\n",
       "      <td>0.355041</td>\n",
       "      <td>0.287159</td>\n",
       "      <td>0.366527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Age</td>\n",
       "      <td>0.481703</td>\n",
       "      <td>0.458753</td>\n",
       "      <td>0.504653</td>\n",
       "      <td>0.427758</td>\n",
       "      <td>0.524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>0.573553</td>\n",
       "      <td>0.552606</td>\n",
       "      <td>0.594490</td>\n",
       "      <td>0.516992</td>\n",
       "      <td>0.604252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.646194</td>\n",
       "      <td>0.626448</td>\n",
       "      <td>0.665929</td>\n",
       "      <td>0.595681</td>\n",
       "      <td>0.680814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y               x  pointest_ppi  conf_low_ppi  conf_high_ppi  \\\n",
       "0  Saved    Intervention      0.068236      0.057351       0.079119   \n",
       "1  Saved         Barrier      0.165336      0.149060       0.181591   \n",
       "2  Saved          Gender      0.159563      0.134662       0.184486   \n",
       "3  Saved         Fitness      0.121818      0.095238       0.148291   \n",
       "4  Saved   Social Status      0.177492      0.105193       0.250504   \n",
       "5  Saved  CrossingSignal      0.336415      0.317788       0.355041   \n",
       "6  Saved             Age      0.481703      0.458753       0.504653   \n",
       "7  Saved     Utilitarian      0.573553      0.552606       0.594490   \n",
       "8  Saved         Species      0.646194      0.626448       0.665929   \n",
       "\n",
       "   conf_low_ols  conf_high_ols  \n",
       "0      0.036889       0.082952  \n",
       "1      0.102490       0.170932  \n",
       "2      0.122257       0.227644  \n",
       "3      0.010840       0.122181  \n",
       "4      0.119301       0.404078  \n",
       "5      0.287159       0.366527  \n",
       "6      0.427758       0.524024  \n",
       "7      0.516992       0.604252  \n",
       "8      0.595681       0.680814  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = df[\"ResponseID\"].unique()\n",
    "n = 5000\n",
    "N = len(ids) - n\n",
    "random.seed(2024)\n",
    "\n",
    "n_ids = random.sample(ids.tolist(), k=n)\n",
    "N_ids = random.sample(list(set(ids) - set(n_ids)), k=N)\n",
    "\n",
    "df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "pd.concat([compute_amce_ppi(df_human, df_silicon, x=\"Intervention\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Barrier\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Gender\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Fitness\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Social Status\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"CrossingSignal\",y=\"Saved\"),\n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Age\", y=\"Saved\"),\n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Utilitarian\", y=\"Saved\"),\n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Species\", y=\"Saved\")\n",
    "           ],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over dependent variable: gpt4turbo_wp_Saved\n",
      "    Predictor: Intervention with human sample size 50\n",
      "    Predictor: Intervention with human sample size 100\n",
      "    Predictor: Intervention with human sample size 500\n",
      "    Predictor: Barrier with human sample size 50\n",
      "    Predictor: Barrier with human sample size 100\n",
      "    Predictor: Barrier with human sample size 500\n",
      "    Predictor: CrossingSignal with human sample size 50\n",
      "    Predictor: CrossingSignal with human sample size 100\n",
      "    Predictor: CrossingSignal with human sample size 500\n",
      "    Predictor: Gender with human sample size 50\n",
      "    Predictor: Gender with human sample size 100\n",
      "    Predictor: Gender with human sample size 500\n",
      "    Predictor: Fitness with human sample size 50\n",
      "    Predictor: Fitness with human sample size 100\n",
      "    Predictor: Fitness with human sample size 500\n",
      "    Predictor: Social Status with human sample size 50\n",
      "    Predictor: Social Status with human sample size 100\n",
      "    Predictor: Social Status with human sample size 500\n",
      "    Predictor: Age with human sample size 50\n",
      "    Predictor: Age with human sample size 100\n",
      "    Predictor: Age with human sample size 500\n",
      "    Predictor: Utilitarian with human sample size 50\n",
      "    Predictor: Utilitarian with human sample size 100\n",
      "    Predictor: Utilitarian with human sample size 500\n",
      "    Predictor: Species with human sample size 50\n",
      "    Predictor: Species with human sample size 100\n",
      "    Predictor: Species with human sample size 500\n",
      "Iterating over dependent variable: gpt4o_wp_Saved\n",
      "    Predictor: Intervention with human sample size 50\n",
      "    Predictor: Intervention with human sample size 100\n",
      "    Predictor: Intervention with human sample size 500\n",
      "    Predictor: Barrier with human sample size 50\n",
      "    Predictor: Barrier with human sample size 100\n",
      "    Predictor: Barrier with human sample size 500\n",
      "    Predictor: CrossingSignal with human sample size 50\n",
      "    Predictor: CrossingSignal with human sample size 100\n",
      "    Predictor: CrossingSignal with human sample size 500\n",
      "    Predictor: Gender with human sample size 50\n",
      "    Predictor: Gender with human sample size 100\n",
      "    Predictor: Gender with human sample size 500\n",
      "    Predictor: Fitness with human sample size 50\n",
      "    Predictor: Fitness with human sample size 100\n",
      "    Predictor: Fitness with human sample size 500\n",
      "    Predictor: Social Status with human sample size 50\n",
      "    Predictor: Social Status with human sample size 100\n",
      "    Predictor: Social Status with human sample size 500\n",
      "    Predictor: Age with human sample size 50\n",
      "    Predictor: Age with human sample size 100\n",
      "    Predictor: Age with human sample size 500\n",
      "    Predictor: Utilitarian with human sample size 50\n",
      "    Predictor: Utilitarian with human sample size 100\n",
      "    Predictor: Utilitarian with human sample size 500\n",
      "    Predictor: Species with human sample size 50\n",
      "    Predictor: Species with human sample size 100\n",
      "    Predictor: Species with human sample size 500\n",
      "Iterating over dependent variable: gpt35turbo0125_wp_Saved\n",
      "    Predictor: Intervention with human sample size 50\n",
      "    Predictor: Intervention with human sample size 100\n",
      "    Predictor: Intervention with human sample size 500\n",
      "    Predictor: Barrier with human sample size 50\n",
      "    Predictor: Barrier with human sample size 100\n",
      "    Predictor: Barrier with human sample size 500\n",
      "    Predictor: CrossingSignal with human sample size 50\n",
      "    Predictor: CrossingSignal with human sample size 100\n",
      "    Predictor: CrossingSignal with human sample size 500\n",
      "    Predictor: Gender with human sample size 50\n",
      "    Predictor: Gender with human sample size 100\n",
      "    Predictor: Gender with human sample size 500\n",
      "    Predictor: Fitness with human sample size 50\n",
      "    Predictor: Fitness with human sample size 100\n",
      "    Predictor: Fitness with human sample size 500\n",
      "    Predictor: Social Status with human sample size 50\n",
      "    Predictor: Social Status with human sample size 100\n",
      "    Predictor: Social Status with human sample size 500\n",
      "    Predictor: Age with human sample size 50\n",
      "    Predictor: Age with human sample size 100\n",
      "    Predictor: Age with human sample size 500\n",
      "    Predictor: Utilitarian with human sample size 50\n",
      "    Predictor: Utilitarian with human sample size 100\n",
      "    Predictor: Utilitarian with human sample size 500\n",
      "    Predictor: Species with human sample size 50\n",
      "    Predictor: Species with human sample size 100\n",
      "    Predictor: Species with human sample size 500\n"
     ]
    }
   ],
   "source": [
    "# sample size human subjects\n",
    "ns = [50,100,500]\n",
    "\n",
    "# sample size silicon subjects\n",
    "Ns = [25,30,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000,\n",
    "      1250,1500,1750,2000,  \n",
    "      2250,2500]\n",
    "\n",
    "# predictions\n",
    "Ys = [\"gpt4turbo_wp_Saved\"]#,\"gpt4o_wp_Saved\",\"gpt35turbo0125_wp_Saved\"]\n",
    "\n",
    "# structural attributes of scenarios\n",
    "Xs_scenarios  = ['Intervention', 'Barrier','CrossingSignal']\n",
    "\n",
    "# attributes of characters\n",
    "Xs_characters = ['Gender','Fitness','Social Status','Age','Utilitarian','Species']\n",
    "\n",
    "# all attributes\n",
    "Xs = Xs_scenarios + Xs_characters\n",
    "\n",
    "# number of repetitions for combinations of n and N\n",
    "reps = 350\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for y in Ys:\n",
    "  print(f\"Iterating over dependent variable: {y}\")\n",
    "  \n",
    "  for x in Xs:\n",
    "\n",
    "    for n in ns:\n",
    "      print(f\"    Predictor: {x} with human sample size {n}\")\n",
    "      \n",
    "      for N in Ns:\n",
    "\n",
    "        for r in range(reps):\n",
    "\n",
    "          # subset to dilemmas with variation on structural attribute\n",
    "          if x in Xs_scenarios:\n",
    "\n",
    "              cnt = df.groupby(\"ResponseID\")[x].nunique()\n",
    "              ids = cnt[ cnt > 1].index.tolist()\n",
    "\n",
    "          # subset to dilemmas with relevant character attribute\n",
    "          if x in Xs_characters:\n",
    "\n",
    "              ids = df.loc[ (df[\"ScenarioType\"]==x) & (df[\"ScenarioTypeStrict\"]==x), \"ResponseID\"].tolist()\n",
    "          \n",
    "          # skip current iteration if target n is larger than population\n",
    "          if (len(ids) < n):\n",
    "             continue \n",
    "\n",
    "          # sample dilemmas for human subjects sample\n",
    "          n_ids = random.sample(ids, k=n)\n",
    "          \n",
    "          # get remaining dilemma ids to sample from\n",
    "          remaining_ids = list(set(ids) - set(n_ids))\n",
    "\n",
    "          # skip current iteration if target N is larger than population\n",
    "          if (len(remaining_ids) < N):\n",
    "             continue \n",
    "          \n",
    "          # sample dilemmas for silicon subjects sample\n",
    "          N_ids = random.sample(remaining_ids, k=N)\n",
    "\n",
    "          # subset data\n",
    "          df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "          df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "          # pool human and silicon subject decisions\n",
    "          df_pooled = pd.concat([df_human,df_silicon], ignore_index=True)\n",
    "\n",
    "          # compute ppi acme\n",
    "          ppi = compute_amce_ppi(n_data=df_human, N_data=df_silicon, x=x, y=y)\n",
    "          \n",
    "          # compute acme on pooled data\n",
    "          pooled = compute_amce(data=df_pooled,x=x,y=y)\n",
    "\n",
    "          # store data\n",
    "          to_append = pd.merge(ppi, pooled, on=['x','y'], how='outer')\n",
    "          to_append[\"n\"] = n\n",
    "          to_append[\"N\"] = N\n",
    "          \n",
    "          result = pd.concat([result, to_append], ignore_index=True)\n",
    "          del ppi \n",
    "          del pooled \n",
    "          del to_append\n",
    "\n",
    "          \n",
    "          \n",
    "\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>pointest_ppi</th>\n",
       "      <th>conf_low_ppi</th>\n",
       "      <th>conf_high_ppi</th>\n",
       "      <th>conf_low_ols</th>\n",
       "      <th>conf_high_ols</th>\n",
       "      <th>pointest_pooled</th>\n",
       "      <th>conf_low_pooled</th>\n",
       "      <th>conf_high_pooled</th>\n",
       "      <th>n</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.292633</td>\n",
       "      <td>0.089101</td>\n",
       "      <td>0.482586</td>\n",
       "      <td>0.020713</td>\n",
       "      <td>0.434234</td>\n",
       "      <td>0.167461</td>\n",
       "      <td>-0.069359</td>\n",
       "      <td>0.404281</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>-0.039405</td>\n",
       "      <td>-0.261729</td>\n",
       "      <td>0.182036</td>\n",
       "      <td>-0.297055</td>\n",
       "      <td>0.165171</td>\n",
       "      <td>-0.063529</td>\n",
       "      <td>-0.303466</td>\n",
       "      <td>0.176407</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.192697</td>\n",
       "      <td>-0.026391</td>\n",
       "      <td>0.412243</td>\n",
       "      <td>-0.010273</td>\n",
       "      <td>0.433142</td>\n",
       "      <td>0.192639</td>\n",
       "      <td>-0.030148</td>\n",
       "      <td>0.415427</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>-0.220217</td>\n",
       "      <td>0.223485</td>\n",
       "      <td>-0.233045</td>\n",
       "      <td>0.220302</td>\n",
       "      <td>0.243587</td>\n",
       "      <td>0.012526</td>\n",
       "      <td>0.474648</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.189441</td>\n",
       "      <td>-0.027783</td>\n",
       "      <td>0.398921</td>\n",
       "      <td>-0.115507</td>\n",
       "      <td>0.333120</td>\n",
       "      <td>0.219924</td>\n",
       "      <td>-0.015733</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639010</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.637495</td>\n",
       "      <td>0.578263</td>\n",
       "      <td>0.696727</td>\n",
       "      <td>0.578294</td>\n",
       "      <td>0.696696</td>\n",
       "      <td>0.166334</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.203171</td>\n",
       "      <td>500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639011</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.635162</td>\n",
       "      <td>0.578368</td>\n",
       "      <td>0.691986</td>\n",
       "      <td>0.580041</td>\n",
       "      <td>0.694163</td>\n",
       "      <td>0.149264</td>\n",
       "      <td>0.111506</td>\n",
       "      <td>0.187021</td>\n",
       "      <td>500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639012</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.570635</td>\n",
       "      <td>0.507844</td>\n",
       "      <td>0.632462</td>\n",
       "      <td>0.507877</td>\n",
       "      <td>0.632429</td>\n",
       "      <td>0.172701</td>\n",
       "      <td>0.135393</td>\n",
       "      <td>0.210009</td>\n",
       "      <td>500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639013</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.722783</td>\n",
       "      <td>0.672224</td>\n",
       "      <td>0.773200</td>\n",
       "      <td>0.674012</td>\n",
       "      <td>0.775505</td>\n",
       "      <td>0.183403</td>\n",
       "      <td>0.146454</td>\n",
       "      <td>0.220351</td>\n",
       "      <td>500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639014</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.697559</td>\n",
       "      <td>0.642570</td>\n",
       "      <td>0.751239</td>\n",
       "      <td>0.643473</td>\n",
       "      <td>0.752203</td>\n",
       "      <td>0.156114</td>\n",
       "      <td>0.119235</td>\n",
       "      <td>0.192993</td>\n",
       "      <td>500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>639015 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              y             x  pointest_ppi  conf_low_ppi  \\\n",
       "0            gpt4turbo_wp_Saved  Intervention      0.292633      0.089101   \n",
       "1            gpt4turbo_wp_Saved  Intervention     -0.039405     -0.261729   \n",
       "2            gpt4turbo_wp_Saved  Intervention      0.192697     -0.026391   \n",
       "3            gpt4turbo_wp_Saved  Intervention      0.001849     -0.220217   \n",
       "4            gpt4turbo_wp_Saved  Intervention      0.189441     -0.027783   \n",
       "...                         ...           ...           ...           ...   \n",
       "639010  gpt35turbo0125_wp_Saved       Species      0.637495      0.578263   \n",
       "639011  gpt35turbo0125_wp_Saved       Species      0.635162      0.578368   \n",
       "639012  gpt35turbo0125_wp_Saved       Species      0.570635      0.507844   \n",
       "639013  gpt35turbo0125_wp_Saved       Species      0.722783      0.672224   \n",
       "639014  gpt35turbo0125_wp_Saved       Species      0.697559      0.642570   \n",
       "\n",
       "        conf_high_ppi  conf_low_ols  conf_high_ols  pointest_pooled  \\\n",
       "0            0.482586      0.020713       0.434234         0.167461   \n",
       "1            0.182036     -0.297055       0.165171        -0.063529   \n",
       "2            0.412243     -0.010273       0.433142         0.192639   \n",
       "3            0.223485     -0.233045       0.220302         0.243587   \n",
       "4            0.398921     -0.115507       0.333120         0.219924   \n",
       "...               ...           ...            ...              ...   \n",
       "639010       0.696727      0.578294       0.696696         0.166334   \n",
       "639011       0.691986      0.580041       0.694163         0.149264   \n",
       "639012       0.632462      0.507877       0.632429         0.172701   \n",
       "639013       0.773200      0.674012       0.775505         0.183403   \n",
       "639014       0.751239      0.643473       0.752203         0.156114   \n",
       "\n",
       "        conf_low_pooled  conf_high_pooled    n     N  \n",
       "0             -0.069359          0.404281   50    25  \n",
       "1             -0.303466          0.176407   50    25  \n",
       "2             -0.030148          0.415427   50    25  \n",
       "3              0.012526          0.474648   50    25  \n",
       "4             -0.015733          0.455581   50    25  \n",
       "...                 ...               ...  ...   ...  \n",
       "639010         0.129496          0.203171  500  2500  \n",
       "639011         0.111506          0.187021  500  2500  \n",
       "639012         0.135393          0.210009  500  2500  \n",
       "639013         0.146454          0.220351  500  2500  \n",
       "639014         0.119235          0.192993  500  2500  \n",
       "\n",
       "[639015 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_csv(\"../Data/6_ResultsPPI.csv.gz\", compression=\"gzip\", index=False)\n",
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
