{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from ppi_py import ppi_ols_ci, classical_ols_ci, ppi_ols_pointestimate\n",
    "\n",
    "df = pd.read_csv(\"../Data/5_SurveySampleLLM.csv.gz\")\n",
    "\n",
    "Covs = ['PedPed', 'Barrier', 'CrossingSignal', 'NumberOfCharacters',\n",
    "        'DiffNumberOFCharacters', 'LeftHand', 'Man', 'Woman', 'Pregnant',\n",
    "        'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
    "        'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive',\n",
    "        'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor',\n",
    "        'MaleDoctor', 'Dog', 'Cat', \n",
    "        'Intervention'\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAs Saved:  0\n",
      "Number of NAs gpt4turbo_wp_Saved:  0\n",
      "Number of NAs gpt4o_wp_Saved:  6\n",
      "Number of NAs gpt35turbo0125_wp_Saved:  2\n"
     ]
    }
   ],
   "source": [
    "# very few missing predicted values for the dependent variable\n",
    "print(\"Number of NAs Saved: \",df[\"Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt4turbo_wp_Saved: \",df[\"gpt4turbo_wp_Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt4o_wp_Saved: \",df[\"gpt4o_wp_Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt35turbo0125_wp_Saved: \",df[\"gpt35turbo0125_wp_Saved\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate weights for conjoint experiment\n",
    "def CalcTheoreticalInt(r):\n",
    "    # this function is applied to each row (r)\n",
    "    if r[\"Intervention\"]==0:\n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: p = 0.48\n",
    "            else: p = 0.32\n",
    "            \n",
    "            if r[\"CrossingSignal\"]==0:   p = p * 0.48\n",
    "            elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "            else: p = p * 0.32\n",
    "        else: p = 0.2\n",
    "\n",
    "    else: \n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: \n",
    "                p = 0.48\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.32\n",
    "                else: p = p * 0.2\n",
    "            else: \n",
    "                p = 0.2\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "                else: p = p * 0.32\n",
    "        else: p = 0.32  \n",
    "    \n",
    "    return(p)  \n",
    "        \n",
    "def calcWeightsTheoretical(profiles):\n",
    "    \n",
    "    p = profiles.apply(CalcTheoreticalInt, axis=1)\n",
    "\n",
    "    weight = 1/p \n",
    "\n",
    "    return(weight) \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate amce for intervention\n",
    "def compute_amce(data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        data.loc[:,\"weights\"] = calcWeightsTheoretical(data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        X = dd[\"Intervention\"]\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]==0) & (data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        X = dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        X = 1 - X\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]!=0) & (data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        X = dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        X = 2 - X \n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Utilitarian\") & (data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        X = (dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Species\") & (data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        X = (dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Gender\") & (data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        X = (dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Fitness\") & (data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        X = (dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Age\") & (data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        X = (dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Social Status\") & (data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        X = (dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "\n",
    "    # fit model and extract estimates\n",
    "    fit = model.fit(cov_type = 'cluster', cov_kwds = {'groups': dd[\"UserID\"]})\n",
    "    coef = fit.params[x]\n",
    "    ci = fit.conf_int(alpha=alpha).loc[x]\n",
    "\n",
    "    # store results\n",
    "    res = pd.DataFrame({\n",
    "        'x': [x],\n",
    "        'y': [y],\n",
    "        'pointest_pooled': [coef],\n",
    "        'conf_low_pooled': [ci[0]],\n",
    "        'conf_high_pooled': [ci[1]]\n",
    "    })\n",
    "\n",
    "    return(res)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate amce with ppi \n",
    "def compute_amce_ppi(n_data, N_data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        n_data.loc[:,\"weights\"] = calcWeightsTheoretical(n_data)\n",
    "        N_data.loc[:,\"weights\"] = calcWeightsTheoretical(N_data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data.dropna(subset=y)\n",
    "        N_dd = N_data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        n_X = n_dd[\"Intervention\"]               \n",
    "        N_X = N_dd[\"Intervention\"]\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]==0) & (n_data[\"PedPed\"]==0), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]==0) & (N_data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        n_X = n_dd[\"Barrier\"]\n",
    "        N_X = N_dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        n_X = 1 - n_X\n",
    "        N_X = 1 - N_X\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]!=0) & (n_data[\"PedPed\"]==1), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]!=0) & (N_data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        n_X = n_dd[\"CrossingSignal\"]\n",
    "        N_X = N_dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        n_X = 2 - n_X \n",
    "        N_X = 2 - N_X \n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "    \n",
    "\n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Utilitarian\") & (n_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Utilitarian\") & (N_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        n_X = (n_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Species\") & (n_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Species\") & (N_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        n_X = (n_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Gender\") & (n_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Gender\") & (N_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        n_X = (n_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Fitness\") & (n_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Fitness\") & (N_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        n_X = (n_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Age\") & (n_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Age\") & (N_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        n_X = (n_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Social Status\") & (n_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Social Status\") & (N_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        n_X = (n_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # calculate point estimate\n",
    "    pointest_ppi = ppi_ols_pointestimate(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                         X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                         w=n_weights, w_unlabeled=N_weights)\n",
    "\n",
    "    # calculate PPI confidence intervals\n",
    "    lower_CI_ppi, upper_CI_ppi = ppi_ols_ci(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                            X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                            w=n_weights, w_unlabeled=N_weights, alpha=alpha)\n",
    "\n",
    "    # calculate OLS confidence intervals\n",
    "    lower_CI_ols, upper_CI_ols = classical_ols_ci(X=n_X, Y=n_Y_human, w=n_weights, alpha=alpha)\n",
    "\n",
    "    # create and return the output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        \"y\": y,                              # Dependent variable (Saved)\n",
    "        \"x\": x,                              # Predictor variable (scenario attribute)\n",
    "        \"pointest_ppi\": pointest_ppi[1],     # PPI point estimate\n",
    "        \"conf_low_ppi\": lower_CI_ppi[1],     # The lower bound of the PPI confidence interval\n",
    "        \"conf_high_ppi\": upper_CI_ppi[1],    # The upper bound of the PPI confidence interval\n",
    "        \"conf_low_ols\": lower_CI_ols[1],     # The lower bound of the OLS confidence interval\n",
    "        \"conf_high_ols\": upper_CI_ols[1]},   # The upper bound of the OLS confidence interval\n",
    "        index=[0])\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>pointest_pooled</th>\n",
       "      <th>conf_low_pooled</th>\n",
       "      <th>conf_high_pooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intervention</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.068216</td>\n",
       "      <td>0.052464</td>\n",
       "      <td>0.083969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrier</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.164845</td>\n",
       "      <td>0.136728</td>\n",
       "      <td>0.192962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gender</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.159646</td>\n",
       "      <td>0.126552</td>\n",
       "      <td>0.192741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fitness</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.120809</td>\n",
       "      <td>0.085368</td>\n",
       "      <td>0.156249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Social Status</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.170991</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>0.262948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.336349</td>\n",
       "      <td>0.306916</td>\n",
       "      <td>0.365781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.481846</td>\n",
       "      <td>0.450556</td>\n",
       "      <td>0.513135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.573304</td>\n",
       "      <td>0.545087</td>\n",
       "      <td>0.601520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Species</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.674794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x      y  pointest_pooled  conf_low_pooled  conf_high_pooled\n",
       "0    Intervention  Saved         0.068216         0.052464          0.083969\n",
       "0         Barrier  Saved         0.164845         0.136728          0.192962\n",
       "0          Gender  Saved         0.159646         0.126552          0.192741\n",
       "0         Fitness  Saved         0.120809         0.085368          0.156249\n",
       "0   Social Status  Saved         0.170991         0.079034          0.262948\n",
       "0  CrossingSignal  Saved         0.336349         0.306916          0.365781\n",
       "0             Age  Saved         0.481846         0.450556          0.513135\n",
       "0     Utilitarian  Saved         0.573304         0.545087          0.601520\n",
       "0         Species  Saved         0.645940         0.617086          0.674794"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([compute_amce(df, x=\"Intervention\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Barrier\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Gender\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Fitness\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Social Status\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"CrossingSignal\",y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Age\", y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Utilitarian\", y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Species\", y=\"Saved\")\n",
    "           ])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AMCE estimates computed above are the same as those calculated with the functions by Awad et al. (2018), see R script `7_CalculateAMCE.R`.\n",
    "\n",
    "\n",
    "|           label            |    dv  |  amce |   se  | conf.low | conf.high |\n",
    "|----------------------------|--------|-------|-------|----------|-----------|\n",
    "|   Intervention             | Saved  | 0.068 | 0.008 |    0.052 |     0.084 |\n",
    "|        Barrier             | Saved  | 0.165 | 0.014 |    0.137 |     0.193 |\n",
    "|            Law             | Saved  | 0.336 | 0.015 |    0.307 |     0.366 |\n",
    "|         Gender             | Saved  | 0.160 | 0.017 |    0.127 |     0.193 |\n",
    "|        Fitness             | Saved  | 0.121 | 0.018 |    0.085 |     0.156 |\n",
    "|  Social Status             | Saved  | 0.171 | 0.047 |    0.079 |     0.263 |\n",
    "|            Age             | Saved  | 0.482 | 0.016 |    0.451 |     0.513 |\n",
    "| No. Characters             | Saved  | 0.573 | 0.014 |    0.545 |     0.602 |\n",
    "|        Species             | Saved  | 0.646 | 0.015 |    0.617 |     0.675 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the PPI point estimates with a large sample size of human responses which we expect to be very close to the AMCE estimates obtained by applying classical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>pointest_ppi</th>\n",
       "      <th>conf_low_ppi</th>\n",
       "      <th>conf_high_ppi</th>\n",
       "      <th>conf_low_ols</th>\n",
       "      <th>conf_high_ols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.068236</td>\n",
       "      <td>0.057351</td>\n",
       "      <td>0.079119</td>\n",
       "      <td>0.036889</td>\n",
       "      <td>0.082952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Barrier</td>\n",
       "      <td>0.165336</td>\n",
       "      <td>0.149060</td>\n",
       "      <td>0.181591</td>\n",
       "      <td>0.102490</td>\n",
       "      <td>0.170932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Gender</td>\n",
       "      <td>0.159563</td>\n",
       "      <td>0.134662</td>\n",
       "      <td>0.184486</td>\n",
       "      <td>0.122257</td>\n",
       "      <td>0.227644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>0.121818</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.148291</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.122181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Social Status</td>\n",
       "      <td>0.177492</td>\n",
       "      <td>0.105193</td>\n",
       "      <td>0.250504</td>\n",
       "      <td>0.119301</td>\n",
       "      <td>0.404078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saved</td>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>0.336415</td>\n",
       "      <td>0.317788</td>\n",
       "      <td>0.355041</td>\n",
       "      <td>0.287159</td>\n",
       "      <td>0.366527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Age</td>\n",
       "      <td>0.481703</td>\n",
       "      <td>0.458753</td>\n",
       "      <td>0.504653</td>\n",
       "      <td>0.427758</td>\n",
       "      <td>0.524024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>0.573553</td>\n",
       "      <td>0.552606</td>\n",
       "      <td>0.594490</td>\n",
       "      <td>0.516992</td>\n",
       "      <td>0.604252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.646194</td>\n",
       "      <td>0.626448</td>\n",
       "      <td>0.665929</td>\n",
       "      <td>0.595681</td>\n",
       "      <td>0.680814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y               x  pointest_ppi  conf_low_ppi  conf_high_ppi  \\\n",
       "0  Saved    Intervention      0.068236      0.057351       0.079119   \n",
       "1  Saved         Barrier      0.165336      0.149060       0.181591   \n",
       "2  Saved          Gender      0.159563      0.134662       0.184486   \n",
       "3  Saved         Fitness      0.121818      0.095238       0.148291   \n",
       "4  Saved   Social Status      0.177492      0.105193       0.250504   \n",
       "5  Saved  CrossingSignal      0.336415      0.317788       0.355041   \n",
       "6  Saved             Age      0.481703      0.458753       0.504653   \n",
       "7  Saved     Utilitarian      0.573553      0.552606       0.594490   \n",
       "8  Saved         Species      0.646194      0.626448       0.665929   \n",
       "\n",
       "   conf_low_ols  conf_high_ols  \n",
       "0      0.036889       0.082952  \n",
       "1      0.102490       0.170932  \n",
       "2      0.122257       0.227644  \n",
       "3      0.010840       0.122181  \n",
       "4      0.119301       0.404078  \n",
       "5      0.287159       0.366527  \n",
       "6      0.427758       0.524024  \n",
       "7      0.516992       0.604252  \n",
       "8      0.595681       0.680814  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = df[\"ResponseID\"].unique()\n",
    "n = 5000\n",
    "N = len(ids) - n\n",
    "random.seed(2024)\n",
    "\n",
    "n_ids = random.sample(ids.tolist(), k=n)\n",
    "N_ids = random.sample(list(set(ids) - set(n_ids)), k=N)\n",
    "\n",
    "df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "pd.concat([compute_amce_ppi(df_human, df_silicon, x=\"Intervention\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Barrier\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Gender\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Fitness\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Social Status\", y=\"Saved\"), \n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"CrossingSignal\",y=\"Saved\"),\n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Age\", y=\"Saved\"),\n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Utilitarian\", y=\"Saved\"),\n",
    "           compute_amce_ppi(df_human, df_silicon, x=\"Species\", y=\"Saved\")\n",
    "           ],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over dependent variable: gpt4turbo_wp_Saved\n",
      "    Predictor: Intervention with human sample size 50\n",
      "    Predictor: Intervention with human sample size 500\n",
      "    Predictor: Barrier with human sample size 50\n",
      "    Predictor: Barrier with human sample size 500\n",
      "    Predictor: CrossingSignal with human sample size 50\n",
      "    Predictor: CrossingSignal with human sample size 500\n",
      "    Predictor: Gender with human sample size 50\n",
      "    Predictor: Gender with human sample size 500\n",
      "    Predictor: Fitness with human sample size 50\n",
      "    Predictor: Fitness with human sample size 500\n",
      "    Predictor: Social Status with human sample size 50\n",
      "    Predictor: Social Status with human sample size 500\n",
      "    Predictor: Age with human sample size 50\n",
      "    Predictor: Age with human sample size 500\n",
      "    Predictor: Utilitarian with human sample size 50\n",
      "    Predictor: Utilitarian with human sample size 500\n",
      "    Predictor: Species with human sample size 50\n",
      "    Predictor: Species with human sample size 500\n",
      "Iterating over dependent variable: gpt4o_wp_Saved\n",
      "    Predictor: Intervention with human sample size 50\n",
      "    Predictor: Intervention with human sample size 500\n",
      "    Predictor: Barrier with human sample size 50\n",
      "    Predictor: Barrier with human sample size 500\n",
      "    Predictor: CrossingSignal with human sample size 50\n",
      "    Predictor: CrossingSignal with human sample size 500\n",
      "    Predictor: Gender with human sample size 50\n",
      "    Predictor: Gender with human sample size 500\n",
      "    Predictor: Fitness with human sample size 50\n",
      "    Predictor: Fitness with human sample size 500\n",
      "    Predictor: Social Status with human sample size 50\n",
      "    Predictor: Social Status with human sample size 500\n",
      "    Predictor: Age with human sample size 50\n",
      "    Predictor: Age with human sample size 500\n",
      "    Predictor: Utilitarian with human sample size 50\n",
      "    Predictor: Utilitarian with human sample size 500\n",
      "    Predictor: Species with human sample size 50\n",
      "    Predictor: Species with human sample size 500\n",
      "Iterating over dependent variable: gpt35turbo0125_wp_Saved\n",
      "    Predictor: Intervention with human sample size 50\n",
      "    Predictor: Intervention with human sample size 500\n",
      "    Predictor: Barrier with human sample size 50\n",
      "    Predictor: Barrier with human sample size 500\n",
      "    Predictor: CrossingSignal with human sample size 50\n",
      "    Predictor: CrossingSignal with human sample size 500\n",
      "    Predictor: Gender with human sample size 50\n",
      "    Predictor: Gender with human sample size 500\n",
      "    Predictor: Fitness with human sample size 50\n",
      "    Predictor: Fitness with human sample size 500\n",
      "    Predictor: Social Status with human sample size 50\n",
      "    Predictor: Social Status with human sample size 500\n",
      "    Predictor: Age with human sample size 50\n",
      "    Predictor: Age with human sample size 500\n",
      "    Predictor: Utilitarian with human sample size 50\n",
      "    Predictor: Utilitarian with human sample size 500\n",
      "    Predictor: Species with human sample size 50\n",
      "    Predictor: Species with human sample size 500\n"
     ]
    }
   ],
   "source": [
    "# sample size human subjects\n",
    "ns = [50,500]\n",
    "\n",
    "# sample size silicon subjects\n",
    "Ns = [250,500,600,700,800,900,1000,   1500,2000,2500,3000,   3500,4000,4500,5000]\n",
    "\n",
    "# predictions\n",
    "Ys = [\"gpt4turbo_wp_Saved\",\"gpt4o_wp_Saved\",\"gpt35turbo0125_wp_Saved\"]\n",
    "\n",
    "# structural attributes of scenarios\n",
    "Xs_scenarios  = ['Intervention', 'Barrier','CrossingSignal']\n",
    "\n",
    "# attributes of characters\n",
    "Xs_characters = ['Gender','Fitness','Social Status','Age','Utilitarian','Species']\n",
    "\n",
    "# all attributes\n",
    "Xs = Xs_scenarios + Xs_characters\n",
    "\n",
    "# number of repetitions for combinations of n and N\n",
    "reps = 200\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for y in Ys:\n",
    "  print(f\"Iterating over dependent variable: {y}\")\n",
    "  \n",
    "  for x in Xs:\n",
    "\n",
    "    for n in ns:\n",
    "      print(f\"    Predictor: {x} with human sample size {n}\")\n",
    "      \n",
    "      for N in Ns:\n",
    "\n",
    "        for r in range(reps):\n",
    "\n",
    "          # subset to dilemmas with variation on structural attribute\n",
    "          if x in Xs_scenarios:\n",
    "\n",
    "              cnt = df.groupby(\"ResponseID\")[x].nunique()\n",
    "              ids = cnt[ cnt > 1].index.tolist()\n",
    "\n",
    "          # subset to dilemmas with relevant character attribute\n",
    "          if x in Xs_characters:\n",
    "\n",
    "              ids = df.loc[ (df[\"ScenarioType\"]==x) & (df[\"ScenarioTypeStrict\"]==x), \"ResponseID\"].tolist()\n",
    "          \n",
    "          # sample dilemmas for human subjects sample\n",
    "          n_ids = random.sample(ids, k=n)\n",
    "          \n",
    "          # get remaining dilemma ids to sample from\n",
    "          remaining_ids = list(set(ids) - set(n_ids))\n",
    "\n",
    "          # skip current iteration if target N is larger than population\n",
    "          if len(remaining_ids) < N:\n",
    "             continue \n",
    "          \n",
    "          # sample dilemmas for silicon subjects sample\n",
    "          N_ids = random.sample(remaining_ids, k=N)\n",
    "\n",
    "          # subset data\n",
    "          df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "          df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "          # pool human and silicon subject decisions\n",
    "          df_pooled = pd.concat([df_human,df_silicon], ignore_index=True)\n",
    "\n",
    "          # compute ppi acme\n",
    "          ppi = compute_amce_ppi(n_data=df_human, N_data=df_silicon, x=x, y=y)\n",
    "          \n",
    "          # compute acme on pooled data\n",
    "          pooled = compute_amce(data=df_pooled,x=x,y=y)\n",
    "\n",
    "          # store data\n",
    "          to_append = pd.merge(ppi, pooled, on=['x','y'], how='outer')\n",
    "          to_append[\"n\"] = n\n",
    "          to_append[\"N\"] = N\n",
    "          \n",
    "          result = pd.concat([result, to_append], ignore_index=True)\n",
    "          del ppi \n",
    "          del pooled \n",
    "          del to_append\n",
    "\n",
    "          \n",
    "          \n",
    "\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>pointest_ppi</th>\n",
       "      <th>conf_low_ppi</th>\n",
       "      <th>conf_high_ppi</th>\n",
       "      <th>conf_low_ols</th>\n",
       "      <th>conf_high_ols</th>\n",
       "      <th>pointest_pooled</th>\n",
       "      <th>conf_low_pooled</th>\n",
       "      <th>conf_high_pooled</th>\n",
       "      <th>n</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.203069</td>\n",
       "      <td>0.026080</td>\n",
       "      <td>0.380186</td>\n",
       "      <td>0.020713</td>\n",
       "      <td>0.434234</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>-0.078142</td>\n",
       "      <td>0.158387</td>\n",
       "      <td>50</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.178846</td>\n",
       "      <td>-0.011135</td>\n",
       "      <td>0.368847</td>\n",
       "      <td>-0.032140</td>\n",
       "      <td>0.406376</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>-0.096376</td>\n",
       "      <td>0.144861</td>\n",
       "      <td>50</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>-0.177940</td>\n",
       "      <td>-0.358822</td>\n",
       "      <td>0.048750</td>\n",
       "      <td>-0.172084</td>\n",
       "      <td>0.283905</td>\n",
       "      <td>0.137596</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>50</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>-0.157201</td>\n",
       "      <td>-0.378616</td>\n",
       "      <td>0.071178</td>\n",
       "      <td>-0.358216</td>\n",
       "      <td>0.095088</td>\n",
       "      <td>0.037306</td>\n",
       "      <td>-0.081922</td>\n",
       "      <td>0.156533</td>\n",
       "      <td>50</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.245935</td>\n",
       "      <td>0.045498</td>\n",
       "      <td>0.441965</td>\n",
       "      <td>-0.017246</td>\n",
       "      <td>0.426965</td>\n",
       "      <td>0.115019</td>\n",
       "      <td>-0.004024</td>\n",
       "      <td>0.234061</td>\n",
       "      <td>50</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127195</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.658638</td>\n",
       "      <td>0.595847</td>\n",
       "      <td>0.713198</td>\n",
       "      <td>0.582624</td>\n",
       "      <td>0.699883</td>\n",
       "      <td>0.168725</td>\n",
       "      <td>0.136388</td>\n",
       "      <td>0.201062</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127196</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.581234</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.581261</td>\n",
       "      <td>0.693449</td>\n",
       "      <td>0.160544</td>\n",
       "      <td>0.128088</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127197</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.634376</td>\n",
       "      <td>0.574548</td>\n",
       "      <td>0.694134</td>\n",
       "      <td>0.575346</td>\n",
       "      <td>0.694973</td>\n",
       "      <td>0.162831</td>\n",
       "      <td>0.130421</td>\n",
       "      <td>0.195241</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127198</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.699072</td>\n",
       "      <td>0.646053</td>\n",
       "      <td>0.752109</td>\n",
       "      <td>0.646077</td>\n",
       "      <td>0.752117</td>\n",
       "      <td>0.165174</td>\n",
       "      <td>0.132876</td>\n",
       "      <td>0.197473</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127199</th>\n",
       "      <td>gpt35turbo0125_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.646868</td>\n",
       "      <td>0.590401</td>\n",
       "      <td>0.703334</td>\n",
       "      <td>0.590430</td>\n",
       "      <td>0.703305</td>\n",
       "      <td>0.163426</td>\n",
       "      <td>0.130991</td>\n",
       "      <td>0.195860</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127200 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              y             x  pointest_ppi  conf_low_ppi  \\\n",
       "0            gpt4turbo_wp_Saved  Intervention      0.203069      0.026080   \n",
       "1            gpt4turbo_wp_Saved  Intervention      0.178846     -0.011135   \n",
       "2            gpt4turbo_wp_Saved  Intervention     -0.177940     -0.358822   \n",
       "3            gpt4turbo_wp_Saved  Intervention     -0.157201     -0.378616   \n",
       "4            gpt4turbo_wp_Saved  Intervention      0.245935      0.045498   \n",
       "...                         ...           ...           ...           ...   \n",
       "127195  gpt35turbo0125_wp_Saved       Species      0.658638      0.595847   \n",
       "127196  gpt35turbo0125_wp_Saved       Species      0.637255      0.581234   \n",
       "127197  gpt35turbo0125_wp_Saved       Species      0.634376      0.574548   \n",
       "127198  gpt35turbo0125_wp_Saved       Species      0.699072      0.646053   \n",
       "127199  gpt35turbo0125_wp_Saved       Species      0.646868      0.590401   \n",
       "\n",
       "        conf_high_ppi  conf_low_ols  conf_high_ols  pointest_pooled  \\\n",
       "0            0.380186      0.020713       0.434234         0.040123   \n",
       "1            0.368847     -0.032140       0.406376         0.024242   \n",
       "2            0.048750     -0.172084       0.283905         0.137596   \n",
       "3            0.071178     -0.358216       0.095088         0.037306   \n",
       "4            0.441965     -0.017246       0.426965         0.115019   \n",
       "...               ...           ...            ...              ...   \n",
       "127195       0.713198      0.582624       0.699883         0.168725   \n",
       "127196       0.693258      0.581261       0.693449         0.160544   \n",
       "127197       0.694134      0.575346       0.694973         0.162831   \n",
       "127198       0.752109      0.646077       0.752117         0.165174   \n",
       "127199       0.703334      0.590430       0.703305         0.163426   \n",
       "\n",
       "        conf_low_pooled  conf_high_pooled    n     N  \n",
       "0             -0.078142          0.158387   50   250  \n",
       "1             -0.096376          0.144861   50   250  \n",
       "2              0.018970          0.256221   50   250  \n",
       "3             -0.081922          0.156533   50   250  \n",
       "4             -0.004024          0.234061   50   250  \n",
       "...                 ...               ...  ...   ...  \n",
       "127195         0.136388          0.201062  500  3500  \n",
       "127196         0.128088          0.193000  500  3500  \n",
       "127197         0.130421          0.195241  500  3500  \n",
       "127198         0.132876          0.197473  500  3500  \n",
       "127199         0.130991          0.195860  500  3500  \n",
       "\n",
       "[127200 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_csv(\"../Data/6_ResultsPPI.csv.gz\", compression=\"gzip\", index=False)\n",
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
