{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "In this section, we load necessary libraries and define custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install PPI library if needed \n",
    "# %pip install git+https://github.com/Michael-Howes/ppi_py.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "from scipy import stats\n",
    "from ppi_py import ppi_ols_ci, classical_ols_ci, ppi_ols_pointestimate\n",
    "\n",
    "df = pd.read_csv(\"../Data/5_SurveySampleLLM.csv.gz\")\n",
    "\n",
    "Covs = ['PedPed', 'Barrier', 'CrossingSignal', 'NumberOfCharacters',\n",
    "        'DiffNumberOFCharacters', 'LeftHand', 'Man', 'Woman', 'Pregnant',\n",
    "        'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
    "        'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive',\n",
    "        'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor',\n",
    "        'MaleDoctor', 'Dog', 'Cat', \n",
    "        'Intervention'\n",
    "        ]\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of respondents:  55893\n",
      "Number of decisions:  581981\n",
      "Number of NAs in observed dependent variable:  0\n",
      "Number of NAs in predicted dependent variable with GPT4 Turbo:  6694\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of respondents: \", len(df[\"UserID\"].unique()))\n",
    "print(\"Number of decisions: \", len(df[\"ResponseID\"].unique()))\n",
    "print(\"Number of NAs in observed dependent variable: \", df[\"Saved\"].isna().sum())\n",
    "print(\"Number of NAs in predicted dependent variable with GPT4 Turbo: \", df[\"gpt4turbo_wp_Saved_1\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce AMCE from R functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awad et al. (2018) use R to estimate the AMCE for the the conjoint experiment. In this section, we verify that we can obtain the results with our Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcTheoreticalInt(r):\n",
    "    # this function is applied to each row (r)\n",
    "    if r[\"Intervention\"]==0:\n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: p = 0.48\n",
    "            else: p = 0.32\n",
    "            \n",
    "            if r[\"CrossingSignal\"]==0:   p = p * 0.48\n",
    "            elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "            else: p = p * 0.32\n",
    "        else: p = 0.2\n",
    "\n",
    "    else: \n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: \n",
    "                p = 0.48\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.32\n",
    "                else: p = p * 0.2\n",
    "            else: \n",
    "                p = 0.2\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "                else: p = p * 0.32\n",
    "        else: p = 0.32  \n",
    "    \n",
    "    return(p)  \n",
    "        \n",
    "def calcWeightsTheoretical(profiles):\n",
    "    \n",
    "    p = profiles.apply(CalcTheoreticalInt, axis=1)\n",
    "\n",
    "    weight = 1/p \n",
    "\n",
    "    return(weight)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from PPI to calculate stats\n",
    "def _ols_get_stats(\n",
    "    pointest,\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    w=None,\n",
    "    w_unlabeled=None,\n",
    "    use_unlabeled=True,\n",
    "):\n",
    "    \"\"\"Computes the statistics needed for the OLS-based prediction-powered inference.\n",
    "\n",
    "    Args:\n",
    "        pointest (ndarray): A point estimate of the coefficients.\n",
    "        X (ndarray): Covariates for the labeled data set.\n",
    "        Y (ndarray): Labels for the labeled data set.\n",
    "        Yhat (ndarray): Predictions for the labeled data set.\n",
    "        X_unlabeled (ndarray): Covariates for the unlabeled data set.\n",
    "        Yhat_unlabeled (ndarray): Predictions for the unlabeled data set.\n",
    "        w (ndarray, optional): Sample weights for the labeled data set.\n",
    "        w_unlabeled (ndarray, optional): Sample weights for the unlabeled data set.\n",
    "        use_unlabeled (bool, optional): Whether to use the unlabeled data set.\n",
    "\n",
    "    Returns:\n",
    "        grads (ndarray): Gradient of the loss function with respect to the coefficients.\n",
    "        grads_hat (ndarray): Gradient of the loss function with respect to the coefficients, evaluated using the labeled predictions.\n",
    "        grads_hat_unlabeled (ndarray): Gradient of the loss function with respect to the coefficients, evaluated using the unlabeled predictions.\n",
    "        inv_hessian (ndarray): Inverse Hessian of the loss function with respect to the coefficients.\n",
    "    \"\"\"\n",
    "    n = Y.shape[0]\n",
    "    N = Yhat_unlabeled.shape[0]\n",
    "    d = X.shape[1]\n",
    "    w = np.ones(n) if w is None else w / np.sum(w) * n\n",
    "    w_unlabeled = (\n",
    "        np.ones(N)\n",
    "        if w_unlabeled is None\n",
    "        else w_unlabeled / np.sum(w_unlabeled) * N\n",
    "    )\n",
    "\n",
    "    hessian = np.zeros((d, d))\n",
    "    grads_hat_unlabeled = np.zeros(X_unlabeled.shape)\n",
    "    if use_unlabeled:\n",
    "        for i in range(N):\n",
    "            hessian += (\n",
    "                w_unlabeled[i]\n",
    "                / (N + n)\n",
    "                * np.outer(X_unlabeled[i], X_unlabeled[i])\n",
    "            )\n",
    "            grads_hat_unlabeled[i, :] = (\n",
    "                w_unlabeled[i]\n",
    "                * X_unlabeled[i, :]\n",
    "                * (np.dot(X_unlabeled[i, :], pointest) - Yhat_unlabeled[i])\n",
    "            )\n",
    "\n",
    "    grads = np.zeros(X.shape)\n",
    "    grads_hat = np.zeros(X.shape)\n",
    "    for i in range(n):\n",
    "        hessian += (\n",
    "            w[i] / (N + n) * np.outer(X[i], X[i])\n",
    "            if use_unlabeled\n",
    "            else w[i] / n * np.outer(X[i], X[i])\n",
    "        )\n",
    "        grads[i, :] = w[i] * X[i, :] * (np.dot(X[i, :], pointest) - Y[i])\n",
    "        grads_hat[i, :] = (\n",
    "            w[i] * X[i, :] * (np.dot(X[i, :], pointest) - Yhat[i])\n",
    "        )\n",
    "\n",
    "    inv_hessian = np.linalg.inv(hessian).reshape(d, d)\n",
    "    return grads, grads_hat, grads_hat_unlabeled, inv_hessian\n",
    "\n",
    "def _power_analysis_stats(grads, grads_hat, inv_hessian):\n",
    "    grads_ = grads - grads.mean(axis=0)\n",
    "    grads_hat_ = grads_hat - grads_hat.mean(axis=0)\n",
    "    cov = inv_hessian @ (grads_[:,None,:] * grads_hat_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    var = inv_hessian @ (grads_[:,None,:]*grads_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    var_hat = inv_hessian @ (grads_hat_[:,None,:]*grads_hat_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    rhos_sq = np.diag(cov)**2/(np.diag(var)*np.diag(var_hat))\n",
    "    sigmas_sq = np.diag(var)\n",
    "    return rhos_sq, sigmas_sq\n",
    "\n",
    "def _estimate_ppi_SE(n, N, rho_sq, var_Y):\n",
    "    if N == np.inf:\n",
    "        return np.sqrt(var_Y*(1-rho_sq)/n)\n",
    "    if N == 0:\n",
    "        return np.sqrt(var_Y/n)\n",
    "    var_ppi = var_Y*(1-rho_sq*N/(n+N))/n\n",
    "    return np.sqrt(var_ppi)\n",
    "\n",
    "def _estimate_classical_SE(n, var_Y):\n",
    "    return np.sqrt(var_Y/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a function to compute the Average Marginal Component Effect (AMCE) for an attribute of the moral dilemmas using  weighted least squares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amce(data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        data.loc[:,\"weights\"] = calcWeightsTheoretical(data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        X = dd[\"Intervention\"]\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]==0) & (data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        X = dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        X = 1 - X\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]!=0) & (data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        X = dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        X = 2 - X \n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Utilitarian\") & (data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        X = (dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Species\") & (data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        X = (dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Gender\") & (data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        X = (dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Fitness\") & (data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        X = (dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Age\") & (data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        X = (dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Social Status\") & (data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        X = (dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "\n",
    "    # fit model and extract estimates\n",
    "    fit = model.fit(cov_type = 'cluster', cov_kwds = {'groups': dd[\"UserID\"]})\n",
    "    coef = fit.params[x]\n",
    "    se = fit.bse[x]\n",
    "    ci = fit.conf_int(alpha=alpha).loc[x]\n",
    "\n",
    "    # store results\n",
    "    res = pd.DataFrame({\n",
    "        'x': [x],\n",
    "        'y': [y],\n",
    "        'beta': [coef],\n",
    "        'se': [se],\n",
    "        'lower': [ci[0]],\n",
    "        'upper': [ci[1]]\n",
    "    })\n",
    "\n",
    "    return(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compute the AMCEs only with data from human subjects using the functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>beta</th>\n",
       "      <th>se</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intervention</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrier</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gender</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fitness</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Social Status</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Species</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x      y   beta     se  lower  upper\n",
       "0    Intervention  Saved  0.081  0.002  0.078  0.084\n",
       "0         Barrier  Saved  0.105  0.003  0.100  0.111\n",
       "0          Gender  Saved  0.135  0.003  0.129  0.142\n",
       "0         Fitness  Saved  0.176  0.004  0.169  0.183\n",
       "0   Social Status  Saved  0.240  0.009  0.221  0.258\n",
       "0  CrossingSignal  Saved  0.377  0.003  0.372  0.383\n",
       "0             Age  Saved  0.508  0.003  0.502  0.514\n",
       "0     Utilitarian  Saved  0.571  0.003  0.565  0.576\n",
       "0         Species  Saved  0.684  0.003  0.678  0.689"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amce_human_subjects = pd.concat([\n",
    "    compute_amce(df, x=\"Intervention\", y=\"Saved\"), \n",
    "    compute_amce(df, x=\"Barrier\", y=\"Saved\"), \n",
    "    compute_amce(df, x=\"Gender\", y=\"Saved\"), \n",
    "    compute_amce(df, x=\"Fitness\", y=\"Saved\"), \n",
    "    compute_amce(df, x=\"Social Status\", y=\"Saved\"), \n",
    "    compute_amce(df, x=\"CrossingSignal\",y=\"Saved\"),\n",
    "    compute_amce(df, x=\"Age\", y=\"Saved\"),\n",
    "    compute_amce(df, x=\"Utilitarian\", y=\"Saved\"),\n",
    "    compute_amce(df, x=\"Species\", y=\"Saved\")\n",
    "])      \n",
    "amce_human_subjects.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AMCE estimates above are the same as those calculated with the functions by Awad et al. (2018), see object `main.Saved` in the R script `8_CalculateAMCE.R`. Hence, the custom functions defined in this notebook give the same results as the functions defined in the original article. \n",
    "\n",
    "\n",
    "|           label            |    dv  |  amce |   se  | conf.low | conf.high |\n",
    "|----------------------------|--------|-------|-------|----------|-----------|\n",
    "|   Intervention             | Saved  | 0.068 | 0.008 |    0.052 |     0.084 |\n",
    "|        Barrier             | Saved  | 0.165 | 0.014 |    0.137 |     0.193 |\n",
    "|            Law             | Saved  | 0.336 | 0.015 |    0.307 |     0.366 |\n",
    "|         Gender             | Saved  | 0.160 | 0.017 |    0.127 |     0.193 |\n",
    "|        Fitness             | Saved  | 0.121 | 0.018 |    0.085 |     0.156 |\n",
    "|  Social Status             | Saved  | 0.171 | 0.047 |    0.079 |     0.263 |\n",
    "|            Age             | Saved  | 0.482 | 0.016 |    0.451 |     0.513 |\n",
    "| No. Characters             | Saved  | 0.573 | 0.014 |    0.545 |     0.602 |\n",
    "|        Species             | Saved  | 0.646 | 0.015 |    0.617 |     0.675 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amce_ppi(n_data, N_data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        n_data.loc[:,\"weights\"] = calcWeightsTheoretical(n_data)\n",
    "        N_data.loc[:,\"weights\"] = calcWeightsTheoretical(N_data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data.dropna(subset=y)\n",
    "        N_dd = N_data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        n_X = n_dd[\"Intervention\"]               \n",
    "        N_X = N_dd[\"Intervention\"]\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]==0) & (n_data[\"PedPed\"]==0), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]==0) & (N_data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        n_X = n_dd[\"Barrier\"]\n",
    "        N_X = N_dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        n_X = 1 - n_X\n",
    "        N_X = 1 - N_X\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]!=0) & (n_data[\"PedPed\"]==1), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]!=0) & (N_data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        n_X = n_dd[\"CrossingSignal\"]\n",
    "        N_X = N_dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        n_X = 2 - n_X \n",
    "        N_X = 2 - N_X \n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "    \n",
    "\n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Utilitarian\") & (n_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Utilitarian\") & (N_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        n_X = (n_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Species\") & (n_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Species\") & (N_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        n_X = (n_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Gender\") & (n_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Gender\") & (N_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        n_X = (n_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Fitness\") & (n_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Fitness\") & (N_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        n_X = (n_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Age\") & (n_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Age\") & (N_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        n_X = (n_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Social Status\") & (n_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Social Status\") & (N_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        n_X = (n_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd.loc[:,\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd.loc[:,y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd.loc[:,\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()                # predicted outcomes\n",
    "        N_weights = N_dd.loc[:,\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    # calculate point estimate\n",
    "    beta_ppi = ppi_ols_pointestimate(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                     X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                     w=n_weights, w_unlabeled=N_weights)\n",
    "    \n",
    "    # using ppi function to calculate point estimates (lambda=0)\n",
    "    beta_hum = ppi_ols_pointestimate(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                     X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                     w=n_weights, w_unlabeled=N_weights, \n",
    "                                     lam=0)\n",
    "    \n",
    "    beta_sil = ppi_ols_pointestimate(X=N_X, Y=N_Y_silicon, Yhat=N_Y_silicon, \n",
    "                                     X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                     w=N_weights, w_unlabeled=N_weights, \n",
    "                                     lam=0)\n",
    "    \n",
    "    # using statsmodels to calculate point estimates (same results as with PPI)\n",
    "    beta_hum_sm = sm.WLS(endog=n_Y_human, exog=n_X, weights=n_weights).fit().params[1]\n",
    "    beta_sil_sm = sm.WLS(endog=N_Y_silicon, exog=N_X, weights=N_weights).fit().params[1]\n",
    "\n",
    "    # calculate confidence intervals for PPI, human subjects, and silicon subjects\n",
    "    lower_CI_ppi, upper_CI_ppi = ppi_ols_ci(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                            X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                            w=n_weights, w_unlabeled=N_weights, alpha=alpha)\n",
    "    \n",
    "    lower_CI_hum, upper_CI_hum = classical_ols_ci(X=n_X, Y=n_Y_human, w=n_weights, alpha=alpha)\n",
    "\n",
    "    lower_CI_sil, upper_CI_sil = classical_ols_ci(X=N_X, Y=N_Y_silicon, w=N_weights, alpha=alpha)\n",
    "\n",
    "\n",
    "    # zscore for two tailed test\n",
    "    z = stats.norm.ppf(0.975)\n",
    "    \n",
    "    # calculate standard errors for PPI, human subjects, and silicon subjects\n",
    "    se_ppi = (upper_CI_ppi[1] - lower_CI_ppi[1]) / (2 * z)\n",
    "    \n",
    "    se_hum = (upper_CI_hum[1] - lower_CI_hum[1]) / (2 * z)\n",
    "\n",
    "    se_sil = (upper_CI_sil[1] - lower_CI_sil[1]) / (2 * z)\n",
    "    \n",
    "\n",
    "    # calculate rho\n",
    "    beta = sm.WLS(n_Y_human, n_X, weights=n_weights).fit().params\n",
    "\n",
    "    grads, grads_hat, grads_hat_unlabeled, inv_hessian = _ols_get_stats(\n",
    "        pointest=beta, \n",
    "        X=n_X,\n",
    "        Y=n_Y_human,\n",
    "        Yhat= n_Y_silicon,\n",
    "        X_unlabeled=N_X,\n",
    "        Yhat_unlabeled=N_Y_silicon,\n",
    "        w=n_weights,\n",
    "        w_unlabeled=N_weights,\n",
    "        use_unlabeled=False)\n",
    "    \n",
    "    rho_sq, var_y = _power_analysis_stats(grads, grads_hat, inv_hessian)\n",
    "\n",
    "    # create and return the output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        \"y\": y,                              \n",
    "        \"x\": x,                               # Predictor variable (scenario attribute)\n",
    "        \"beta_ppi\": beta_ppi[1],              # PPI point estimate\n",
    "        \"beta_hum\": beta_hum[1],              # Human subjects point estimate\n",
    "        \"beta_hum_sm\": beta_hum_sm,           # Human subjects point estimate (statsmodels)\n",
    "        \"beta_sil\": beta_sil[1],              # Silicon subjects point estimate\n",
    "        \"beta_sil_sm\": beta_sil_sm,           # Silicon subjects point estimate (statsmodels)\n",
    "        \"se_ppi\": se_ppi,                     # PPI standard error\n",
    "        \"se_hum\": se_hum,                     # Human subjects standard error\n",
    "        \"se_sil\": se_sil,                     # Silicon subjects standard error\n",
    "        \"lower_ppi\": lower_CI_ppi[1],         # The lower bound of the PPI confidence interval\n",
    "        \"upper_ppi\": upper_CI_ppi[1],         # The upper bound of the PPI confidence interval\n",
    "        \"lower_hum\": lower_CI_hum[1],         # The lower bound of the human subjects confidence interval\n",
    "        \"upper_hum\": upper_CI_hum[1],         # The upper bound of the human subjects confidence interval\n",
    "        \"lower_sil\": lower_CI_sil[1],         # The lower bound of the silicon subjects confidence interval\n",
    "        \"upper_sil\": upper_CI_sil[1],         # The upper bound of the silicon subjects confidence interval\n",
    "        \"ppi_corr\": np.sqrt(rho_sq[1])},      # The association between predictions and outcomes\n",
    "        index=[0])\n",
    "    \n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gpt4turbo_wp_Saved_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>beta_ppi</th>\n",
       "      <th>beta_hum</th>\n",
       "      <th>beta_hum_sm</th>\n",
       "      <th>beta_sil</th>\n",
       "      <th>beta_sil_sm</th>\n",
       "      <th>se_ppi</th>\n",
       "      <th>se_hum</th>\n",
       "      <th>se_sil</th>\n",
       "      <th>lower_ppi</th>\n",
       "      <th>upper_ppi</th>\n",
       "      <th>lower_hum</th>\n",
       "      <th>upper_hum</th>\n",
       "      <th>lower_sil</th>\n",
       "      <th>upper_sil</th>\n",
       "      <th>ppi_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.088327</td>\n",
       "      <td>0.087635</td>\n",
       "      <td>0.087635</td>\n",
       "      <td>0.085495</td>\n",
       "      <td>0.085495</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.078031</td>\n",
       "      <td>0.098625</td>\n",
       "      <td>0.076681</td>\n",
       "      <td>0.098589</td>\n",
       "      <td>0.083385</td>\n",
       "      <td>0.087606</td>\n",
       "      <td>0.347161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Barrier</td>\n",
       "      <td>0.098643</td>\n",
       "      <td>0.094870</td>\n",
       "      <td>0.094870</td>\n",
       "      <td>0.486976</td>\n",
       "      <td>0.486976</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>0.008437</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.082938</td>\n",
       "      <td>0.114351</td>\n",
       "      <td>0.078333</td>\n",
       "      <td>0.111406</td>\n",
       "      <td>0.484219</td>\n",
       "      <td>0.489734</td>\n",
       "      <td>0.322217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>0.376197</td>\n",
       "      <td>0.385165</td>\n",
       "      <td>0.385165</td>\n",
       "      <td>0.656098</td>\n",
       "      <td>0.656098</td>\n",
       "      <td>0.008870</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.358815</td>\n",
       "      <td>0.393586</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>0.403450</td>\n",
       "      <td>0.653109</td>\n",
       "      <td>0.659087</td>\n",
       "      <td>0.316794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>0.177001</td>\n",
       "      <td>0.181841</td>\n",
       "      <td>0.181841</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.013119</td>\n",
       "      <td>0.013586</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.151250</td>\n",
       "      <td>0.202674</td>\n",
       "      <td>0.155213</td>\n",
       "      <td>0.208468</td>\n",
       "      <td>0.016487</td>\n",
       "      <td>0.026655</td>\n",
       "      <td>0.265618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Gender</td>\n",
       "      <td>0.120163</td>\n",
       "      <td>0.120048</td>\n",
       "      <td>0.120048</td>\n",
       "      <td>0.201523</td>\n",
       "      <td>0.201523</td>\n",
       "      <td>0.012625</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.095419</td>\n",
       "      <td>0.144908</td>\n",
       "      <td>0.094592</td>\n",
       "      <td>0.145504</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.206322</td>\n",
       "      <td>0.239029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Age</td>\n",
       "      <td>0.518358</td>\n",
       "      <td>0.513868</td>\n",
       "      <td>0.513868</td>\n",
       "      <td>0.181983</td>\n",
       "      <td>0.181983</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>0.011708</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.495850</td>\n",
       "      <td>0.540722</td>\n",
       "      <td>0.490921</td>\n",
       "      <td>0.536815</td>\n",
       "      <td>0.177077</td>\n",
       "      <td>0.186888</td>\n",
       "      <td>0.219661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Social Status</td>\n",
       "      <td>0.236123</td>\n",
       "      <td>0.240562</td>\n",
       "      <td>0.240562</td>\n",
       "      <td>0.040554</td>\n",
       "      <td>0.040554</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>0.037505</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>0.164071</td>\n",
       "      <td>0.308130</td>\n",
       "      <td>0.167054</td>\n",
       "      <td>0.314070</td>\n",
       "      <td>0.026890</td>\n",
       "      <td>0.054217</td>\n",
       "      <td>0.204905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>0.557395</td>\n",
       "      <td>0.558782</td>\n",
       "      <td>0.558782</td>\n",
       "      <td>0.554648</td>\n",
       "      <td>0.554648</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>0.010860</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.536496</td>\n",
       "      <td>0.578281</td>\n",
       "      <td>0.537496</td>\n",
       "      <td>0.580067</td>\n",
       "      <td>0.550533</td>\n",
       "      <td>0.558763</td>\n",
       "      <td>0.199685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.671840</td>\n",
       "      <td>0.672469</td>\n",
       "      <td>0.672469</td>\n",
       "      <td>0.844214</td>\n",
       "      <td>0.844214</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.652837</td>\n",
       "      <td>0.690846</td>\n",
       "      <td>0.653452</td>\n",
       "      <td>0.691486</td>\n",
       "      <td>0.841372</td>\n",
       "      <td>0.847055</td>\n",
       "      <td>0.040058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      y               x  beta_ppi  beta_hum  beta_hum_sm  \\\n",
       "0  gpt4turbo_wp_Saved_1    Intervention  0.088327  0.087635     0.087635   \n",
       "1  gpt4turbo_wp_Saved_1         Barrier  0.098643  0.094870     0.094870   \n",
       "5  gpt4turbo_wp_Saved_1  CrossingSignal  0.376197  0.385165     0.385165   \n",
       "3  gpt4turbo_wp_Saved_1         Fitness  0.177001  0.181841     0.181841   \n",
       "2  gpt4turbo_wp_Saved_1          Gender  0.120163  0.120048     0.120048   \n",
       "6  gpt4turbo_wp_Saved_1             Age  0.518358  0.513868     0.513868   \n",
       "4  gpt4turbo_wp_Saved_1   Social Status  0.236123  0.240562     0.240562   \n",
       "7  gpt4turbo_wp_Saved_1     Utilitarian  0.557395  0.558782     0.558782   \n",
       "8  gpt4turbo_wp_Saved_1         Species  0.671840  0.672469     0.672469   \n",
       "\n",
       "   beta_sil  beta_sil_sm    se_ppi    se_hum    se_sil  lower_ppi  upper_ppi  \\\n",
       "0  0.085495     0.085495  0.005254  0.005589  0.001077   0.078031   0.098625   \n",
       "1  0.486976     0.486976  0.008014  0.008437  0.001407   0.082938   0.114351   \n",
       "5  0.656098     0.656098  0.008870  0.009329  0.001525   0.358815   0.393586   \n",
       "3  0.021571     0.021571  0.013119  0.013586  0.002594   0.151250   0.202674   \n",
       "2  0.201523     0.201523  0.012625  0.012988  0.002448   0.095419   0.144908   \n",
       "6  0.181983     0.181983  0.011447  0.011708  0.002503   0.495850   0.540722   \n",
       "4  0.040554     0.040554  0.036750  0.037505  0.006971   0.164071   0.308130   \n",
       "7  0.554648     0.554648  0.010660  0.010860  0.002100   0.536496   0.578281   \n",
       "8  0.844214     0.844214  0.009696  0.009703  0.001450   0.652837   0.690846   \n",
       "\n",
       "   lower_hum  upper_hum  lower_sil  upper_sil  ppi_corr  \n",
       "0   0.076681   0.098589   0.083385   0.087606  0.347161  \n",
       "1   0.078333   0.111406   0.484219   0.489734  0.322217  \n",
       "5   0.366881   0.403450   0.653109   0.659087  0.316794  \n",
       "3   0.155213   0.208468   0.016487   0.026655  0.265618  \n",
       "2   0.094592   0.145504   0.196725   0.206322  0.239029  \n",
       "6   0.490921   0.536815   0.177077   0.186888  0.219661  \n",
       "4   0.167054   0.314070   0.026890   0.054217  0.204905  \n",
       "7   0.537496   0.580067   0.550533   0.558763  0.199685  \n",
       "8   0.653452   0.691486   0.841372   0.847055  0.040058  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = df[\"ResponseID\"].unique()\n",
    "n = 22000\n",
    "N = len(ids) - n\n",
    "random.seed(2024)\n",
    "\n",
    "n_ids = random.sample(ids.tolist(), k=n)\n",
    "N_ids = random.sample(list(set(ids) - set(n_ids)), k=N)\n",
    "\n",
    "df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "models = [\"gpt4turbo_wp_Saved_1\"]\n",
    "\n",
    "results2 = pd.DataFrame()\n",
    "for model in models: \n",
    "    \n",
    "    print(\"Model: \", model)\n",
    "    results1 = pd.concat([\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Intervention\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Barrier\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Gender\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Fitness\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Social Status\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"CrossingSignal\",y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Age\", y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Utilitarian\", y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Species\", y=model)\n",
    "    ],ignore_index=True)\n",
    "    \n",
    "    results2 = pd.concat([results2, results1],ignore_index=True).sort_values(by=[\"y\",\"ppi_corr\"], ascending=False)\n",
    "    \n",
    "results2.to_csv(\"../Data/7_rho.csv\", index=False)\n",
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we vary the number of human subjects and silicon subjects in a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating predictions from the model: gpt4turbo_wp_Saved_1\n",
      "    Predictor: Intervention\n",
      "        Human sample size: 500\n",
      "    Predictor: Barrier\n",
      "        Human sample size: 500\n",
      "    Predictor: CrossingSignal\n",
      "        Human sample size: 500\n",
      "    Predictor: Gender\n",
      "        Human sample size: 500\n",
      "    Predictor: Fitness\n",
      "        Human sample size: 500\n",
      "    Predictor: Social Status\n",
      "        Human sample size: 500\n",
      "    Predictor: Age\n",
      "        Human sample size: 500\n",
      "    Predictor: Utilitarian\n",
      "        Human sample size: 500\n",
      "    Predictor: Species\n",
      "        Human sample size: 500\n"
     ]
    }
   ],
   "source": [
    "# sample size of human subjects\n",
    "ns = [500,750]\n",
    "ns= [500]\n",
    "\n",
    "# multiples of human subjects sample size\n",
    "ks = list([0.1, 0.25, 0.5, 0.75]) + list(np.arange(1, 10.5, 0.5))\n",
    "\n",
    "# number of repetitions for combinations of n and N\n",
    "reps = 50\n",
    "\n",
    "# LLM predictions\n",
    "Ys = models\n",
    "Ys = [\"gpt4turbo_wp_Saved_1\"]\n",
    "\n",
    "# structural attributes of scenarios\n",
    "Xs_structural  = ['Intervention', 'Barrier','CrossingSignal']\n",
    "\n",
    "# attributes of characters\n",
    "Xs_characters = ['Gender','Fitness','Social Status','Age','Utilitarian','Species']\n",
    "\n",
    "# all attributes\n",
    "Xs = Xs_structural + Xs_characters\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "# loop models\n",
    "for y in Ys:\n",
    "  \n",
    "  print(f\"Iterating predictions from the model: {y}\")\n",
    "  \n",
    "  # loop over predictors\n",
    "  for x in Xs:\n",
    "    print(f\"    Predictor: {x}\")\n",
    "\n",
    "    # loop over sample sizes of human subjects\n",
    "    for n in ns:\n",
    "      print(f\"        Human sample size: {n}\")\n",
    "\n",
    "      # sample size silicon subjects \n",
    "      Ns = [int(n * k) for n in ns for k in ks]\n",
    "      \n",
    "      # loop over sample sizes of silicon subjects\n",
    "      for N in Ns:\n",
    "        \n",
    "        # loop over repetitions\n",
    "        for r in range(reps):\n",
    "\n",
    "          # subset to dilemmas with variation on structural attribute\n",
    "          if x in Xs_structural:\n",
    "\n",
    "              cnt = df.groupby(\"ResponseID\")[x].nunique()\n",
    "              ids = cnt[ cnt > 1].index.tolist()\n",
    "\n",
    "          # subset to dilemmas with relevant character attribute\n",
    "          if x in Xs_characters:\n",
    "\n",
    "              ids = df.loc[ (df[\"ScenarioType\"]==x) & (df[\"ScenarioTypeStrict\"]==x), \"ResponseID\"].tolist()\n",
    "          \n",
    "          # skip current iteration if target n is larger than population\n",
    "          if (len(ids) < n):\n",
    "             continue \n",
    "\n",
    "          # sample dilemmas for human subjects sample\n",
    "          n_ids = random.sample(ids, k=n)\n",
    "          \n",
    "          # get remaining dilemma ids to sample from\n",
    "          remaining_ids = list(set(ids) - set(n_ids))\n",
    "\n",
    "          # skip current iteration if target N is larger than population\n",
    "          if (len(remaining_ids) < N):\n",
    "             continue \n",
    "          \n",
    "          # sample dilemmas for silicon subjects sample\n",
    "          N_ids = random.sample(remaining_ids, k=N)\n",
    "\n",
    "          # subset data\n",
    "          df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "          df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "          # compute acme on n human subjects and N silicon subjects\n",
    "          ppi = compute_amce_ppi(n_data=df_human, N_data=df_silicon, x=x, y=y)\n",
    "\n",
    "          # store data\n",
    "          ppi[\"n\"] = n\n",
    "          ppi[\"N\"] = N\n",
    "          \n",
    "          result = pd.concat([result, ppi], ignore_index=True)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We benchmark the silicon subjects design and the mixed subjects design against a human subjects approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>beta_ppi</th>\n",
       "      <th>beta_hum</th>\n",
       "      <th>beta_hum_sm</th>\n",
       "      <th>beta_sil</th>\n",
       "      <th>beta_sil_sm</th>\n",
       "      <th>se_ppi</th>\n",
       "      <th>se_hum</th>\n",
       "      <th>se_sil</th>\n",
       "      <th>...</th>\n",
       "      <th>upper_hum</th>\n",
       "      <th>lower_sil</th>\n",
       "      <th>upper_sil</th>\n",
       "      <th>ppi_corr</th>\n",
       "      <th>n</th>\n",
       "      <th>N</th>\n",
       "      <th>param</th>\n",
       "      <th>coverage_ppi</th>\n",
       "      <th>coverage_sil</th>\n",
       "      <th>coverage_hum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.058802</td>\n",
       "      <td>0.054994</td>\n",
       "      <td>0.054994</td>\n",
       "      <td>0.222829</td>\n",
       "      <td>0.222829</td>\n",
       "      <td>0.036640</td>\n",
       "      <td>0.036926</td>\n",
       "      <td>0.106417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127368</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.431403</td>\n",
       "      <td>0.403999</td>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>0.065392</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.081533</td>\n",
       "      <td>0.086763</td>\n",
       "      <td>0.086763</td>\n",
       "      <td>0.016593</td>\n",
       "      <td>0.016593</td>\n",
       "      <td>0.036614</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158973</td>\n",
       "      <td>-0.197238</td>\n",
       "      <td>0.230424</td>\n",
       "      <td>0.365939</td>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>0.065392</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.113304</td>\n",
       "      <td>0.113304</td>\n",
       "      <td>0.206762</td>\n",
       "      <td>0.206762</td>\n",
       "      <td>0.036811</td>\n",
       "      <td>0.037042</td>\n",
       "      <td>0.108244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185905</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>0.418916</td>\n",
       "      <td>0.342911</td>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>0.065392</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.121787</td>\n",
       "      <td>0.124721</td>\n",
       "      <td>0.124721</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>0.036348</td>\n",
       "      <td>0.036486</td>\n",
       "      <td>0.116379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196233</td>\n",
       "      <td>-0.221415</td>\n",
       "      <td>0.234782</td>\n",
       "      <td>0.300395</td>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>0.065392</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.080476</td>\n",
       "      <td>0.075860</td>\n",
       "      <td>0.075860</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>0.036433</td>\n",
       "      <td>0.036719</td>\n",
       "      <td>0.115225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147829</td>\n",
       "      <td>-0.096443</td>\n",
       "      <td>0.355232</td>\n",
       "      <td>0.429904</td>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>0.065392</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7645</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.610867</td>\n",
       "      <td>0.612326</td>\n",
       "      <td>0.612326</td>\n",
       "      <td>0.838337</td>\n",
       "      <td>0.838337</td>\n",
       "      <td>0.030345</td>\n",
       "      <td>0.030329</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671770</td>\n",
       "      <td>0.822749</td>\n",
       "      <td>0.853926</td>\n",
       "      <td>0.074048</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.632202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7646</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.660920</td>\n",
       "      <td>0.657357</td>\n",
       "      <td>0.657357</td>\n",
       "      <td>0.846128</td>\n",
       "      <td>0.846128</td>\n",
       "      <td>0.028433</td>\n",
       "      <td>0.028550</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713314</td>\n",
       "      <td>0.830849</td>\n",
       "      <td>0.861407</td>\n",
       "      <td>0.084042</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.632202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7647</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.589789</td>\n",
       "      <td>0.589789</td>\n",
       "      <td>0.589789</td>\n",
       "      <td>0.845596</td>\n",
       "      <td>0.845596</td>\n",
       "      <td>0.031758</td>\n",
       "      <td>0.031742</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652002</td>\n",
       "      <td>0.830344</td>\n",
       "      <td>0.860848</td>\n",
       "      <td>0.023384</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.632202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7648</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.626570</td>\n",
       "      <td>0.623711</td>\n",
       "      <td>0.623711</td>\n",
       "      <td>0.848598</td>\n",
       "      <td>0.848598</td>\n",
       "      <td>0.030143</td>\n",
       "      <td>0.030161</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682826</td>\n",
       "      <td>0.833516</td>\n",
       "      <td>0.863681</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.632202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7649</th>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.643804</td>\n",
       "      <td>0.643984</td>\n",
       "      <td>0.643984</td>\n",
       "      <td>0.839054</td>\n",
       "      <td>0.839054</td>\n",
       "      <td>0.028636</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700080</td>\n",
       "      <td>0.823504</td>\n",
       "      <td>0.854603</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.632202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7650 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         y             x  beta_ppi  beta_hum  beta_hum_sm  \\\n",
       "0     gpt4turbo_wp_Saved_1  Intervention  0.058802  0.054994     0.054994   \n",
       "1     gpt4turbo_wp_Saved_1  Intervention  0.081533  0.086763     0.086763   \n",
       "2     gpt4turbo_wp_Saved_1  Intervention  0.118200  0.113304     0.113304   \n",
       "3     gpt4turbo_wp_Saved_1  Intervention  0.121787  0.124721     0.124721   \n",
       "4     gpt4turbo_wp_Saved_1  Intervention  0.080476  0.075860     0.075860   \n",
       "...                    ...           ...       ...       ...          ...   \n",
       "7645  gpt4turbo_wp_Saved_1       Species  0.610867  0.612326     0.612326   \n",
       "7646  gpt4turbo_wp_Saved_1       Species  0.660920  0.657357     0.657357   \n",
       "7647  gpt4turbo_wp_Saved_1       Species  0.589789  0.589789     0.589789   \n",
       "7648  gpt4turbo_wp_Saved_1       Species  0.626570  0.623711     0.623711   \n",
       "7649  gpt4turbo_wp_Saved_1       Species  0.643804  0.643984     0.643984   \n",
       "\n",
       "      beta_sil  beta_sil_sm    se_ppi    se_hum    se_sil  ...  upper_hum  \\\n",
       "0     0.222829     0.222829  0.036640  0.036926  0.106417  ...   0.127368   \n",
       "1     0.016593     0.016593  0.036614  0.036842  0.109100  ...   0.158973   \n",
       "2     0.206762     0.206762  0.036811  0.037042  0.108244  ...   0.185905   \n",
       "3     0.006683     0.006683  0.036348  0.036486  0.116379  ...   0.196233   \n",
       "4     0.129394     0.129394  0.036433  0.036719  0.115225  ...   0.147829   \n",
       "...        ...          ...       ...       ...       ...  ...        ...   \n",
       "7645  0.838337     0.838337  0.030345  0.030329  0.007954  ...   0.671770   \n",
       "7646  0.846128     0.846128  0.028433  0.028550  0.007795  ...   0.713314   \n",
       "7647  0.845596     0.845596  0.031758  0.031742  0.007782  ...   0.652002   \n",
       "7648  0.848598     0.848598  0.030143  0.030161  0.007695  ...   0.682826   \n",
       "7649  0.839054     0.839054  0.028636  0.028621  0.007934  ...   0.700080   \n",
       "\n",
       "      lower_sil  upper_sil  ppi_corr    n     N     param  coverage_ppi  \\\n",
       "0      0.014255   0.431403  0.403999  500    50  0.065392             1   \n",
       "1     -0.197238   0.230424  0.365939  500    50  0.065392             1   \n",
       "2     -0.005393   0.418916  0.342911  500    50  0.065392             1   \n",
       "3     -0.221415   0.234782  0.300395  500    50  0.065392             1   \n",
       "4     -0.096443   0.355232  0.429904  500    50  0.065392             1   \n",
       "...         ...        ...       ...  ...   ...       ...           ...   \n",
       "7645   0.822749   0.853926  0.074048  500  3500  0.632202             1   \n",
       "7646   0.830849   0.861407  0.084042  500  3500  0.632202             1   \n",
       "7647   0.830344   0.860848  0.023384  500  3500  0.632202             1   \n",
       "7648   0.833516   0.863681  0.049116  500  3500  0.632202             1   \n",
       "7649   0.823504   0.854603  0.003215  500  3500  0.632202             1   \n",
       "\n",
       "      coverage_sil  coverage_hum  \n",
       "0                1             1  \n",
       "1                1             1  \n",
       "2                1             1  \n",
       "3                1             1  \n",
       "4                1             1  \n",
       "...            ...           ...  \n",
       "7645             0             1  \n",
       "7646             0             1  \n",
       "7647             0             1  \n",
       "7648             0             1  \n",
       "7649             0             1  \n",
       "\n",
       "[7650 rows x 23 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset point estimates of AMCEs from the entire human subjects sample\n",
    "benchmark = amce_human_subjects.loc[:, ['x', 'beta']].rename(columns={'beta': 'param'})\n",
    "\n",
    "# merge benchmark with results from simulation\n",
    "result_wb = pd.merge(result, benchmark, on='x', how='left')\n",
    "\n",
    "# report if true value is within the confidence interval from the mixed subjects \n",
    "result_wb['coverage_ppi'] = (\n",
    "    (result_wb['lower_ppi'] <= result_wb['param']) & \n",
    "    (result_wb['param'] <= result_wb['upper_ppi'])\n",
    ").astype(int) \n",
    "\n",
    "# report if true value is within the confidence interval from the silicon subjects \n",
    "result_wb['coverage_sil'] = (\n",
    "    (result_wb['lower_sil'] <= result_wb['param']) & \n",
    "    (result_wb['param'] <= result_wb['upper_sil'])\n",
    ").astype(int) \n",
    "\n",
    "# report if true value is within the confidence interval from the silicon subjects \n",
    "result_wb['coverage_hum'] = (\n",
    "    (result_wb['lower_hum'] <= result_wb['param']) & \n",
    "    (result_wb['param'] <= result_wb['upper_hum'])\n",
    ").astype(int) \n",
    "\n",
    "result_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>N</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>param</th>\n",
       "      <th>beta_ppi</th>\n",
       "      <th>se_ppi</th>\n",
       "      <th>lower_ppi</th>\n",
       "      <th>upper_ppi</th>\n",
       "      <th>coverage_ppi</th>\n",
       "      <th>...</th>\n",
       "      <th>lower_hum</th>\n",
       "      <th>upper_hum</th>\n",
       "      <th>coverage_hum</th>\n",
       "      <th>repetitions</th>\n",
       "      <th>bias_ppi</th>\n",
       "      <th>bias_sil</th>\n",
       "      <th>bias_hum</th>\n",
       "      <th>rmse_ppi</th>\n",
       "      <th>rmse_sil</th>\n",
       "      <th>rmse_hum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Age</td>\n",
       "      <td>0.494434</td>\n",
       "      <td>0.498906</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.433780</td>\n",
       "      <td>0.563136</td>\n",
       "      <td>0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432631</td>\n",
       "      <td>0.562199</td>\n",
       "      <td>0.80</td>\n",
       "      <td>50</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>-0.309638</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.328346</td>\n",
       "      <td>0.033188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Barrier</td>\n",
       "      <td>0.150271</td>\n",
       "      <td>0.164233</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.083722</td>\n",
       "      <td>0.244815</td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082326</td>\n",
       "      <td>0.244227</td>\n",
       "      <td>0.94</td>\n",
       "      <td>50</td>\n",
       "      <td>0.013962</td>\n",
       "      <td>0.351512</td>\n",
       "      <td>0.013006</td>\n",
       "      <td>0.043403</td>\n",
       "      <td>0.367377</td>\n",
       "      <td>0.043301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>0.339410</td>\n",
       "      <td>0.343985</td>\n",
       "      <td>0.046715</td>\n",
       "      <td>0.252480</td>\n",
       "      <td>0.435601</td>\n",
       "      <td>0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251228</td>\n",
       "      <td>0.434949</td>\n",
       "      <td>0.82</td>\n",
       "      <td>50</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.345429</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.046939</td>\n",
       "      <td>0.362781</td>\n",
       "      <td>0.047012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>0.130361</td>\n",
       "      <td>0.119855</td>\n",
       "      <td>0.037244</td>\n",
       "      <td>0.046906</td>\n",
       "      <td>0.192902</td>\n",
       "      <td>0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047424</td>\n",
       "      <td>0.193992</td>\n",
       "      <td>0.82</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.010507</td>\n",
       "      <td>-0.129311</td>\n",
       "      <td>-0.009653</td>\n",
       "      <td>0.038698</td>\n",
       "      <td>0.169223</td>\n",
       "      <td>0.038617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Gender</td>\n",
       "      <td>0.155021</td>\n",
       "      <td>0.144870</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.216847</td>\n",
       "      <td>0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073309</td>\n",
       "      <td>0.217922</td>\n",
       "      <td>0.90</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.010151</td>\n",
       "      <td>0.070246</td>\n",
       "      <td>-0.009405</td>\n",
       "      <td>0.038148</td>\n",
       "      <td>0.129350</td>\n",
       "      <td>0.038072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>500</td>\n",
       "      <td>4750</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>0.339410</td>\n",
       "      <td>0.327705</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>0.237868</td>\n",
       "      <td>0.417541</td>\n",
       "      <td>0.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234371</td>\n",
       "      <td>0.420453</td>\n",
       "      <td>0.88</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.011705</td>\n",
       "      <td>0.326303</td>\n",
       "      <td>-0.011998</td>\n",
       "      <td>0.047307</td>\n",
       "      <td>0.326530</td>\n",
       "      <td>0.048964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>500</td>\n",
       "      <td>4750</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.065392</td>\n",
       "      <td>0.068602</td>\n",
       "      <td>0.034713</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.136552</td>\n",
       "      <td>0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005902</td>\n",
       "      <td>0.139112</td>\n",
       "      <td>0.80</td>\n",
       "      <td>50</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.034861</td>\n",
       "      <td>0.025611</td>\n",
       "      <td>0.037014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Barrier</td>\n",
       "      <td>0.150271</td>\n",
       "      <td>0.168721</td>\n",
       "      <td>0.039190</td>\n",
       "      <td>0.091976</td>\n",
       "      <td>0.245596</td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085994</td>\n",
       "      <td>0.247469</td>\n",
       "      <td>0.94</td>\n",
       "      <td>50</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>0.331450</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>0.043315</td>\n",
       "      <td>0.331634</td>\n",
       "      <td>0.044361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>0.339410</td>\n",
       "      <td>0.344361</td>\n",
       "      <td>0.044902</td>\n",
       "      <td>0.256287</td>\n",
       "      <td>0.432301</td>\n",
       "      <td>0.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245883</td>\n",
       "      <td>0.429332</td>\n",
       "      <td>0.86</td>\n",
       "      <td>50</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>0.324124</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>0.045174</td>\n",
       "      <td>0.324343</td>\n",
       "      <td>0.046834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>gpt4turbo_wp_Saved_1</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.065392</td>\n",
       "      <td>0.073911</td>\n",
       "      <td>0.034780</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.142031</td>\n",
       "      <td>0.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.146613</td>\n",
       "      <td>0.82</td>\n",
       "      <td>50</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>0.008777</td>\n",
       "      <td>0.035808</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.037990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n     N                     y               x     param  beta_ppi  \\\n",
       "0    500    50  gpt4turbo_wp_Saved_1             Age  0.494434  0.498906   \n",
       "1    500    50  gpt4turbo_wp_Saved_1         Barrier  0.150271  0.164233   \n",
       "2    500    50  gpt4turbo_wp_Saved_1  CrossingSignal  0.339410  0.343985   \n",
       "3    500    50  gpt4turbo_wp_Saved_1         Fitness  0.130361  0.119855   \n",
       "4    500    50  gpt4turbo_wp_Saved_1          Gender  0.155021  0.144870   \n",
       "..   ...   ...                   ...             ...       ...       ...   \n",
       "148  500  4750  gpt4turbo_wp_Saved_1  CrossingSignal  0.339410  0.327705   \n",
       "149  500  4750  gpt4turbo_wp_Saved_1    Intervention  0.065392  0.068602   \n",
       "150  500  5000  gpt4turbo_wp_Saved_1         Barrier  0.150271  0.168721   \n",
       "151  500  5000  gpt4turbo_wp_Saved_1  CrossingSignal  0.339410  0.344361   \n",
       "152  500  5000  gpt4turbo_wp_Saved_1    Intervention  0.065392  0.073911   \n",
       "\n",
       "       se_ppi  lower_ppi  upper_ppi  coverage_ppi  ...  lower_hum  upper_hum  \\\n",
       "0    0.033000   0.433780   0.563136          0.82  ...   0.432631   0.562199   \n",
       "1    0.041096   0.083722   0.244815          0.92  ...   0.082326   0.244227   \n",
       "2    0.046715   0.252480   0.435601          0.82  ...   0.251228   0.434949   \n",
       "3    0.037244   0.046906   0.192902          0.82  ...   0.047424   0.193992   \n",
       "4    0.036773   0.072700   0.216847          0.90  ...   0.073309   0.217922   \n",
       "..        ...        ...        ...           ...  ...        ...        ...   \n",
       "148  0.045836   0.237868   0.417541          0.88  ...   0.234371   0.420453   \n",
       "149  0.034713   0.000479   0.136552          0.76  ...  -0.005902   0.139112   \n",
       "150  0.039190   0.091976   0.245596          0.92  ...   0.085994   0.247469   \n",
       "151  0.044902   0.256287   0.432301          0.88  ...   0.245883   0.429332   \n",
       "152  0.034780   0.005697   0.142031          0.84  ...   0.001724   0.146613   \n",
       "\n",
       "     coverage_hum  repetitions  bias_ppi  bias_sil  bias_hum  rmse_ppi  \\\n",
       "0            0.80           50  0.004471 -0.309638  0.002980  0.033301   \n",
       "1            0.94           50  0.013962  0.351512  0.013006  0.043403   \n",
       "2            0.82           50  0.004574  0.345429  0.003678  0.046939   \n",
       "3            0.82           50 -0.010507 -0.129311 -0.009653  0.038698   \n",
       "4            0.90           50 -0.010151  0.070246 -0.009405  0.038148   \n",
       "..            ...          ...       ...       ...       ...       ...   \n",
       "148          0.88           50 -0.011705  0.326303 -0.011998  0.047307   \n",
       "149          0.80           50  0.003210  0.022771  0.001213  0.034861   \n",
       "150          0.94           50  0.018450  0.331450  0.016461  0.043315   \n",
       "151          0.86           50  0.004951  0.324124 -0.001803  0.045174   \n",
       "152          0.82           50  0.008519  0.021785  0.008777  0.035808   \n",
       "\n",
       "     rmse_sil  rmse_hum  \n",
       "0    0.328346  0.033188  \n",
       "1    0.367377  0.043301  \n",
       "2    0.362781  0.047012  \n",
       "3    0.169223  0.038617  \n",
       "4    0.129350  0.038072  \n",
       "..        ...       ...  \n",
       "148  0.326530  0.048964  \n",
       "149  0.025611  0.037014  \n",
       "150  0.331634  0.044361  \n",
       "151  0.324343  0.046834  \n",
       "152  0.024607  0.037990  \n",
       "\n",
       "[153 rows x 28 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by n, N, and LLM then calculate mean across repetitions\n",
    "stats = ['beta_ppi','se_ppi','lower_ppi','upper_ppi','coverage_ppi','ppi_corr',\n",
    "         'beta_sil','se_sil','lower_sil','upper_sil','coverage_sil',\n",
    "         'beta_hum','se_hum','lower_hum','upper_hum','coverage_hum']\n",
    "\n",
    "summ = result_wb.groupby(['n','N','y','x','param'])[stats].mean().reset_index()\n",
    "\n",
    "# Calculate bias columns\n",
    "summ['repetitions'] = reps\n",
    "summ['bias_ppi'] = summ['beta_ppi'] - summ['param']\n",
    "summ['bias_sil'] = summ['beta_sil'] - summ['param']\n",
    "summ['bias_hum'] = summ['beta_hum'] - summ['param']\n",
    "\n",
    "summ['rmse_ppi'] = np.sqrt(summ['bias_ppi']**2 + summ['se_ppi']**2)\n",
    "summ['rmse_sil'] = np.sqrt(summ['bias_sil']**2 + summ['se_sil']**2)\n",
    "summ['rmse_hum'] = np.sqrt(summ['bias_hum']**2 + summ['se_hum']**2)\n",
    "\n",
    "# Save averaged simulation results to compressed csv file\n",
    "summ.to_csv(\"../Data/7_ResultsPPI.csv.gz\", compression=\"gzip\", index=False)\n",
    "summ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
