{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "from ppi_py import ppi_ols_ci, classical_ols_ci, ppi_ols_pointestimate\n",
    "\n",
    "df = pd.read_csv(\"../Data/5_SurveySampleLLM.csv.gz\")\n",
    "\n",
    "Covs = ['PedPed', 'Barrier', 'CrossingSignal', 'NumberOfCharacters',\n",
    "        'DiffNumberOFCharacters', 'LeftHand', 'Man', 'Woman', 'Pregnant',\n",
    "        'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
    "        'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive',\n",
    "        'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor',\n",
    "        'MaleDoctor', 'Dog', 'Cat', \n",
    "        'Intervention'\n",
    "        ]\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of respondents:  2097\n",
      "Number of decisions:  22315\n",
      "Number of NAs in observed dependent variable:  0\n",
      "Number of NAs in predicted dependent variable with GPT4 Turbo:  0\n"
     ]
    }
   ],
   "source": [
    "# basic statistics\n",
    "print(\"Number of respondents: \", len(df[\"UserID\"].unique()))\n",
    "print(\"Number of decisions: \", len(df[\"ResponseID\"].unique()))\n",
    "print(\"Number of NAs in observed dependent variable: \", df[\"Saved\"].isna().sum())\n",
    "print(\"Number of NAs in predicted dependent variable with GPT4 Turbo: \", df[\"gpt4turbo_wp_Saved\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate weights for conjoint experiment\n",
    "def CalcTheoreticalInt(r):\n",
    "    # this function is applied to each row (r)\n",
    "    if r[\"Intervention\"]==0:\n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: p = 0.48\n",
    "            else: p = 0.32\n",
    "            \n",
    "            if r[\"CrossingSignal\"]==0:   p = p * 0.48\n",
    "            elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "            else: p = p * 0.32\n",
    "        else: p = 0.2\n",
    "\n",
    "    else: \n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: \n",
    "                p = 0.48\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.32\n",
    "                else: p = p * 0.2\n",
    "            else: \n",
    "                p = 0.2\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "                else: p = p * 0.32\n",
    "        else: p = 0.32  \n",
    "    \n",
    "    return(p)  \n",
    "        \n",
    "def calcWeightsTheoretical(profiles):\n",
    "    \n",
    "    p = profiles.apply(CalcTheoreticalInt, axis=1)\n",
    "\n",
    "    weight = 1/p \n",
    "\n",
    "    return(weight) \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from PPI to calculate stats\n",
    "def _ols_get_stats(\n",
    "    pointest,\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    w=None,\n",
    "    w_unlabeled=None,\n",
    "    use_unlabeled=True,\n",
    "):\n",
    "    \"\"\"Computes the statistics needed for the OLS-based prediction-powered inference.\n",
    "\n",
    "    Args:\n",
    "        pointest (ndarray): A point estimate of the coefficients.\n",
    "        X (ndarray): Covariates for the labeled data set.\n",
    "        Y (ndarray): Labels for the labeled data set.\n",
    "        Yhat (ndarray): Predictions for the labeled data set.\n",
    "        X_unlabeled (ndarray): Covariates for the unlabeled data set.\n",
    "        Yhat_unlabeled (ndarray): Predictions for the unlabeled data set.\n",
    "        w (ndarray, optional): Sample weights for the labeled data set.\n",
    "        w_unlabeled (ndarray, optional): Sample weights for the unlabeled data set.\n",
    "        use_unlabeled (bool, optional): Whether to use the unlabeled data set.\n",
    "\n",
    "    Returns:\n",
    "        grads (ndarray): Gradient of the loss function with respect to the coefficients.\n",
    "        grads_hat (ndarray): Gradient of the loss function with respect to the coefficients, evaluated using the labeled predictions.\n",
    "        grads_hat_unlabeled (ndarray): Gradient of the loss function with respect to the coefficients, evaluated using the unlabeled predictions.\n",
    "        inv_hessian (ndarray): Inverse Hessian of the loss function with respect to the coefficients.\n",
    "    \"\"\"\n",
    "    n = Y.shape[0]\n",
    "    N = Yhat_unlabeled.shape[0]\n",
    "    d = X.shape[1]\n",
    "    w = np.ones(n) if w is None else w / np.sum(w) * n\n",
    "    w_unlabeled = (\n",
    "        np.ones(N)\n",
    "        if w_unlabeled is None\n",
    "        else w_unlabeled / np.sum(w_unlabeled) * N\n",
    "    )\n",
    "\n",
    "    hessian = np.zeros((d, d))\n",
    "    grads_hat_unlabeled = np.zeros(X_unlabeled.shape)\n",
    "    if use_unlabeled:\n",
    "        for i in range(N):\n",
    "            hessian += (\n",
    "                w_unlabeled[i]\n",
    "                / (N + n)\n",
    "                * np.outer(X_unlabeled[i], X_unlabeled[i])\n",
    "            )\n",
    "            grads_hat_unlabeled[i, :] = (\n",
    "                w_unlabeled[i]\n",
    "                * X_unlabeled[i, :]\n",
    "                * (np.dot(X_unlabeled[i, :], pointest) - Yhat_unlabeled[i])\n",
    "            )\n",
    "\n",
    "    grads = np.zeros(X.shape)\n",
    "    grads_hat = np.zeros(X.shape)\n",
    "    for i in range(n):\n",
    "        hessian += (\n",
    "            w[i] / (N + n) * np.outer(X[i], X[i])\n",
    "            if use_unlabeled\n",
    "            else w[i] / n * np.outer(X[i], X[i])\n",
    "        )\n",
    "        grads[i, :] = w[i] * X[i, :] * (np.dot(X[i, :], pointest) - Y[i])\n",
    "        grads_hat[i, :] = (\n",
    "            w[i] * X[i, :] * (np.dot(X[i, :], pointest) - Yhat[i])\n",
    "        )\n",
    "\n",
    "    inv_hessian = np.linalg.inv(hessian).reshape(d, d)\n",
    "    return grads, grads_hat, grads_hat_unlabeled, inv_hessian\n",
    "\n",
    "def _power_analysis_stats(grads, grads_hat, inv_hessian):\n",
    "    grads_ = grads - grads.mean(axis=0)\n",
    "    grads_hat_ = grads_hat - grads_hat.mean(axis=0)\n",
    "    cov = inv_hessian @ (grads_[:,None,:] * grads_hat_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    var = inv_hessian @ (grads_[:,None,:]*grads_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    var_hat = inv_hessian @ (grads_hat_[:,None,:]*grads_hat_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    rhos_sq = np.diag(cov)**2/(np.diag(var)*np.diag(var_hat))\n",
    "    sigmas_sq = np.diag(var)\n",
    "    return rhos_sq, sigmas_sq\n",
    "\n",
    "def _estimate_ppi_SE(n, N, rho_sq, var_Y):\n",
    "    if N == np.inf:\n",
    "        return np.sqrt(var_Y*(1-rho_sq)/n)\n",
    "    if N == 0:\n",
    "        return np.sqrt(var_Y/n)\n",
    "    var_ppi = var_Y*(1-rho_sq*N/(n+N))/n\n",
    "    return np.sqrt(var_ppi)\n",
    "\n",
    "def _estimate_classical_SE(n, var_Y):\n",
    "    return np.sqrt(var_Y/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate amce for intervention\n",
    "def compute_amce(data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        data.loc[:,\"weights\"] = calcWeightsTheoretical(data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        X = dd[\"Intervention\"]\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]==0) & (data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        X = dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        X = 1 - X\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]!=0) & (data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        X = dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        X = 2 - X \n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Utilitarian\") & (data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        X = (dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Species\") & (data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        X = (dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Gender\") & (data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        X = (dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Fitness\") & (data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        X = (dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Age\") & (data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        X = (dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Social Status\") & (data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        X = (dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "\n",
    "    # fit model and extract estimates\n",
    "    fit = model.fit(cov_type = 'cluster', cov_kwds = {'groups': dd[\"UserID\"]})\n",
    "    coef = fit.params[x]\n",
    "    ci = fit.conf_int(alpha=alpha).loc[x]\n",
    "\n",
    "    # store results\n",
    "    res = pd.DataFrame({\n",
    "        'x': [x],\n",
    "        'y': [y],\n",
    "        'pointest_pooled': [coef],\n",
    "        'conf_low_pooled': [ci[0]],\n",
    "        'conf_high_pooled': [ci[1]]\n",
    "    })\n",
    "\n",
    "    return(res)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate amce with ppi \n",
    "def compute_amce_ppi(n_data, N_data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        n_data.loc[:,\"weights\"] = calcWeightsTheoretical(n_data)\n",
    "        N_data.loc[:,\"weights\"] = calcWeightsTheoretical(N_data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data.dropna(subset=y)\n",
    "        N_dd = N_data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        n_X = n_dd[\"Intervention\"]               \n",
    "        N_X = N_dd[\"Intervention\"]\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]==0) & (n_data[\"PedPed\"]==0), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]==0) & (N_data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        n_X = n_dd[\"Barrier\"]\n",
    "        N_X = N_dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        n_X = 1 - n_X\n",
    "        N_X = 1 - N_X\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]!=0) & (n_data[\"PedPed\"]==1), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]!=0) & (N_data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        n_X = n_dd[\"CrossingSignal\"]\n",
    "        N_X = N_dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        n_X = 2 - n_X \n",
    "        N_X = 2 - N_X \n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "    \n",
    "\n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Utilitarian\") & (n_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Utilitarian\") & (N_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        n_X = (n_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Species\") & (n_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Species\") & (N_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        n_X = (n_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Gender\") & (n_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Gender\") & (N_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        n_X = (n_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Fitness\") & (n_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Fitness\") & (N_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        n_X = (n_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Age\") & (n_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Age\") & (N_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        n_X = (n_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Social Status\") & (n_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Social Status\") & (N_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        n_X = (n_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd.loc[:,\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd.loc[:,y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd.loc[:,\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd.loc[:,\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    # calculate point estimate\n",
    "    pointest_ppi = ppi_ols_pointestimate(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                         X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                         w=n_weights, w_unlabeled=N_weights)\n",
    "\n",
    "    # calculate PPI confidence intervals\n",
    "    lower_CI_ppi, upper_CI_ppi = ppi_ols_ci(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                            X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                            w=n_weights, w_unlabeled=N_weights, alpha=alpha)\n",
    "    \n",
    "    # calculate OLS confidence intervals\n",
    "    lower_CI_ols, upper_CI_ols = classical_ols_ci(X=n_X, Y=n_Y_human, w=n_weights, alpha=alpha)\n",
    "\n",
    "    # calculate rho\n",
    "    beta = sm.WLS(n_Y_human, n_X, weights=n_weights).fit().params\n",
    "\n",
    "    grads, grads_hat, grads_hat_unlabeled, inv_hessian = _ols_get_stats(\n",
    "        pointest=beta, \n",
    "        X=n_X,\n",
    "        Y=n_Y_human,\n",
    "        Yhat= n_Y_silicon,\n",
    "        X_unlabeled=N_X,\n",
    "        Yhat_unlabeled=N_Y_silicon,\n",
    "        w=n_weights,\n",
    "        w_unlabeled=N_weights,\n",
    "        use_unlabeled=False)\n",
    "    \n",
    "    rho_sq, var_y = _power_analysis_stats(grads, grads_hat, inv_hessian)\n",
    "\n",
    "    # create and return the output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        \"y\": y,                              \n",
    "        \"x\": x,                              # Predictor variable (scenario attribute)\n",
    "        \"pointest_ppi\": pointest_ppi[1],     # PPI point estimate\n",
    "        \"conf_low_ppi\": lower_CI_ppi[1],     # The lower bound of the PPI confidence interval\n",
    "        \"conf_high_ppi\": upper_CI_ppi[1],    # The upper bound of the PPI confidence interval\n",
    "        \"conf_low_ols\": lower_CI_ols[1],     # The lower bound of the OLS confidence interval\n",
    "        \"conf_high_ols\": upper_CI_ols[1],    # The upper bound of the OLS confidence interval\n",
    "        \"rho\": np.sqrt(rho_sq[1])},          # The association between predictions and outcomes\n",
    "        index=[0])\n",
    "    \n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>pointest_pooled</th>\n",
       "      <th>conf_low_pooled</th>\n",
       "      <th>conf_high_pooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intervention</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.068216</td>\n",
       "      <td>0.052464</td>\n",
       "      <td>0.083969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrier</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.164845</td>\n",
       "      <td>0.136728</td>\n",
       "      <td>0.192962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gender</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.159646</td>\n",
       "      <td>0.126552</td>\n",
       "      <td>0.192741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fitness</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.120809</td>\n",
       "      <td>0.085368</td>\n",
       "      <td>0.156249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Social Status</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.170991</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>0.262948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.336349</td>\n",
       "      <td>0.306916</td>\n",
       "      <td>0.365781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.481846</td>\n",
       "      <td>0.450556</td>\n",
       "      <td>0.513135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.573304</td>\n",
       "      <td>0.545087</td>\n",
       "      <td>0.601520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Species</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.674794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x      y  pointest_pooled  conf_low_pooled  conf_high_pooled\n",
       "0    Intervention  Saved         0.068216         0.052464          0.083969\n",
       "0         Barrier  Saved         0.164845         0.136728          0.192962\n",
       "0          Gender  Saved         0.159646         0.126552          0.192741\n",
       "0         Fitness  Saved         0.120809         0.085368          0.156249\n",
       "0   Social Status  Saved         0.170991         0.079034          0.262948\n",
       "0  CrossingSignal  Saved         0.336349         0.306916          0.365781\n",
       "0             Age  Saved         0.481846         0.450556          0.513135\n",
       "0     Utilitarian  Saved         0.573304         0.545087          0.601520\n",
       "0         Species  Saved         0.645940         0.617086          0.674794"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([compute_amce(df, x=\"Intervention\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Barrier\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Gender\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Fitness\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Social Status\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"CrossingSignal\",y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Age\", y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Utilitarian\", y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Species\", y=\"Saved\")\n",
    "           ])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AMCE estimates computed above are the same as those calculated with the functions by Awad et al. (2018), see R script `7_CalculateAMCE.R`.\n",
    "\n",
    "\n",
    "|           label            |    dv  |  amce |   se  | conf.low | conf.high |\n",
    "|----------------------------|--------|-------|-------|----------|-----------|\n",
    "|   Intervention             | Saved  | 0.068 | 0.008 |    0.052 |     0.084 |\n",
    "|        Barrier             | Saved  | 0.165 | 0.014 |    0.137 |     0.193 |\n",
    "|            Law             | Saved  | 0.336 | 0.015 |    0.307 |     0.366 |\n",
    "|         Gender             | Saved  | 0.160 | 0.017 |    0.127 |     0.193 |\n",
    "|        Fitness             | Saved  | 0.121 | 0.018 |    0.085 |     0.156 |\n",
    "|  Social Status             | Saved  | 0.171 | 0.047 |    0.079 |     0.263 |\n",
    "|            Age             | Saved  | 0.482 | 0.016 |    0.451 |     0.513 |\n",
    "| No. Characters             | Saved  | 0.573 | 0.014 |    0.545 |     0.602 |\n",
    "|        Species             | Saved  | 0.646 | 0.015 |    0.617 |     0.675 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the PPI point estimates with a large sample size of human responses which we expect to be very close to the AMCE estimates obtained by applying classical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gpt4turbo_wp_Saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>pointest_ppi</th>\n",
       "      <th>conf_low_ppi</th>\n",
       "      <th>conf_high_ppi</th>\n",
       "      <th>conf_low_ols</th>\n",
       "      <th>conf_high_ols</th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.068727</td>\n",
       "      <td>0.057839</td>\n",
       "      <td>0.079650</td>\n",
       "      <td>0.058259</td>\n",
       "      <td>0.080076</td>\n",
       "      <td>0.346944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Barrier</td>\n",
       "      <td>0.164488</td>\n",
       "      <td>0.148184</td>\n",
       "      <td>0.180794</td>\n",
       "      <td>0.148194</td>\n",
       "      <td>0.180806</td>\n",
       "      <td>0.313079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Gender</td>\n",
       "      <td>0.158650</td>\n",
       "      <td>0.133778</td>\n",
       "      <td>0.183732</td>\n",
       "      <td>0.134436</td>\n",
       "      <td>0.184396</td>\n",
       "      <td>0.270778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>0.122703</td>\n",
       "      <td>0.095917</td>\n",
       "      <td>0.149050</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>0.148450</td>\n",
       "      <td>0.302953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Social Status</td>\n",
       "      <td>0.170737</td>\n",
       "      <td>0.097780</td>\n",
       "      <td>0.242566</td>\n",
       "      <td>0.097578</td>\n",
       "      <td>0.242294</td>\n",
       "      <td>0.136502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>0.337612</td>\n",
       "      <td>0.318956</td>\n",
       "      <td>0.356269</td>\n",
       "      <td>0.319125</td>\n",
       "      <td>0.356442</td>\n",
       "      <td>0.274261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Age</td>\n",
       "      <td>0.481574</td>\n",
       "      <td>0.458599</td>\n",
       "      <td>0.504549</td>\n",
       "      <td>0.458558</td>\n",
       "      <td>0.504511</td>\n",
       "      <td>0.208462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>0.572184</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.593111</td>\n",
       "      <td>0.550904</td>\n",
       "      <td>0.592918</td>\n",
       "      <td>0.195671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.646907</td>\n",
       "      <td>0.627176</td>\n",
       "      <td>0.666639</td>\n",
       "      <td>0.627161</td>\n",
       "      <td>0.666621</td>\n",
       "      <td>0.048616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    y               x  pointest_ppi  conf_low_ppi  \\\n",
       "0  gpt4turbo_wp_Saved    Intervention      0.068727      0.057839   \n",
       "1  gpt4turbo_wp_Saved         Barrier      0.164488      0.148184   \n",
       "2  gpt4turbo_wp_Saved          Gender      0.158650      0.133778   \n",
       "3  gpt4turbo_wp_Saved         Fitness      0.122703      0.095917   \n",
       "4  gpt4turbo_wp_Saved   Social Status      0.170737      0.097780   \n",
       "5  gpt4turbo_wp_Saved  CrossingSignal      0.337612      0.318956   \n",
       "6  gpt4turbo_wp_Saved             Age      0.481574      0.458599   \n",
       "7  gpt4turbo_wp_Saved     Utilitarian      0.572184      0.551100   \n",
       "8  gpt4turbo_wp_Saved         Species      0.646907      0.627176   \n",
       "\n",
       "   conf_high_ppi  conf_low_ols  conf_high_ols       rho  \n",
       "0       0.079650      0.058259       0.080076  0.346944  \n",
       "1       0.180794      0.148194       0.180806  0.313079  \n",
       "2       0.183732      0.134436       0.184396  0.270778  \n",
       "3       0.149050      0.095310       0.148450  0.302953  \n",
       "4       0.242566      0.097578       0.242294  0.136502  \n",
       "5       0.356269      0.319125       0.356442  0.274261  \n",
       "6       0.504549      0.458558       0.504511  0.208462  \n",
       "7       0.593111      0.550904       0.592918  0.195671  \n",
       "8       0.666639      0.627161       0.666621  0.048616  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = df[\"ResponseID\"].unique()\n",
    "N = 100\n",
    "n = len(ids) - N\n",
    "random.seed(2024)\n",
    "\n",
    "n_ids = random.sample(ids.tolist(), k=n)\n",
    "N_ids = random.sample(list(set(ids) - set(n_ids)), k=N)\n",
    "\n",
    "df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "models = [\"gpt4turbo_wp_Saved\"]#,\"gpt4o_wp_Saved\",\"gpt35turbo0125_wp_Saved\"]\n",
    "\n",
    "results2 = pd.DataFrame()\n",
    "for model in models: \n",
    "    \n",
    "    print(\"Model: \", model)\n",
    "    results1 = pd.concat([\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Intervention\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Barrier\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Gender\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Fitness\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Social Status\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"CrossingSignal\",y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Age\", y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Utilitarian\", y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Species\", y=model)\n",
    "        ],ignore_index=True)\n",
    "    \n",
    "    results2 = pd.concat([results2, results1],ignore_index=True)\n",
    "    \n",
    "results2.to_csv(\"../Data/7_rho.csv\", index=False)\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over dependent variable: gpt4turbo_wp_Saved\n",
      "    Predictor: Intervention with human sample size 100\n",
      "    Predictor: Intervention with human sample size 200\n",
      "    Predictor: Intervention with human sample size 300\n",
      "    Predictor: Intervention with human sample size 500\n",
      "    Predictor: Barrier with human sample size 100\n",
      "    Predictor: Barrier with human sample size 200\n",
      "    Predictor: Barrier with human sample size 300\n",
      "    Predictor: Barrier with human sample size 500\n",
      "    Predictor: CrossingSignal with human sample size 100\n",
      "    Predictor: CrossingSignal with human sample size 200\n",
      "    Predictor: CrossingSignal with human sample size 300\n",
      "    Predictor: CrossingSignal with human sample size 500\n",
      "    Predictor: Gender with human sample size 100\n",
      "    Predictor: Gender with human sample size 200\n",
      "    Predictor: Gender with human sample size 300\n",
      "    Predictor: Gender with human sample size 500\n",
      "    Predictor: Fitness with human sample size 100\n",
      "    Predictor: Fitness with human sample size 200\n",
      "    Predictor: Fitness with human sample size 300\n",
      "    Predictor: Fitness with human sample size 500\n",
      "    Predictor: Social Status with human sample size 100\n",
      "    Predictor: Social Status with human sample size 200\n",
      "    Predictor: Social Status with human sample size 300\n",
      "    Predictor: Social Status with human sample size 500\n",
      "    Predictor: Age with human sample size 100\n",
      "    Predictor: Age with human sample size 200\n",
      "    Predictor: Age with human sample size 300\n",
      "    Predictor: Age with human sample size 500\n",
      "    Predictor: Utilitarian with human sample size 100\n",
      "    Predictor: Utilitarian with human sample size 200\n",
      "    Predictor: Utilitarian with human sample size 300\n",
      "    Predictor: Utilitarian with human sample size 500\n",
      "    Predictor: Species with human sample size 100\n",
      "    Predictor: Species with human sample size 200\n",
      "    Predictor: Species with human sample size 300\n",
      "    Predictor: Species with human sample size 500\n"
     ]
    }
   ],
   "source": [
    "# sample size human subjects\n",
    "ns = [100, 200, 300, 500]\n",
    "\n",
    "# multiples of human subjects sample size\n",
    "ks = list([0.25, 0.5, 0.75]) + list(np.arange(1, 10.5, 0.5))\n",
    "\n",
    "# number of repetitions for combinations of n and N\n",
    "reps = 300\n",
    "\n",
    "# predictions\n",
    "Ys = [\"gpt4turbo_wp_Saved\"]#,\"gpt4o_wp_Saved\",\"gpt35turbo0125_wp_Saved\"]\n",
    "\n",
    "# structural attributes of scenarios\n",
    "Xs_structural  = ['Intervention', 'Barrier','CrossingSignal']\n",
    "\n",
    "# attributes of characters\n",
    "Xs_characters = ['Gender','Fitness','Social Status','Age','Utilitarian','Species']\n",
    "\n",
    "# all attributes\n",
    "Xs = Xs_structural + Xs_characters\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for y in Ys:\n",
    "  print(f\"Iterating over dependent variable: {y}\")\n",
    "  \n",
    "  for x in Xs:\n",
    "\n",
    "    for n in ns:\n",
    "      print(f\"    Predictor: {x} with human sample size {n}\")\n",
    "\n",
    "      # sample size silicon subjects \n",
    "      Ns = [int(n * k) for n in ns for k in ks]\n",
    "      \n",
    "      for N in Ns:\n",
    "\n",
    "        for r in range(reps):\n",
    "\n",
    "          # subset to dilemmas with variation on structural attribute\n",
    "          if x in Xs_structural:\n",
    "\n",
    "              cnt = df.groupby(\"ResponseID\")[x].nunique()\n",
    "              ids = cnt[ cnt > 1].index.tolist()\n",
    "\n",
    "          # subset to dilemmas with relevant character attribute\n",
    "          if x in Xs_characters:\n",
    "\n",
    "              ids = df.loc[ (df[\"ScenarioType\"]==x) & (df[\"ScenarioTypeStrict\"]==x), \"ResponseID\"].tolist()\n",
    "          \n",
    "          # skip current iteration if target n is larger than population\n",
    "          if (len(ids) < n):\n",
    "             continue \n",
    "\n",
    "          # sample dilemmas for human subjects sample\n",
    "          n_ids = random.sample(ids, k=n)\n",
    "          \n",
    "          # get remaining dilemma ids to sample from\n",
    "          remaining_ids = list(set(ids) - set(n_ids))\n",
    "\n",
    "          # skip current iteration if target N is larger than population\n",
    "          if (len(remaining_ids) < N):\n",
    "             continue \n",
    "          \n",
    "          # sample dilemmas for silicon subjects sample\n",
    "          N_ids = random.sample(remaining_ids, k=N)\n",
    "\n",
    "          # subset data\n",
    "          df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "          df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "          # pool human and silicon subject decisions\n",
    "          df_pooled = pd.concat([df_human,df_silicon], ignore_index=True)\n",
    "\n",
    "          # compute ppi acme\n",
    "          ppi = compute_amce_ppi(n_data=df_human, N_data=df_silicon, x=x, y=y)\n",
    "          \n",
    "          # compute acme on pooled data\n",
    "          pooled = compute_amce(data=df_pooled,x=x,y=y)\n",
    "\n",
    "          # store data\n",
    "          to_append = pd.merge(ppi, pooled, on=['x','y'], how='outer')\n",
    "          to_append[\"n\"] = n\n",
    "          to_append[\"N\"] = N\n",
    "          \n",
    "          result = pd.concat([result, to_append], ignore_index=True)\n",
    "          del ppi \n",
    "          del pooled \n",
    "          del to_append\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>pointest_ppi</th>\n",
       "      <th>conf_low_ppi</th>\n",
       "      <th>conf_high_ppi</th>\n",
       "      <th>conf_low_ols</th>\n",
       "      <th>conf_high_ols</th>\n",
       "      <th>rho</th>\n",
       "      <th>pointest_pooled</th>\n",
       "      <th>conf_low_pooled</th>\n",
       "      <th>conf_high_pooled</th>\n",
       "      <th>n</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>-0.012467</td>\n",
       "      <td>-0.169065</td>\n",
       "      <td>0.141834</td>\n",
       "      <td>-0.192798</td>\n",
       "      <td>0.126948</td>\n",
       "      <td>0.605763</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>-0.168990</td>\n",
       "      <td>0.200332</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.177026</td>\n",
       "      <td>0.017979</td>\n",
       "      <td>0.334638</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.326981</td>\n",
       "      <td>0.300923</td>\n",
       "      <td>0.288079</td>\n",
       "      <td>0.110683</td>\n",
       "      <td>0.465475</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.202877</td>\n",
       "      <td>0.046981</td>\n",
       "      <td>0.358296</td>\n",
       "      <td>0.028092</td>\n",
       "      <td>0.342801</td>\n",
       "      <td>0.337780</td>\n",
       "      <td>0.215382</td>\n",
       "      <td>0.040185</td>\n",
       "      <td>0.390580</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>-0.154562</td>\n",
       "      <td>0.161366</td>\n",
       "      <td>-0.153806</td>\n",
       "      <td>0.163140</td>\n",
       "      <td>0.238589</td>\n",
       "      <td>0.104670</td>\n",
       "      <td>-0.081041</td>\n",
       "      <td>0.290381</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.111148</td>\n",
       "      <td>-0.051877</td>\n",
       "      <td>0.274307</td>\n",
       "      <td>-0.049051</td>\n",
       "      <td>0.277744</td>\n",
       "      <td>0.127821</td>\n",
       "      <td>0.163455</td>\n",
       "      <td>-0.022992</td>\n",
       "      <td>0.349902</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829653</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.647765</td>\n",
       "      <td>0.591814</td>\n",
       "      <td>0.703691</td>\n",
       "      <td>0.591833</td>\n",
       "      <td>0.703617</td>\n",
       "      <td>0.009432</td>\n",
       "      <td>0.843668</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.862302</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829654</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.614023</td>\n",
       "      <td>0.553409</td>\n",
       "      <td>0.673597</td>\n",
       "      <td>0.552853</td>\n",
       "      <td>0.673150</td>\n",
       "      <td>0.012170</td>\n",
       "      <td>0.840800</td>\n",
       "      <td>0.821977</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829655</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.662509</td>\n",
       "      <td>0.607187</td>\n",
       "      <td>0.717831</td>\n",
       "      <td>0.607215</td>\n",
       "      <td>0.717803</td>\n",
       "      <td>0.011459</td>\n",
       "      <td>0.841112</td>\n",
       "      <td>0.822289</td>\n",
       "      <td>0.859935</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829656</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.614951</td>\n",
       "      <td>0.554113</td>\n",
       "      <td>0.675789</td>\n",
       "      <td>0.554144</td>\n",
       "      <td>0.675758</td>\n",
       "      <td>0.046603</td>\n",
       "      <td>0.842234</td>\n",
       "      <td>0.823620</td>\n",
       "      <td>0.860848</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829657</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.669497</td>\n",
       "      <td>0.612410</td>\n",
       "      <td>0.725617</td>\n",
       "      <td>0.602505</td>\n",
       "      <td>0.716391</td>\n",
       "      <td>0.138355</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>0.823786</td>\n",
       "      <td>0.861213</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>829658 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         y             x  pointest_ppi  conf_low_ppi  \\\n",
       "0       gpt4turbo_wp_Saved  Intervention     -0.012467     -0.169065   \n",
       "1       gpt4turbo_wp_Saved  Intervention      0.177026      0.017979   \n",
       "2       gpt4turbo_wp_Saved  Intervention      0.202877      0.046981   \n",
       "3       gpt4turbo_wp_Saved  Intervention      0.003399     -0.154562   \n",
       "4       gpt4turbo_wp_Saved  Intervention      0.111148     -0.051877   \n",
       "...                    ...           ...           ...           ...   \n",
       "829653  gpt4turbo_wp_Saved       Species      0.647765      0.591814   \n",
       "829654  gpt4turbo_wp_Saved       Species      0.614023      0.553409   \n",
       "829655  gpt4turbo_wp_Saved       Species      0.662509      0.607187   \n",
       "829656  gpt4turbo_wp_Saved       Species      0.614951      0.554113   \n",
       "829657  gpt4turbo_wp_Saved       Species      0.669497      0.612410   \n",
       "\n",
       "        conf_high_ppi  conf_low_ols  conf_high_ols       rho  pointest_pooled  \\\n",
       "0            0.141834     -0.192798       0.126948  0.605763         0.015671   \n",
       "1            0.334638      0.007065       0.326981  0.300923         0.288079   \n",
       "2            0.358296      0.028092       0.342801  0.337780         0.215382   \n",
       "3            0.161366     -0.153806       0.163140  0.238589         0.104670   \n",
       "4            0.274307     -0.049051       0.277744  0.127821         0.163455   \n",
       "...               ...           ...            ...       ...              ...   \n",
       "829653       0.703691      0.591833       0.703617  0.009432         0.843668   \n",
       "829654       0.673597      0.552853       0.673150  0.012170         0.840800   \n",
       "829655       0.717831      0.607215       0.717803  0.011459         0.841112   \n",
       "829656       0.675789      0.554144       0.675758  0.046603         0.842234   \n",
       "829657       0.725617      0.602505       0.716391  0.138355         0.842500   \n",
       "\n",
       "        conf_low_pooled  conf_high_pooled    n     N  \n",
       "0             -0.168990          0.200332  100    25  \n",
       "1              0.110683          0.465475  100    25  \n",
       "2              0.040185          0.390580  100    25  \n",
       "3             -0.081041          0.290381  100    25  \n",
       "4             -0.022992          0.349902  100    25  \n",
       "...                 ...               ...  ...   ...  \n",
       "829653         0.825034          0.862302  500  3500  \n",
       "829654         0.821977          0.859623  500  3500  \n",
       "829655         0.822289          0.859935  500  3500  \n",
       "829656         0.823620          0.860848  500  3500  \n",
       "829657         0.823786          0.861213  500  3500  \n",
       "\n",
       "[829658 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_csv(\"../Data/7_ResultsPPI.csv.gz\", compression=\"gzip\", index=False)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
