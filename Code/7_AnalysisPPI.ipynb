{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "from ppi_py import ppi_ols_ci, classical_ols_ci, ppi_ols_pointestimate\n",
    "\n",
    "df = pd.read_csv(\"../Data/5_SurveySampleLLM.csv.gz\")\n",
    "\n",
    "Covs = ['PedPed', 'Barrier', 'CrossingSignal', 'NumberOfCharacters',\n",
    "        'DiffNumberOFCharacters', 'LeftHand', 'Man', 'Woman', 'Pregnant',\n",
    "        'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
    "        'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive',\n",
    "        'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor',\n",
    "        'MaleDoctor', 'Dog', 'Cat', \n",
    "        'Intervention'\n",
    "        ]\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAs Saved:  0\n",
      "Number of NAs gpt4turbo_wp_Saved:  0\n",
      "Number of NAs gpt4o_wp_Saved:  6\n",
      "Number of NAs gpt35turbo0125_wp_Saved:  2\n"
     ]
    }
   ],
   "source": [
    "# very few missing predicted values for the dependent variable\n",
    "print(\"Number of NAs Saved: \",df[\"Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt4turbo_wp_Saved: \",df[\"gpt4turbo_wp_Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt4o_wp_Saved: \",df[\"gpt4o_wp_Saved\"].isna().sum())\n",
    "print(\"Number of NAs gpt35turbo0125_wp_Saved: \",df[\"gpt35turbo0125_wp_Saved\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate weights for conjoint experiment\n",
    "def CalcTheoreticalInt(r):\n",
    "    # this function is applied to each row (r)\n",
    "    if r[\"Intervention\"]==0:\n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: p = 0.48\n",
    "            else: p = 0.32\n",
    "            \n",
    "            if r[\"CrossingSignal\"]==0:   p = p * 0.48\n",
    "            elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "            else: p = p * 0.32\n",
    "        else: p = 0.2\n",
    "\n",
    "    else: \n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: \n",
    "                p = 0.48\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.32\n",
    "                else: p = p * 0.2\n",
    "            else: \n",
    "                p = 0.2\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "                else: p = p * 0.32\n",
    "        else: p = 0.32  \n",
    "    \n",
    "    return(p)  \n",
    "        \n",
    "def calcWeightsTheoretical(profiles):\n",
    "    \n",
    "    p = profiles.apply(CalcTheoreticalInt, axis=1)\n",
    "\n",
    "    weight = 1/p \n",
    "\n",
    "    return(weight) \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from PPI to calculate stats\n",
    "def _ols_get_stats(\n",
    "    pointest,\n",
    "    X,\n",
    "    Y,\n",
    "    Yhat,\n",
    "    X_unlabeled,\n",
    "    Yhat_unlabeled,\n",
    "    w=None,\n",
    "    w_unlabeled=None,\n",
    "    use_unlabeled=True,\n",
    "):\n",
    "    \"\"\"Computes the statistics needed for the OLS-based prediction-powered inference.\n",
    "\n",
    "    Args:\n",
    "        pointest (ndarray): A point estimate of the coefficients.\n",
    "        X (ndarray): Covariates for the labeled data set.\n",
    "        Y (ndarray): Labels for the labeled data set.\n",
    "        Yhat (ndarray): Predictions for the labeled data set.\n",
    "        X_unlabeled (ndarray): Covariates for the unlabeled data set.\n",
    "        Yhat_unlabeled (ndarray): Predictions for the unlabeled data set.\n",
    "        w (ndarray, optional): Sample weights for the labeled data set.\n",
    "        w_unlabeled (ndarray, optional): Sample weights for the unlabeled data set.\n",
    "        use_unlabeled (bool, optional): Whether to use the unlabeled data set.\n",
    "\n",
    "    Returns:\n",
    "        grads (ndarray): Gradient of the loss function with respect to the coefficients.\n",
    "        grads_hat (ndarray): Gradient of the loss function with respect to the coefficients, evaluated using the labeled predictions.\n",
    "        grads_hat_unlabeled (ndarray): Gradient of the loss function with respect to the coefficients, evaluated using the unlabeled predictions.\n",
    "        inv_hessian (ndarray): Inverse Hessian of the loss function with respect to the coefficients.\n",
    "    \"\"\"\n",
    "    n = Y.shape[0]\n",
    "    N = Yhat_unlabeled.shape[0]\n",
    "    d = X.shape[1]\n",
    "    w = np.ones(n) if w is None else w / np.sum(w) * n\n",
    "    w_unlabeled = (\n",
    "        np.ones(N)\n",
    "        if w_unlabeled is None\n",
    "        else w_unlabeled / np.sum(w_unlabeled) * N\n",
    "    )\n",
    "\n",
    "    hessian = np.zeros((d, d))\n",
    "    grads_hat_unlabeled = np.zeros(X_unlabeled.shape)\n",
    "    if use_unlabeled:\n",
    "        for i in range(N):\n",
    "            hessian += (\n",
    "                w_unlabeled[i]\n",
    "                / (N + n)\n",
    "                * np.outer(X_unlabeled[i], X_unlabeled[i])\n",
    "            )\n",
    "            grads_hat_unlabeled[i, :] = (\n",
    "                w_unlabeled[i]\n",
    "                * X_unlabeled[i, :]\n",
    "                * (np.dot(X_unlabeled[i, :], pointest) - Yhat_unlabeled[i])\n",
    "            )\n",
    "\n",
    "    grads = np.zeros(X.shape)\n",
    "    grads_hat = np.zeros(X.shape)\n",
    "    for i in range(n):\n",
    "        hessian += (\n",
    "            w[i] / (N + n) * np.outer(X[i], X[i])\n",
    "            if use_unlabeled\n",
    "            else w[i] / n * np.outer(X[i], X[i])\n",
    "        )\n",
    "        grads[i, :] = w[i] * X[i, :] * (np.dot(X[i, :], pointest) - Y[i])\n",
    "        grads_hat[i, :] = (\n",
    "            w[i] * X[i, :] * (np.dot(X[i, :], pointest) - Yhat[i])\n",
    "        )\n",
    "\n",
    "    inv_hessian = np.linalg.inv(hessian).reshape(d, d)\n",
    "    return grads, grads_hat, grads_hat_unlabeled, inv_hessian\n",
    "\n",
    "def _power_analysis_stats(grads, grads_hat, inv_hessian):\n",
    "    grads_ = grads - grads.mean(axis=0)\n",
    "    grads_hat_ = grads_hat - grads_hat.mean(axis=0)\n",
    "    cov = inv_hessian @ (grads_[:,None,:] * grads_hat_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    var = inv_hessian @ (grads_[:,None,:]*grads_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    var_hat = inv_hessian @ (grads_hat_[:,None,:]*grads_hat_[:,:,None]).mean(axis=0) @ inv_hessian\n",
    "    rhos_sq = np.diag(cov)**2/(np.diag(var)*np.diag(var_hat))\n",
    "    sigmas_sq = np.diag(var)\n",
    "    return rhos_sq, sigmas_sq\n",
    "\n",
    "def _estimate_ppi_SE(n, N, rho_sq, var_Y):\n",
    "    if N == np.inf:\n",
    "        return np.sqrt(var_Y*(1-rho_sq)/n)\n",
    "    if N == 0:\n",
    "        return np.sqrt(var_Y/n)\n",
    "    var_ppi = var_Y*(1-rho_sq*N/(n+N))/n\n",
    "    return np.sqrt(var_ppi)\n",
    "\n",
    "def _estimate_classical_SE(n, var_Y):\n",
    "    return np.sqrt(var_Y/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate amce for intervention\n",
    "def compute_amce(data, x, y, alpha=0.05, suffix=\"\"):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        data.loc[:,\"weights\"] = calcWeightsTheoretical(data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        X = dd[\"Intervention\"]\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]==0) & (data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        X = dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        X = 1 - X\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        data_sub = data.loc[(data[\"CrossingSignal\"]!=0) & (data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        X = dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        X = 2 - X \n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Utilitarian\") & (data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        X = (dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Species\") & (data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        X = (dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Gender\") & (data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        X = (dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Fitness\") & (data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        X = (dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Age\") & (data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        X = (dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        data_sub = data.loc[(data[\"ScenarioType\"]==\"Social Status\") & (data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        dd = data_sub.dropna(subset=y)\n",
    "        dd = dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        X = (dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # define model with standard errors clustered on UserID\n",
    "        model = sm.WLS(dd[y], X, weights=dd[\"weights\"])\n",
    "\n",
    "\n",
    "\n",
    "    # fit model and extract estimates\n",
    "    fit = model.fit(cov_type = 'cluster', cov_kwds = {'groups': dd[\"UserID\"]})\n",
    "    coef = fit.params[x]\n",
    "    ci = fit.conf_int(alpha=alpha).loc[x]\n",
    "\n",
    "    # store results\n",
    "    res = pd.DataFrame({\n",
    "        'x': [x],\n",
    "        'y': [y],\n",
    "        'pointest' + suffix: [coef],\n",
    "        'conf_low' + suffix: [ci[0]],\n",
    "        'conf_high'+ suffix: [ci[1]]\n",
    "    })\n",
    "\n",
    "    return(res)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate amce with ppi \n",
    "def compute_amce_ppi(n_data, N_data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        n_data.loc[:,\"weights\"] = calcWeightsTheoretical(n_data)\n",
    "        N_data.loc[:,\"weights\"] = calcWeightsTheoretical(N_data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data.dropna(subset=y)\n",
    "        N_dd = N_data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        n_X = n_dd[\"Intervention\"]               \n",
    "        N_X = N_dd[\"Intervention\"]\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]==0) & (n_data[\"PedPed\"]==0), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]==0) & (N_data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        n_X = n_dd[\"Barrier\"]\n",
    "        N_X = N_dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        n_X = 1 - n_X\n",
    "        N_X = 1 - N_X\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]!=0) & (n_data[\"PedPed\"]==1), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]!=0) & (N_data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        n_X = n_dd[\"CrossingSignal\"]\n",
    "        N_X = N_dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        n_X = 2 - n_X \n",
    "        N_X = 2 - N_X \n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "    \n",
    "\n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Utilitarian\") & (n_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Utilitarian\") & (N_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        n_X = (n_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Species\") & (n_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Species\") & (N_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        n_X = (n_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Gender\") & (n_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Gender\") & (N_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        n_X = (n_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Fitness\") & (n_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Fitness\") & (N_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        n_X = (n_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Age\") & (n_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Age\") & (N_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        n_X = (n_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Social Status\") & (n_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Social Status\") & (N_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        n_X = (n_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd.loc[:,\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd.loc[:,y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd.loc[:,\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd.loc[:,\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    # calculate point estimate\n",
    "    pointest_ppi = ppi_ols_pointestimate(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                         X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                         w=n_weights, w_unlabeled=N_weights)\n",
    "\n",
    "    # calculate PPI confidence intervals\n",
    "    lower_CI_ppi, upper_CI_ppi = ppi_ols_ci(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                            X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                            w=n_weights, w_unlabeled=N_weights, alpha=alpha)\n",
    "\n",
    "    # calculate rho\n",
    "    beta = sm.WLS(n_Y_human, n_X, weights=n_weights).fit().params\n",
    "\n",
    "    grads, grads_hat, grads_hat_unlabeled, inv_hessian = _ols_get_stats(\n",
    "        pointest=beta, \n",
    "        X=n_X,\n",
    "        Y=n_Y_human,\n",
    "        Yhat= n_Y_silicon,\n",
    "        X_unlabeled=N_X,\n",
    "        Yhat_unlabeled=N_Y_silicon,\n",
    "        w=n_weights,\n",
    "        w_unlabeled=N_weights,\n",
    "        use_unlabeled=False)\n",
    "    \n",
    "    rho_sq, var_y = _power_analysis_stats(grads, grads_hat, inv_hessian)\n",
    "\n",
    "    # create and return the output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        \"y\": y,                              \n",
    "        \"x\": x,                              # Predictor variable (scenario attribute)\n",
    "        \"rho\": np.sqrt(rho_sq[1]),           # The association between predictions and outcomes\n",
    "        \"pointest_ppi\": pointest_ppi[1],     # PPI point estimate\n",
    "        \"conf_low_ppi\": lower_CI_ppi[1],     # The lower bound of the PPI confidence interval\n",
    "        \"conf_high_ppi\": upper_CI_ppi[1]},    # The upper bound of the PPI confidence interval\n",
    "        index=[0])\n",
    "    \n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>pointest</th>\n",
       "      <th>conf_low</th>\n",
       "      <th>conf_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intervention</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.068216</td>\n",
       "      <td>0.052464</td>\n",
       "      <td>0.083969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrier</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.164845</td>\n",
       "      <td>0.136728</td>\n",
       "      <td>0.192962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gender</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.159646</td>\n",
       "      <td>0.126552</td>\n",
       "      <td>0.192741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fitness</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.120809</td>\n",
       "      <td>0.085368</td>\n",
       "      <td>0.156249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Social Status</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.170991</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>0.262948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.336349</td>\n",
       "      <td>0.306916</td>\n",
       "      <td>0.365781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.481846</td>\n",
       "      <td>0.450556</td>\n",
       "      <td>0.513135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.573304</td>\n",
       "      <td>0.545087</td>\n",
       "      <td>0.601520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Species</td>\n",
       "      <td>Saved</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>0.617086</td>\n",
       "      <td>0.674794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x      y  pointest  conf_low  conf_high\n",
       "0    Intervention  Saved  0.068216  0.052464   0.083969\n",
       "0         Barrier  Saved  0.164845  0.136728   0.192962\n",
       "0          Gender  Saved  0.159646  0.126552   0.192741\n",
       "0         Fitness  Saved  0.120809  0.085368   0.156249\n",
       "0   Social Status  Saved  0.170991  0.079034   0.262948\n",
       "0  CrossingSignal  Saved  0.336349  0.306916   0.365781\n",
       "0             Age  Saved  0.481846  0.450556   0.513135\n",
       "0     Utilitarian  Saved  0.573304  0.545087   0.601520\n",
       "0         Species  Saved  0.645940  0.617086   0.674794"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([compute_amce(df, x=\"Intervention\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Barrier\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Gender\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Fitness\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"Social Status\", y=\"Saved\"), \n",
    "           compute_amce(df, x=\"CrossingSignal\",y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Age\", y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Utilitarian\", y=\"Saved\"),\n",
    "           compute_amce(df, x=\"Species\", y=\"Saved\")\n",
    "           ])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AMCE estimates computed above are the same as those calculated with the functions by Awad et al. (2018), see R script `7_CalculateAMCE.R`.\n",
    "\n",
    "\n",
    "|           label            |    dv  |  amce |   se  | conf.low | conf.high |\n",
    "|----------------------------|--------|-------|-------|----------|-----------|\n",
    "|   Intervention             | Saved  | 0.068 | 0.008 |    0.052 |     0.084 |\n",
    "|        Barrier             | Saved  | 0.165 | 0.014 |    0.137 |     0.193 |\n",
    "|            Law             | Saved  | 0.336 | 0.015 |    0.307 |     0.366 |\n",
    "|         Gender             | Saved  | 0.160 | 0.017 |    0.127 |     0.193 |\n",
    "|        Fitness             | Saved  | 0.121 | 0.018 |    0.085 |     0.156 |\n",
    "|  Social Status             | Saved  | 0.171 | 0.047 |    0.079 |     0.263 |\n",
    "|            Age             | Saved  | 0.482 | 0.016 |    0.451 |     0.513 |\n",
    "| No. Characters             | Saved  | 0.573 | 0.014 |    0.545 |     0.602 |\n",
    "|        Species             | Saved  | 0.646 | 0.015 |    0.617 |     0.675 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the PPI point estimates with a large sample size of human responses which we expect to be very close to the AMCE estimates obtained by applying classical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gpt4turbo_wp_Saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models: \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel: \u001b[39m\u001b[39m\"\u001b[39m, model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     results1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mIntervention\u001b[39;49m\u001b[39m\"\u001b[39;49m, y\u001b[39m=\u001b[39;49mmodel), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBarrier\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39mmodel), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGender\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39mmodel), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFitness\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39mmodel), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSocial Status\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39mmodel), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCrossingSignal\u001b[39m\u001b[39m\"\u001b[39m,y\u001b[39m=\u001b[39mmodel),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAge\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39mmodel),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUtilitarian\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39mmodel),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         compute_amce_ppi(df_human, df_silicon, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpecies\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         ],ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     results2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([results2, results1],ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m results2\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mData/7_rho.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=328'>329</a>\u001b[0m     N_weights \u001b[39m=\u001b[39m N_dd\u001b[39m.\u001b[39mloc[:,\u001b[39m\"\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_numpy()    \u001b[39m# define weights\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=331'>332</a>\u001b[0m \u001b[39m# calculate point estimate\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=332'>333</a>\u001b[0m pointest_ppi \u001b[39m=\u001b[39m ppi_ols_pointestimate(X\u001b[39m=\u001b[39;49mn_X, Y\u001b[39m=\u001b[39;49mn_Y_human, Yhat\u001b[39m=\u001b[39;49mn_Y_silicon, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=333'>334</a>\u001b[0m                                      X_unlabeled\u001b[39m=\u001b[39;49mN_X, Yhat_unlabeled\u001b[39m=\u001b[39;49mN_Y_silicon, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=334'>335</a>\u001b[0m                                      w\u001b[39m=\u001b[39;49mn_weights, w_unlabeled\u001b[39m=\u001b[39;49mN_weights)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=336'>337</a>\u001b[0m \u001b[39m# calculate PPI confidence intervals\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=337'>338</a>\u001b[0m lower_CI_ppi, upper_CI_ppi \u001b[39m=\u001b[39m ppi_ols_ci(X\u001b[39m=\u001b[39mn_X, Y\u001b[39m=\u001b[39mn_Y_human, Yhat\u001b[39m=\u001b[39mn_Y_silicon, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=338'>339</a>\u001b[0m                                         X_unlabeled\u001b[39m=\u001b[39mN_X, Yhat_unlabeled\u001b[39m=\u001b[39mN_Y_silicon, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/7_AnalysisPPI.ipynb#X12sZmlsZQ%3D%3D?line=339'>340</a>\u001b[0m                                         w\u001b[39m=\u001b[39mn_weights, w_unlabeled\u001b[39m=\u001b[39mN_weights, alpha\u001b[39m=\u001b[39malpha)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ppi_py/ppi.py:654\u001b[0m, in \u001b[0;36mppi_ols_pointestimate\u001b[0;34m(X, Y, Yhat, X_unlabeled, Yhat_unlabeled, lhat, coord, w, w_unlabeled, one_step)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mif\u001b[39;00m lhat \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m     grads, grads_hat, grads_hat_unlabeled, inv_hessian \u001b[39m=\u001b[39m _ols_get_stats(\n\u001b[1;32m    644\u001b[0m         theta_pp,\n\u001b[1;32m    645\u001b[0m         X\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    652\u001b[0m         use_unlabeled\u001b[39m=\u001b[39muse_unlabeled,\n\u001b[1;32m    653\u001b[0m     )\n\u001b[0;32m--> 654\u001b[0m     lhat \u001b[39m=\u001b[39m _calc_lhat_glm(\n\u001b[1;32m    655\u001b[0m         grads,\n\u001b[1;32m    656\u001b[0m         grads_hat,\n\u001b[1;32m    657\u001b[0m         grads_hat_unlabeled,\n\u001b[1;32m    658\u001b[0m         inv_hessian,\n\u001b[1;32m    659\u001b[0m         coord,\n\u001b[1;32m    660\u001b[0m         clip\u001b[39m=\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m one_step),\n\u001b[1;32m    661\u001b[0m     )\n\u001b[1;32m    662\u001b[0m     \u001b[39mif\u001b[39;00m one_step:\n\u001b[1;32m    663\u001b[0m         \u001b[39mreturn\u001b[39;00m theta_pp \u001b[39m-\u001b[39m inv_hessian \u001b[39m@\u001b[39m (\n\u001b[1;32m    664\u001b[0m             lhat \u001b[39m*\u001b[39m grads_hat_unlabeled\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    665\u001b[0m             \u001b[39m+\u001b[39m grads\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    666\u001b[0m             \u001b[39m-\u001b[39m lhat \u001b[39m*\u001b[39m grads_hat\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    667\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ppi_py/ppi.py:1361\u001b[0m, in \u001b[0;36m_calc_lhat_glm\u001b[0;34m(grads, grads_hat, grads_hat_unlabeled, inv_hessian, coord, clip)\u001b[0m\n\u001b[1;32m   1351\u001b[0m cov_grads \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((d, d))\n\u001b[1;32m   1353\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[1;32m   1354\u001b[0m     cov_grads \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m n) \u001b[39m*\u001b[39m (\n\u001b[1;32m   1355\u001b[0m         np\u001b[39m.\u001b[39mouter(\n\u001b[1;32m   1356\u001b[0m             grads[i] \u001b[39m-\u001b[39m grads\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m   1357\u001b[0m             grads_hat[i] \u001b[39m-\u001b[39m grads_hat\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m   1358\u001b[0m         )\n\u001b[1;32m   1359\u001b[0m         \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mouter(\n\u001b[1;32m   1360\u001b[0m             grads_hat[i] \u001b[39m-\u001b[39m grads_hat\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[0;32m-> 1361\u001b[0m             grads[i] \u001b[39m-\u001b[39m grads\u001b[39m.\u001b[39;49mmean(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m),\n\u001b[1;32m   1362\u001b[0m         )\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m var_grads_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(\n\u001b[1;32m   1365\u001b[0m     np\u001b[39m.\u001b[39mconcatenate([grads_hat, grads_hat_unlabeled], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mT\n\u001b[1;32m   1366\u001b[0m )\n\u001b[1;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m coord \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    115\u001b[0m         dtype \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m         is_float16_result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39mwhere)\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ids = df[\"ResponseID\"].unique()\n",
    "N = 100\n",
    "n = len(ids) - N\n",
    "random.seed(2024)\n",
    "\n",
    "n_ids = random.sample(ids.tolist(), k=n)\n",
    "N_ids = random.sample(list(set(ids) - set(n_ids)), k=N)\n",
    "\n",
    "df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "models = [\"gpt4turbo_wp_Saved\"]#,\"gpt4o_wp_Saved\",\"gpt35turbo0125_wp_Saved\"]\n",
    "\n",
    "results2 = pd.DataFrame()\n",
    "for model in models: \n",
    "    \n",
    "    print(\"Model: \", model)\n",
    "    results1 = pd.concat([\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Intervention\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Barrier\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Gender\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Fitness\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Social Status\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"CrossingSignal\",y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Age\", y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Utilitarian\", y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Species\", y=model)\n",
    "        ],ignore_index=True)\n",
    "    \n",
    "    results2 = pd.concat([results2, results1],ignore_index=True)\n",
    "    \n",
    "results2.to_csv(\"Data/7_rho.csv\", index=False)\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over dependent variable: gpt4turbo_wp_Saved\n",
      "    Predictor: Intervention with human sample size 100\n",
      "    Predictor: Intervention with human sample size 200\n",
      "    Predictor: Intervention with human sample size 500\n",
      "    Predictor: Barrier with human sample size 100\n",
      "    Predictor: Barrier with human sample size 200\n",
      "    Predictor: Barrier with human sample size 500\n",
      "    Predictor: CrossingSignal with human sample size 100\n",
      "    Predictor: CrossingSignal with human sample size 200\n",
      "    Predictor: CrossingSignal with human sample size 500\n",
      "    Predictor: Gender with human sample size 100\n",
      "    Predictor: Gender with human sample size 200\n",
      "    Predictor: Gender with human sample size 500\n",
      "    Predictor: Fitness with human sample size 100\n",
      "    Predictor: Fitness with human sample size 200\n",
      "    Predictor: Fitness with human sample size 500\n",
      "    Predictor: Social Status with human sample size 100\n",
      "    Predictor: Social Status with human sample size 200\n",
      "    Predictor: Social Status with human sample size 500\n",
      "    Predictor: Age with human sample size 100\n",
      "    Predictor: Age with human sample size 200\n",
      "    Predictor: Age with human sample size 500\n",
      "    Predictor: Utilitarian with human sample size 100\n",
      "    Predictor: Utilitarian with human sample size 200\n",
      "    Predictor: Utilitarian with human sample size 500\n",
      "    Predictor: Species with human sample size 100\n",
      "    Predictor: Species with human sample size 200\n",
      "    Predictor: Species with human sample size 500\n"
     ]
    }
   ],
   "source": [
    "# sample size human subjects\n",
    "ns = [100, 200, 500]\n",
    "\n",
    "# multiples of human subjects sample size\n",
    "ks = list([0.25, 0.5, 0.75]) + list(np.arange(1, 10.5, 0.5))\n",
    "\n",
    "# predictions\n",
    "Ys = [\"gpt4turbo_wp_Saved\"]#,\"gpt4o_wp_Saved\",\"gpt35turbo0125_wp_Saved\"]\n",
    "\n",
    "# structural attributes of scenarios\n",
    "Xs_structural  = ['Intervention', 'Barrier','CrossingSignal']\n",
    "\n",
    "# attributes of characters\n",
    "Xs_characters = ['Gender','Fitness','Social Status','Age','Utilitarian','Species']\n",
    "\n",
    "# all attributes\n",
    "Xs = Xs_structural + Xs_characters\n",
    "\n",
    "# number of repetitions for combinations of n and N\n",
    "reps = 300\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for y in Ys:\n",
    "  print(f\"Iterating over dependent variable: {y}\")\n",
    "  \n",
    "  for x in Xs:\n",
    "\n",
    "    for n in ns:\n",
    "      print(f\"    Predictor: {x} with human sample size {n}\")\n",
    "\n",
    "      # sample size silicon subjects \n",
    "      Ns = [int(n * k) for n in ns for k in ks]\n",
    "      \n",
    "      for N in Ns:\n",
    "\n",
    "        for r in range(reps):\n",
    "\n",
    "          # subset to dilemmas with variation on structural attribute\n",
    "          if x in Xs_structural:\n",
    "\n",
    "              cnt = df.groupby(\"ResponseID\")[x].nunique()\n",
    "              ids = cnt[ cnt > 1].index.tolist()\n",
    "\n",
    "          # subset to dilemmas with relevant character attribute\n",
    "          if x in Xs_characters:\n",
    "\n",
    "              ids = df.loc[ (df[\"ScenarioType\"]==x) & (df[\"ScenarioTypeStrict\"]==x), \"ResponseID\"].tolist()\n",
    "          \n",
    "          # skip current iteration if target n is larger than population\n",
    "          if (len(ids) < n):\n",
    "             continue \n",
    "\n",
    "          # sample dilemmas for human subjects sample\n",
    "          n_ids = random.sample(ids, k=n)\n",
    "          \n",
    "          # get remaining dilemma ids to sample from\n",
    "          remaining_ids = list(set(ids) - set(n_ids))\n",
    "\n",
    "          # skip current iteration if target N is larger than population\n",
    "          if (len(remaining_ids) < N):\n",
    "             continue \n",
    "          \n",
    "          # sample dilemmas for silicon subjects sample\n",
    "          N_ids = random.sample(remaining_ids, k=N)\n",
    "\n",
    "          # subset data\n",
    "          df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "          df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "          # pool human and silicon subject decisions\n",
    "          df_pooled = pd.concat([df_human,df_silicon], ignore_index=True)\n",
    "          \n",
    "          # compute amce on human data only \n",
    "          human = compute_amce(data=df_human,x=x,y=y,suffix=\"_human\")\n",
    "\n",
    "          # compute amce on silicon data only\n",
    "          silicon = compute_amce(data=df_silicon,x=x,y=y,suffix=\"_silicon\")\n",
    "\n",
    "          # compute ppi acme\n",
    "          ppi = compute_amce_ppi(n_data=df_human, N_data=df_silicon, x=x, y=y)\n",
    "          \n",
    "          # compute amce on pooled data\n",
    "          pooled = compute_amce(data=df_pooled,x=x,y=y,suffix=\"_pooled\")\n",
    "\n",
    "          # store data\n",
    "          to_append1 = pd.merge(ppi, pooled, on=['x','y'], how='outer')\n",
    "          to_append2 = pd.merge(to_append1, human, on=['x','y'], how='outer')\n",
    "          to_append3 = pd.merge(to_append2, silicon, on=['x','y'], how='outer')\n",
    "\n",
    "          # record sample sizes\n",
    "          to_append3.loc[:,\"n\"] = n\n",
    "          to_append3.loc[:,\"N\"] = N\n",
    "          \n",
    "          result = pd.concat([result, to_append3], ignore_index=True)\n",
    "          del ppi \n",
    "          del pooled \n",
    "          del to_append1\n",
    "          del to_append2\n",
    "          del to_append3\n",
    "\n",
    "          \n",
    "          \n",
    "\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>rho</th>\n",
       "      <th>pointest_ppi</th>\n",
       "      <th>conf_low_ppi</th>\n",
       "      <th>conf_high_ppi</th>\n",
       "      <th>pointest_pooled</th>\n",
       "      <th>conf_low_pooled</th>\n",
       "      <th>conf_high_pooled</th>\n",
       "      <th>pointest_human</th>\n",
       "      <th>conf_low_human</th>\n",
       "      <th>conf_high_human</th>\n",
       "      <th>pointest_silicon</th>\n",
       "      <th>conf_low_silicon</th>\n",
       "      <th>conf_high_silicon</th>\n",
       "      <th>n</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.278433</td>\n",
       "      <td>-0.163842</td>\n",
       "      <td>-0.323455</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>0.018235</td>\n",
       "      <td>-0.171109</td>\n",
       "      <td>0.207579</td>\n",
       "      <td>0.042442</td>\n",
       "      <td>-0.164934</td>\n",
       "      <td>0.249817</td>\n",
       "      <td>-0.076287</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>0.356775</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.453710</td>\n",
       "      <td>-0.025419</td>\n",
       "      <td>-0.178626</td>\n",
       "      <td>0.124482</td>\n",
       "      <td>0.038999</td>\n",
       "      <td>-0.146288</td>\n",
       "      <td>0.224286</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>-0.203644</td>\n",
       "      <td>0.205167</td>\n",
       "      <td>0.212623</td>\n",
       "      <td>-0.196007</td>\n",
       "      <td>0.621254</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.122095</td>\n",
       "      <td>-0.061954</td>\n",
       "      <td>-0.226223</td>\n",
       "      <td>0.097644</td>\n",
       "      <td>-0.025929</td>\n",
       "      <td>-0.196265</td>\n",
       "      <td>0.144407</td>\n",
       "      <td>-0.083427</td>\n",
       "      <td>-0.290350</td>\n",
       "      <td>0.123495</td>\n",
       "      <td>0.091720</td>\n",
       "      <td>-0.191110</td>\n",
       "      <td>0.374550</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.167916</td>\n",
       "      <td>0.081045</td>\n",
       "      <td>-0.084283</td>\n",
       "      <td>0.243945</td>\n",
       "      <td>0.085255</td>\n",
       "      <td>-0.083998</td>\n",
       "      <td>0.254508</td>\n",
       "      <td>0.065806</td>\n",
       "      <td>-0.142909</td>\n",
       "      <td>0.274521</td>\n",
       "      <td>0.130007</td>\n",
       "      <td>-0.170256</td>\n",
       "      <td>0.430269</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.512941</td>\n",
       "      <td>0.032399</td>\n",
       "      <td>-0.115603</td>\n",
       "      <td>0.181096</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>-0.153252</td>\n",
       "      <td>0.156254</td>\n",
       "      <td>0.041707</td>\n",
       "      <td>-0.164044</td>\n",
       "      <td>0.247457</td>\n",
       "      <td>-0.052206</td>\n",
       "      <td>-0.290665</td>\n",
       "      <td>0.186253</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.113321</td>\n",
       "      <td>0.614700</td>\n",
       "      <td>0.553189</td>\n",
       "      <td>0.675792</td>\n",
       "      <td>0.844114</td>\n",
       "      <td>0.823988</td>\n",
       "      <td>0.864239</td>\n",
       "      <td>0.807634</td>\n",
       "      <td>0.749019</td>\n",
       "      <td>0.866250</td>\n",
       "      <td>0.849861</td>\n",
       "      <td>0.828352</td>\n",
       "      <td>0.871370</td>\n",
       "      <td>500</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.097007</td>\n",
       "      <td>0.647833</td>\n",
       "      <td>0.591496</td>\n",
       "      <td>0.704517</td>\n",
       "      <td>0.843017</td>\n",
       "      <td>0.823828</td>\n",
       "      <td>0.862207</td>\n",
       "      <td>0.873651</td>\n",
       "      <td>0.826964</td>\n",
       "      <td>0.920338</td>\n",
       "      <td>0.838549</td>\n",
       "      <td>0.817758</td>\n",
       "      <td>0.859339</td>\n",
       "      <td>500</td>\n",
       "      <td>3250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.048352</td>\n",
       "      <td>0.707911</td>\n",
       "      <td>0.656268</td>\n",
       "      <td>0.759610</td>\n",
       "      <td>0.837044</td>\n",
       "      <td>0.817364</td>\n",
       "      <td>0.856723</td>\n",
       "      <td>0.856346</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.908822</td>\n",
       "      <td>0.834201</td>\n",
       "      <td>0.813270</td>\n",
       "      <td>0.855131</td>\n",
       "      <td>500</td>\n",
       "      <td>3250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.023688</td>\n",
       "      <td>0.628161</td>\n",
       "      <td>0.568854</td>\n",
       "      <td>0.687468</td>\n",
       "      <td>0.841414</td>\n",
       "      <td>0.822635</td>\n",
       "      <td>0.860193</td>\n",
       "      <td>0.856027</td>\n",
       "      <td>0.801082</td>\n",
       "      <td>0.910972</td>\n",
       "      <td>0.839430</td>\n",
       "      <td>0.819417</td>\n",
       "      <td>0.859442</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.051220</td>\n",
       "      <td>0.727063</td>\n",
       "      <td>0.678137</td>\n",
       "      <td>0.776159</td>\n",
       "      <td>0.842734</td>\n",
       "      <td>0.824073</td>\n",
       "      <td>0.861396</td>\n",
       "      <td>0.860041</td>\n",
       "      <td>0.809697</td>\n",
       "      <td>0.910385</td>\n",
       "      <td>0.840201</td>\n",
       "      <td>0.819985</td>\n",
       "      <td>0.860418</td>\n",
       "      <td>500</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3076 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       y             x       rho  pointest_ppi  conf_low_ppi  \\\n",
       "0     gpt4turbo_wp_Saved  Intervention  0.278433     -0.163842     -0.323455   \n",
       "1     gpt4turbo_wp_Saved  Intervention  0.453710     -0.025419     -0.178626   \n",
       "2     gpt4turbo_wp_Saved  Intervention  0.122095     -0.061954     -0.226223   \n",
       "3     gpt4turbo_wp_Saved  Intervention  0.167916      0.081045     -0.084283   \n",
       "4     gpt4turbo_wp_Saved  Intervention  0.512941      0.032399     -0.115603   \n",
       "...                  ...           ...       ...           ...           ...   \n",
       "3071  gpt4turbo_wp_Saved       Species  0.113321      0.614700      0.553189   \n",
       "3072  gpt4turbo_wp_Saved       Species  0.097007      0.647833      0.591496   \n",
       "3073  gpt4turbo_wp_Saved       Species  0.048352      0.707911      0.656268   \n",
       "3074  gpt4turbo_wp_Saved       Species  0.023688      0.628161      0.568854   \n",
       "3075  gpt4turbo_wp_Saved       Species  0.051220      0.727063      0.678137   \n",
       "\n",
       "      conf_high_ppi  pointest_pooled  conf_low_pooled  conf_high_pooled  \\\n",
       "0         -0.003043         0.018235        -0.171109          0.207579   \n",
       "1          0.124482         0.038999        -0.146288          0.224286   \n",
       "2          0.097644        -0.025929        -0.196265          0.144407   \n",
       "3          0.243945         0.085255        -0.083998          0.254508   \n",
       "4          0.181096         0.001501        -0.153252          0.156254   \n",
       "...             ...              ...              ...               ...   \n",
       "3071       0.675792         0.844114         0.823988          0.864239   \n",
       "3072       0.704517         0.843017         0.823828          0.862207   \n",
       "3073       0.759610         0.837044         0.817364          0.856723   \n",
       "3074       0.687468         0.841414         0.822635          0.860193   \n",
       "3075       0.776159         0.842734         0.824073          0.861396   \n",
       "\n",
       "      pointest_human  conf_low_human  conf_high_human  pointest_silicon  \\\n",
       "0           0.042442       -0.164934         0.249817         -0.076287   \n",
       "1           0.000762       -0.203644         0.205167          0.212623   \n",
       "2          -0.083427       -0.290350         0.123495          0.091720   \n",
       "3           0.065806       -0.142909         0.274521          0.130007   \n",
       "4           0.041707       -0.164044         0.247457         -0.052206   \n",
       "...              ...             ...              ...               ...   \n",
       "3071        0.807634        0.749019         0.866250          0.849861   \n",
       "3072        0.873651        0.826964         0.920338          0.838549   \n",
       "3073        0.856346        0.803870         0.908822          0.834201   \n",
       "3074        0.856027        0.801082         0.910972          0.839430   \n",
       "3075        0.860041        0.809697         0.910385          0.840201   \n",
       "\n",
       "      conf_low_silicon  conf_high_silicon    n     N  \n",
       "0            -0.509348           0.356775  100    25  \n",
       "1            -0.196007           0.621254  100    25  \n",
       "2            -0.191110           0.374550  100    50  \n",
       "3            -0.170256           0.430269  100    50  \n",
       "4            -0.290665           0.186253  100    75  \n",
       "...                ...                ...  ...   ...  \n",
       "3071          0.828352           0.871370  500  3000  \n",
       "3072          0.817758           0.859339  500  3250  \n",
       "3073          0.813270           0.855131  500  3250  \n",
       "3074          0.819417           0.859442  500  3500  \n",
       "3075          0.819985           0.860418  500  3500  \n",
       "\n",
       "[3076 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_csv(\"../Data/7_ResultsPPI.csv.gz\", compression=\"gzip\", index=False)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
