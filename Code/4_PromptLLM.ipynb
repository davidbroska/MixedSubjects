{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prompt LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates a prompt for the LLM with a demographic profile of a survey respondents and a description of the two scenarios that the respondents encountered during the experiments. \n",
    "\n",
    "The demographic profile on the age, education, gender, and income that survey respondents reported after the Moral Machine experiment. The order in which these characteristics appear is randomized. It was up to the respondents to decide whether to take the survey or not.\n",
    "\n",
    "The description of the scenarios is generated from the data matrix. One dilemma consists of two scenarios, presented side by side during the experiment. If respondents click on the left side, they opt for the death of the characters represented in that outcome while the characters on the right side will be saved (and vice versa). \n",
    "\n",
    "To generate the descriptions of the dilemmas, we extended code written by Takemoto (2024). This study generates new dilemmas by randomly combining features of scenarios (e.g. the composition of characters) and prompts LLMs to evaluate these dilemmas. In contrast, our study takes the existing dilemmas from Awad et al. (2018) and prompts LLMs to predict how survey respondents evluated the dilemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import random\n",
    "import openai\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scenario(response, include_persona=True):\n",
    "\n",
    "    if include_persona:\n",
    "\n",
    "        # \"Annual income, including tips, dividends, interest, etc (in US dollars)\" [dropdown menu]\n",
    "        inc_descriptions = {\n",
    "            2500:   \"Under $5,000\",                              # \"Under $5,000\"\n",
    "            7500:   \"7,500\",                                     # \"$5,000-$10,000\"\n",
    "            12500:  \"12,500\",                                    # \"$10,001-$15,000\"\n",
    "            20000:  \"20,000\",                                    # \"$15,001-$25,000\"\n",
    "            30000:  \"30,000\",                                    # \"$25,001-$35,000\" \n",
    "            42500:  \"42,500\",                                    # \"$35,001-$50,000\"\n",
    "            65000:  \"65,000\",                                    # \"$50,000-$85,000\"\n",
    "            90000:  \"90,000\",                                    # \"$80,001-$100,000\"\n",
    "            150000: \"more than 100,000\"                          # \"Over $100,000\"\n",
    "        }\n",
    "        inc_val = response[\"Review_ContinuousIncome\"].unique()[0]\n",
    "        inc_des = \"You earn an annual income of {} US dollars.\".format(inc_descriptions[inc_val])\n",
    "\n",
    "\n",
    "        # \"How old are you?\" [text box]\n",
    "        age_val = response[\"Review_age\"].unique()[0]\n",
    "        age_des = \"You are {} years old.\".format(age_val)\n",
    "\n",
    "        # \"Highest level of education\" [dropdown menu]\n",
    "        edu_descriptions = {\n",
    "            'underHigh': \"less than a high school diploma\",      # \"Less than a High School Diploma\"\n",
    "            'high': \"a high school diploma\",                     # \"High School Diploma\"\n",
    "            'vocational': \"vocational training\",                 # \"Vocational training\"\n",
    "            'college': \"that you attended college\",              # \"Attended College\"\n",
    "            'bachelor': \"a bachelor degree\",                     # \"Bachelor Degree\"\n",
    "            'graduate': \"graduate degree\"                        # \"Graduate Degree\"\n",
    "        }\n",
    "        edu_val = response[\"Review_education\"].unique()[0]\n",
    "        edu_des = \"Your highest level of education is {}.\".format(edu_descriptions[edu_val])\n",
    "\n",
    "\n",
    "        # \"What is your gender?\" [dropdown menu]\n",
    "        gen_descriptions = {\n",
    "            'Man':   \"You are a man.\",                           # \"Male\"\n",
    "            'Woman': \"You are a woman.\",                         # \"Female\"\n",
    "            \"Other\": \"You do not identify as a woman or a man.\"  # \"Other\"\n",
    "        }\n",
    "        gen_val = response[\"Review_gender\"].unique()[0]\n",
    "        gen_des = gen_descriptions[gen_val]\n",
    "\n",
    "        # \"What are your religious views?\" [slider scale]\n",
    "        rel_val = response[\"Review_religious\"].unique()[0]\n",
    "        rel_des = \"On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of {} for your religious views.\".format(rel_val)\n",
    "\n",
    "        # \"What are your political views?\" [slider scale]\n",
    "        pol_val = response[\"Review_political\"].unique()[0]\n",
    "        pol_des = \"On a scale from 0 (Conservative) to 100 (Progressive), you report a score of {} for your political views.\".format(pol_val)\n",
    "\n",
    "        # reshuffle order of persona characterists\n",
    "        persona_characteristics = [inc_des, age_des, edu_des, gen_des, pol_des, rel_des]\n",
    "        random.shuffle(persona_characteristics)\n",
    "    \n",
    "        persona = '\\n'.join([f' - {c}' for c in persona_characteristics])\n",
    "    \n",
    "    # set seed for order of characters in description of scenarios\n",
    "    random.seed(a=2024)\n",
    "\n",
    "    # column names in survey data\n",
    "    cols = ['Man','Woman','Pregnant','Stroller','OldMan','OldWoman','Boy','Girl','Homeless','LargeWoman','LargeMan','Criminal',\n",
    "            'MaleExecutive','FemaleExecutive','FemaleAthlete','MaleAthlete','FemaleDoctor','MaleDoctor','Dog','Cat']\n",
    "    \n",
    "    # corresponding words used in the descriptions\n",
    "    character = [\"man\", \"woman\", \"pregnant woman\", \"baby\", \"elderly man\", \"elderly woman\", \"boy\", \"girl\", \"homeless person\", \"large woman\", \"large man\",\n",
    "                 \"criminal\", \"male executive\", \"female executive\", \"female athlete\", \"male athlete\", \"female doctor\", \"male doctor\", \"dog\",\"cat\"]\n",
    "\n",
    "    # plural conversion\n",
    "    characters_plural = [\"men\", \"women\", \"pregnant women\", \"babies\", \"elderly men\", \"elderly women\", \"boys\", \"girls\", \"homeless persons\", \"large women\", \"large men\", \n",
    "                         \"criminals\", \"male executives\", \"female executives\", \"female athletes\", \"male athletes\", \"female doctors\", \"male doctors\", \"dogs\", \"cats\"]\n",
    "    def plural(elem, cnt):\n",
    "        if cnt > 1:\n",
    "            return characters_plural[character.index(elem)]\n",
    "        else:\n",
    "            return elem\n",
    "        \n",
    "    # baseline description\n",
    "    base_description = \"A self-driving car with sudden brake failure will \"\n",
    "\n",
    "    # shortcuts for text\n",
    "    traffic_light_pattern_descriptions = {\n",
    "        'red': \"flouting the law by crossing on the red signal\",\n",
    "        'green': \"abiding by the law by crossing on the green signal\" \n",
    "    }\n",
    "\n",
    "    # whether scenarios involve a car and a pedestrian\n",
    "    is_pasvsped = all(response[\"PedPed\"] == 0)\n",
    "\n",
    "    # passengers vs pedestrians\n",
    "    if is_pasvsped:\n",
    "\n",
    "        pas_description = \"\"\n",
    "        ped_description = \"\"\n",
    "        \n",
    "        # passengers died if Barrier=1\n",
    "        pas = response[ response[\"Barrier\"]==1 ]\n",
    "        # long format to get characters and frequency\n",
    "        pas_T = pas[cols].T.reset_index()\n",
    "        pas_ind = pas_T[pas_T.iloc[:,1] >= 1 ].index\n",
    "        # get list of unique characters in scenario\n",
    "        pas_char = [character[i] for i in pas_ind]\n",
    "        # get number of times each one of them appears\n",
    "        pas_numchar = list(pas_T[pas_T.iloc[:,1] >= 1 ].iloc[:,1])\n",
    "        # repeat characters if they occur multiple times\n",
    "        passengers  = [elem for count, elem in zip(pas_numchar, pas_char) for _ in repeat(None, count)]\n",
    "        # reshuffle order\n",
    "        random.shuffle(passengers)\n",
    "\n",
    "        # pedestrians died if Barrier=0\n",
    "        ped = response[ response[\"Barrier\"]==0 ]\n",
    "        # long format to get characters and frequency\n",
    "        ped_T = ped[cols].T.reset_index()\n",
    "        ped_ind = ped_T[ped_T.iloc[:,1] >= 1 ].index\n",
    "        # get list of unique characters in scenario\n",
    "        ped_char = [character[i] for i in ped_ind]\n",
    "        # get number of times each one of them appears\n",
    "        ped_numchar = list(ped_T[ped_T.iloc[:,1] >= 1 ].iloc[:,1])\n",
    "        # repeat characters if they occur multiple times\n",
    "        pedestrians = [elem for count, elem in zip(ped_numchar, ped_char) for _ in repeat(None, count)]\n",
    "        # reshuffle order\n",
    "        random.shuffle(pedestrians)\n",
    "\n",
    "        pas_dict = Counter(passengers)\n",
    "        ped_dict = Counter(pedestrians)\n",
    "\n",
    "        passengers_set = \"\"\n",
    "        nb_passengers_set = 0\n",
    "        for i, (element, count) in enumerate(pas_dict.items()):\n",
    "            nb_passengers_set += count\n",
    "            passengers_set += \"{} {}\".format(count, plural(element, count))\n",
    "            if i == len(pas_dict) - 2:\n",
    "                if len(pas_dict) == 2:\n",
    "                    passengers_set += \" and \"\n",
    "                else:\n",
    "                    passengers_set += \", and \"\n",
    "            elif i < len(pas_dict) - 1:\n",
    "                passengers_set += \", \"\n",
    "\n",
    "        pedestrians_set = \"\"\n",
    "        nb_pedestrians_set = 0\n",
    "        for i, (element, count) in enumerate(ped_dict.items()):\n",
    "            nb_pedestrians_set += count\n",
    "            pedestrians_set += \"{} {}\".format(count, plural(element, count))\n",
    "            if i == len(ped_dict) - 2:\n",
    "                if len(ped_dict) == 2:\n",
    "                    pedestrians_set += \" and \"\n",
    "                else:\n",
    "                    pedestrians_set += \", and \"\n",
    "            elif i < len(ped_dict) - 1:\n",
    "                pedestrians_set += \", \"\n",
    "\n",
    "        # code legality for pedestrians\n",
    "        if ped.iloc[0][\"CrossingSignal\"] == 0: \n",
    "            ped_signal = \"NA\" \n",
    "        elif ped.iloc[0][\"CrossingSignal\"] == 1:\n",
    "            ped_signal = \"green\"\n",
    "        elif ped.iloc[0][\"CrossingSignal\"] == 2:\n",
    "            ped_signal  = \"red\"\n",
    "        \n",
    "        # passengers died as the result of swerving\n",
    "        if pas.iloc[0][\"Intervention\"] == 1:\n",
    "            pas_description = pas_description + base_description + \"swerve and crash into a concrete barrier. This will result in the death of {}, who {} inside the car.\".format(\n",
    "                passengers_set,\n",
    "                'were' if nb_passengers_set >= 2 else 'was',\n",
    "            )\n",
    "            if ped_signal != \"NA\":\n",
    "                ped_description = ped_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} {}.\".format(\n",
    "                    pedestrians_set,\n",
    "                    'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped_signal],\n",
    "                )\n",
    "            else:\n",
    "                ped_description = ped_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} crossing.\".format(\n",
    "                    pedestrians_set,\n",
    "                    'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                )\n",
    "        # passengers died from continuing ahead and crashing into a barrier\n",
    "        else:\n",
    "            pas_description = pas_description + base_description + \"continue ahead and crash into a concrete barrier. This will result in the death of {}, who {} inside the car.\".format(\n",
    "                passengers_set,\n",
    "                'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "            )\n",
    "            if ped_signal != \"NA\":\n",
    "                ped_description = ped_description + base_description + \"swerve to avoid crashing into a concrete barrier and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} {}.\".format(\n",
    "                    pedestrians_set,\n",
    "                    'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped_signal],\n",
    "                )\n",
    "            else:\n",
    "                ped_description = ped_description + base_description + \"swerve to avoid crashing into a concrete barrier and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} crossing.\".format(\n",
    "                    pedestrians_set,\n",
    "                    'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                )\n",
    "\n",
    "        pas_description = pas_description + \" Meanwhile, it will spare {}\".format(pedestrians_set)\n",
    "        if ped_signal != \"NA\":\n",
    "            pas_description = pas_description + \", who {} {}.\\n\".format(\n",
    "                'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                traffic_light_pattern_descriptions[ped_signal],\n",
    "            )\n",
    "        else:\n",
    "            pas_description = pas_description + \", who {} crossing.\\n\".format('were' if nb_pedestrians_set >= 2 else 'was')\n",
    "\n",
    "        ped_description = ped_description + \" Meanwhile, it will spare {}\".format(passengers_set)\n",
    "        ped_description = ped_description + \", who {} inside the car.\\n\".format('were' if nb_passengers_set >= 2 else 'was')\n",
    "\n",
    "        # order descriptions of scenarios as presented to respondent \n",
    "        if pas.iloc[0][\"LeftHand\"] == 1: \n",
    "            c1_description = \"Case 1.\\n\" + pas_description\n",
    "            c2_description = \"Case 2.\\n\" + ped_description\n",
    "        else:\n",
    "            c1_description = \"Case 1.\\n\" + ped_description\n",
    "            c2_description = \"Case 2.\\n\" + pas_description\n",
    "\n",
    "    \n",
    "    # pedestrians vs pedestrians\n",
    "    else:\n",
    "            \n",
    "        ped1_description = \"\"\n",
    "        ped2_description = \"\"\n",
    "        \n",
    "        # pedestrians 1 mentioned first if LeftHand=1\n",
    "        ped1 = response[ response[\"LeftHand\"]==1 ]\n",
    "        # long format to get characters and frequency\n",
    "        ped1_T = ped1[cols].T.reset_index()\n",
    "        ped1_ind = ped1_T[ped1_T.iloc[:,1] >= 1 ].index\n",
    "        # get list of unique characters in scenario\n",
    "        ped1_char = [character[i] for i in ped1_ind]\n",
    "        # get number of times each one of them appears\n",
    "        ped1_numchar = list(ped1_T[ped1_T.iloc[:,1] >= 1 ].iloc[:,1])\n",
    "        # repeat characters if they occur multiple times\n",
    "        pedestrians1 = [elem for count, elem in zip(ped1_numchar, ped1_char) for _ in repeat(None, count)]\n",
    "        # reshuffle order\n",
    "        random.shuffle(pedestrians1)\n",
    "\n",
    "        # pedestrians died if Barrier=0\n",
    "        ped2 = response[ response[\"LeftHand\"]==0 ]\n",
    "        # long format to get characters and frequency\n",
    "        ped2_T = ped2[cols].T.reset_index()\n",
    "        ped2_ind = ped2_T[ped2_T.iloc[:,1] >= 1 ].index\n",
    "        # get list of unique characters in scenario\n",
    "        ped2_char = [character[i] for i in ped2_ind]\n",
    "        # get number of times each one of them appears\n",
    "        ped2_numchar = list(ped2_T[ped2_T.iloc[:,1] >= 1 ].iloc[:,1])\n",
    "        # repeat characters if they occur multiple times\n",
    "        pedestrians2 = [elem for count, elem in zip(ped2_numchar, ped2_char) for _ in repeat(None, count)]\n",
    "        # reshuffle order\n",
    "        random.shuffle(pedestrians2)\n",
    "\n",
    "        ped1_dict = Counter(pedestrians1)\n",
    "        ped2_dict = Counter(pedestrians2)\n",
    "\n",
    "        pedestrians1_set = \"\"\n",
    "        nb_pedestrians1_set = 0\n",
    "        for i, (element, count) in enumerate(ped1_dict.items()):\n",
    "            nb_pedestrians1_set += count\n",
    "            pedestrians1_set += \"{} {}\".format(count, plural(element, count))\n",
    "            if i == len(ped1_dict) - 2:\n",
    "                if len(ped1_dict) == 2:\n",
    "                    pedestrians1_set += \" and \"\n",
    "                else:\n",
    "                    pedestrians1_set += \", and \"\n",
    "            elif i < len(ped1_dict) - 1:\n",
    "                pedestrians1_set += \", \"\n",
    "\n",
    "        pedestrians2_set = \"\"\n",
    "        nb_pedestrians2_set = 0\n",
    "        for i, (element, count) in enumerate(ped2_dict.items()):\n",
    "            nb_pedestrians2_set += count\n",
    "            pedestrians2_set += \"{} {}\".format(count, plural(element, count))\n",
    "            if i == len(ped2_dict) - 2:\n",
    "                if len(ped2_dict) == 2:\n",
    "                    pedestrians2_set += \" and \"\n",
    "                else:\n",
    "                    pedestrians2_set += \", and \"\n",
    "            elif i < len(ped2_dict) - 1:\n",
    "                pedestrians2_set += \", \"\n",
    "\n",
    "        # code legality for pedestrians 1\n",
    "        if ped1.iloc[0][\"CrossingSignal\"] == 0: \n",
    "            ped1_signal = \"NA\" \n",
    "        elif ped1.iloc[0][\"CrossingSignal\"] == 1:\n",
    "            ped1_signal = \"green\"\n",
    "        elif ped1.iloc[0][\"CrossingSignal\"] == 2:\n",
    "            ped1_signal  = \"red\"\n",
    "\n",
    "        # code legality for pedestrians 2 \n",
    "        if ped2.iloc[0][\"CrossingSignal\"] == 0: \n",
    "            ped2_signal = \"NA\" \n",
    "        elif ped2.iloc[0][\"CrossingSignal\"] == 1:\n",
    "            ped2_signal = \"green\"\n",
    "        elif ped2.iloc[0][\"CrossingSignal\"] == 2:\n",
    "            ped2_signal  = \"red\"\n",
    "        \n",
    "        # pedestrians 1 died because respondent let the AV swerve\n",
    "        if ped1.iloc[0][\"Intervention\"] == 1:\n",
    "            if ped1_signal != \"NA\":\n",
    "                ped1_description = ped1_description + base_description + \"swerve and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} {} in the other lane.\".format(\n",
    "                    pedestrians1_set,\n",
    "                    'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped1_signal],\n",
    "                )\n",
    "                ped2_description = ped2_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} {} ahead of the car.\".format(\n",
    "                    pedestrians2_set,\n",
    "                    'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped2_signal],\n",
    "                )\n",
    "            else:\n",
    "                ped1_description = ped1_description + base_description + \"swerve and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} crossing in the other lane.\".format(\n",
    "                    pedestrians1_set,\n",
    "                    'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                )\n",
    "                ped2_description = ped2_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} crossing ahead of the car.\".format(\n",
    "                    pedestrians2_set,\n",
    "                    'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                )\n",
    "        # pedestrians 2 died because respondet let the AV stay on course\n",
    "        else:\n",
    "            if ped1_signal != \"NA\":\n",
    "                ped1_description = ped1_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} {} ahead of the car.\".format(\n",
    "                    pedestrians1_set,\n",
    "                    'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped1_signal],\n",
    "                )\n",
    "                ped2_description = ped2_description + base_description + \"swerve and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} {} in the other lane.\".format(\n",
    "                    pedestrians2_set,\n",
    "                    'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped2_signal],\n",
    "                )\n",
    "            else:\n",
    "                ped1_description = ped1_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} crossing ahead of the car.\".format(\n",
    "                    pedestrians1_set,\n",
    "                    'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                )\n",
    "                ped2_description = ped2_description + base_description + \"swerve and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} crossing in the other lane.\".format(\n",
    "                    pedestrians2_set,\n",
    "                    'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                )\n",
    "\n",
    "        ped1_description = ped1_description + \" Meanwhile, it will spare {}\".format(pedestrians2_set)\n",
    "        if ped2_signal != \"NA\":\n",
    "            ped1_description = ped1_description + \", who {} {} \".format(\n",
    "                'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                traffic_light_pattern_descriptions[ped2_signal],\n",
    "            )\n",
    "        else:\n",
    "            ped1_description = ped1_description + \", who {} crossing \".format('were' if nb_pedestrians2_set >= 2 else 'was')\n",
    "\n",
    "        if ped1.iloc[0][\"Intervention\"] == 1:\n",
    "            ped1_description = ped1_description + \"ahead of the car.\\n\"\n",
    "        else:\n",
    "            ped1_description = ped1_description + \"in the other lane.\\n\"\n",
    "        \n",
    "        ped2_description = ped2_description + \" Meanwhile, it will spare {}\".format(pedestrians1_set)\n",
    "        if ped1_signal != \"NA\":\n",
    "            ped2_description = ped2_description + \", who {} {} \".format(\n",
    "                'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                traffic_light_pattern_descriptions[ped1_signal],\n",
    "            )\n",
    "        else:\n",
    "            ped2_description = ped2_description + \", who {} crossing \".format('were' if nb_pedestrians1_set >= 2 else 'was')\n",
    "\n",
    "        if ped1.iloc[0][\"Intervention\"] == 1:\n",
    "            ped2_description = ped2_description + \"in the other lane.\\n\"\n",
    "        else:\n",
    "            ped2_description = ped2_description + \"ahead of the car.\\n\"\n",
    "\n",
    "        c1_description = \"Case 1.\\n\" + ped1_description\n",
    "        c2_description = \"Case 2.\\n\" + ped2_description\n",
    "\n",
    "    # system content base description\n",
    "    system_content = \"Carefully consider the following information and think step by step. \"\n",
    "\n",
    "    # add information about survey respondent\n",
    "    if include_persona:\n",
    "        system_content += \"You are an American with the following characteristics:\\n\" + persona + \"\\n\"\n",
    "    else: \n",
    "        system_content += \"\\n\"\n",
    "\n",
    "    # add information about the structure of the scenario      \n",
    "    if is_pasvsped:\n",
    "        system_content += \"Consider that pedestrians are crossing a zebra crossing that spans two lanes. \"\n",
    "    else:\n",
    "        system_content += \"Consider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. \"\n",
    "    \n",
    "    # add task description and requested output format\n",
    "    system_content += \"Please indicate which case is better for autonomous driving by choosing either \\'Case 1\\' or \\'Case 2\\'. Respond with one word only.\\n\"\n",
    "    \n",
    "    # user content contains description of scenario\n",
    "    user_content = c1_description + \"\\n\" + c2_description\n",
    "\n",
    "    return system_content, user_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_response(row, column_name):\n",
    "\n",
    "    # row refers to one row in the dataframe with llm reponses\n",
    "    # column_name refers to the column with llm text output\n",
    "    text = row[ column_name ].lower()\n",
    "\n",
    "    # define regular expression for whether llm opted for 1 or 2\n",
    "    pattern_case1 = re.compile(r\"case\\s?1.?\")\n",
    "    pattern_case2 = re.compile(r\"case\\s?2.?\")\n",
    "\n",
    "    # whether text matches case1 and case2 pattern, respectively\n",
    "    match_case1 = pattern_case1.search(text) is not None\n",
    "    match_case2 = pattern_case2.search(text) is not None\n",
    "\n",
    "    if match_case1 and not match_case2:\n",
    "        case = 1\n",
    "    elif not match_case1 and match_case2:\n",
    "        case = 2\n",
    "    else:\n",
    "        case = np.NaN\n",
    "\n",
    "    # row refers to case 1 and llm chose case 1, so people described in case 1 are not saved\n",
    "    if  row[\"LeftHand\"]==1 and case==1: \n",
    "        saved = 0\n",
    "    # row refers to case 1 but llm chose case 2, so people described in case 2 are saved\n",
    "    elif row[\"LeftHand\"]==1 and case==2:\n",
    "        saved = 1\n",
    "    # row refers to case 2 and llm chose case 2, so people described in case 2 are not saved\n",
    "    elif row[\"LeftHand\"]==0 and case==2:\n",
    "        saved = 0\n",
    "    # row refers to case 2 but llm chose case 1, so people described in case 2 are saved\n",
    "    elif row[\"LeftHand\"]==0 and case==1:\n",
    "        saved = 1\n",
    "    else: \n",
    "        saved = np.NaN\n",
    "    \n",
    "\n",
    "    return saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_openai(data, model, api_key,csv_path, include_persona=True):\n",
    "\n",
    "    # data:          DataFrame with scenario descriptions and profiles of survey respondents\n",
    "    # model:         Name of model, e.g. GPT-4o\n",
    "    # api_key:       OpenAI API key for billing\n",
    "    # csv_path:      File path to existing .csv for saving output of API calls, creates .csv if not exists\n",
    "\n",
    "    # prompt \n",
    "    if os.path.exists(csv_path): \n",
    "\n",
    "        # get existing reponses\n",
    "        existing = pd.read_csv(csv_path, usecols = [\"ResponseID\"])\n",
    "        print(\"Existing responses:\", existing[\"ResponseID\"].unique().shape[0])\n",
    "\n",
    "        # define column indicating in which dataframe ResponseID is present \n",
    "        toprompt = pd.merge(data, existing, indicator=True, on=\"ResponseID\", how=\"left\")\n",
    "\n",
    "        # keep rows that haven't been used for prompting\n",
    "        ids_toprompt = toprompt.loc[toprompt['_merge'] == 'left_only', 'ResponseID'].unique()\n",
    "        print(\"Number of remaining prompts:\", len(ids_toprompt))\n",
    "\n",
    "    else:\n",
    "        ids_toprompt = data[\"ResponseID\"].unique()\n",
    "\n",
    "\n",
    "    if len(ids_toprompt) > 0: \n",
    "        \n",
    "        i = 1\n",
    "        for id in ids_toprompt: \n",
    "\n",
    "            # track progress\n",
    "            if i==1 or i % 50 == 0: \n",
    "                print(f\"Prompt {i} out of {len(ids_toprompt)}\")\n",
    "            i = i+1\n",
    "            \n",
    "            survey_response = data[ data[\"ResponseID\"]== id ]\n",
    "\n",
    "            prompt = generate_scenario(survey_response, include_persona=include_persona)\n",
    "\n",
    "            client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "            reply = client.chat.completions.create(   \n",
    "                model= model,\n",
    "                messages = [\n",
    "                        {\"role\": \"system\", \"content\": prompt[0]},\n",
    "                        {\"role\": \"user\", \"content\": prompt[1]}\n",
    "                    ],\n",
    "            )\n",
    "\n",
    "            llm_response = reply.choices[0].message.content\n",
    "\n",
    "            # prefix based on llm name and whether prompt contained the persona\n",
    "            column_prefix = re.sub('[-._ ]', '', model) + (\"_wp_\" if include_persona else \"_np_\")\n",
    "\n",
    "            # Create a dictionary for the new row\n",
    "            new_values = {\n",
    "                column_prefix+'Timestamp': datetime.now().isoformat(),\n",
    "                column_prefix+'SystemPrompt': prompt[0],\n",
    "                column_prefix+'UserPrompt': prompt[1],\n",
    "                column_prefix+'Persona': int(include_persona),\n",
    "                column_prefix+'Label': llm_response}        \n",
    "\n",
    "            survey_response = survey_response.assign(**new_values)\n",
    "\n",
    "            survey_response[column_prefix+\"Saved\"] = survey_response.apply(classify_response, column_name=column_prefix+\"Label\", axis=1)\n",
    "\n",
    "            if os.path.isfile(csv_path): \n",
    "                survey_response.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "            else: \n",
    "                survey_response.to_csv(csv_path, index = False)\n",
    "\n",
    "    else:\n",
    "        print(\"No remaining responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Dilemmas\n",
    "\n",
    "Below we illustrate how the entries in the data matrix describe a scenario. We created the images with the design functionalities of [moralmachine.net](https://www.moralmachine.net/). These images illustrate the examples but play no furhter role in the study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "On the left side (`LeftHand=1`), respondents saw an AV that swerves to the other lane (`Intervention=1`) and kills 5 pedestrians (`Barrier=0`) – 1 baby, 1 female athlete, 1 male athlete, 1 female doctor, and 1 cat – who were crossing on a green light (`CrossingSignal=1`).\n",
    "\n",
    "On the right side (`LeftHand=0`), respondents saw an AV that would continue ahead and crash into a barrier, resulting in the dealth of the 4 passengers (`Barrier=1`) - 1 baby, 1 female athlete, 1 female doctor, and a cat.\n",
    "\n",
    "This example is taken from the [supplementary material](https://osf.io/wt6mc?view_only=4bb49492edee4a8eb1758552a362a2cf) in Awad et al. (2018). \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../Figures/4_ScenarioExamples/2224g4ytARX4QT5rB.png\" alt=\"Example 1 2224g4ytARX4QT5rB (SI, Awad et al. 2018)\" width=\"70%\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With persona:\n",
      " (\"Carefully consider the following information and think step by step. You are an American with the following characteristics:\\n - You earn an annual income of 30,000 US dollars.\\n - Your highest level of education is a bachelor degree.\\n - On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of 24 for your religious views.\\n - You are a man.\\n - You are 36 years old.\\n - On a scale from 0 (Conservative) to 100 (Progressive), you report a score of 54 for your political views.\\nConsider that pedestrians are crossing a zebra crossing that spans two lanes. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will swerve to avoid crashing into a concrete barrier and drive through a pedestrian crossing in the other lane. This will result in the death of 1 baby, 1 cat, 1 male athlete, 1 female doctor, and 1 female athlete, who were abiding by the law by crossing on the green signal. Meanwhile, it will spare 1 female doctor, 1 female athlete, 1 baby, and 1 cat, who were inside the car.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will continue ahead and crash into a concrete barrier. This will result in the death of 1 female doctor, 1 female athlete, 1 baby, and 1 cat, who were inside the car. Meanwhile, it will spare 1 baby, 1 cat, 1 male athlete, 1 female doctor, and 1 female athlete, who were abiding by the law by crossing on the green signal.\\n') \n",
      "\n",
      "No persona:\n",
      " (\"Carefully consider the following information and think step by step. \\nConsider that pedestrians are crossing a zebra crossing that spans two lanes. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will swerve to avoid crashing into a concrete barrier and drive through a pedestrian crossing in the other lane. This will result in the death of 1 baby, 1 cat, 1 male athlete, 1 female doctor, and 1 female athlete, who were abiding by the law by crossing on the green signal. Meanwhile, it will spare 1 female doctor, 1 female athlete, 1 baby, and 1 cat, who were inside the car.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will continue ahead and crash into a concrete barrier. This will result in the death of 1 female doctor, 1 female athlete, 1 baby, and 1 cat, who were inside the car. Meanwhile, it will spare 1 baby, 1 cat, 1 male athlete, 1 female doctor, and 1 female athlete, who were abiding by the law by crossing on the green signal.\\n') \n",
      "\n",
      "Label assigned by LLM:  ['Case 2']\n",
      "Outcomes for these scenarios:\n",
      " 0    1\n",
      "1    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data1 = {\n",
    "    \"ResponseID\": [\"2224g4ytARX4QT5rB\", \"2224g4ytARX4QT5rB\"],\n",
    "    \"ExtendedSessionID\": [\"213978760_9992828917431898.0\", \"213978760_9992828917431898.0\"],\n",
    "    \"UserID\": [9.992829e+15, 9.992829e+15],\n",
    "    # Imputed demographics just for this illustration\n",
    "    \"Review_age\": [36,36],                     \n",
    "    \"Review_education\": [\"bachelor\",\"bachelor\"],\n",
    "    \"Review_gender\": [\"Man\",\"Man\"],\n",
    "    \"Review_ContinuousIncome\": [30000,30000],\n",
    "    \"Review_political\": [54,54],\n",
    "    \"Review_religious\": [24,24],\n",
    "    \"ScenarioOrder\": [7, 7],\n",
    "    \"Intervention\": [1, 0],\n",
    "    \"PedPed\": [0, 0],\n",
    "    \"Barrier\": [0, 1],\n",
    "    \"CrossingSignal\": [1, 0],\n",
    "    \"AttributeLevel\": [\"More\", \"Less\"],\n",
    "    \"ScenarioTypeStrict\": [\"Utilitarian\", \"Utilitarian\"],\n",
    "    \"ScenarioType\": [\"Utilitarian\", \"Utilitarian\"],\n",
    "    \"DefaultChoice\": [\"More\", \"More\"],\n",
    "    \"NonDefaultChoice\": [\"Less\", \"Less\"],\n",
    "    \"DefaultChoiceIsOmission\": [0, 0],\n",
    "    \"NumberOfCharacters\": [5, 4],\n",
    "    \"DiffNumberOFCharacters\": [1, 1],\n",
    "    \"Saved\": [0, 1],\n",
    "    'Label': ['Case 2','Case 2'],\n",
    "    \"Template\": [\"Desktop\", \"Desktop\"],\n",
    "    \"DescriptionShown\": [1, 1],\n",
    "    \"LeftHand\": [1, 0],\n",
    "    \"UserCountry3\": [\"USA\", \"USA\"],\n",
    "    \"Man\": [0, 0],\n",
    "    \"Woman\": [0, 0],\n",
    "    \"Pregnant\": [0, 0],\n",
    "    \"Stroller\": [1, 1],\n",
    "    \"OldMan\": [0, 0],\n",
    "    \"OldWoman\": [0, 0],\n",
    "    \"Boy\": [0, 0],\n",
    "    \"Girl\": [0, 0],\n",
    "    \"Homeless\": [0, 0],\n",
    "    \"LargeWoman\": [0, 0],\n",
    "    \"LargeMan\": [0, 0],\n",
    "    \"Criminal\": [0, 0],\n",
    "    \"MaleExecutive\": [0, 0],\n",
    "    \"FemaleExecutive\": [0, 0],\n",
    "    \"FemaleAthlete\": [1, 1],\n",
    "    \"MaleAthlete\": [1, 0],\n",
    "    \"FemaleDoctor\": [1, 1],\n",
    "    \"MaleDoctor\": [0, 0],\n",
    "    \"Dog\": [0, 0],\n",
    "    \"Cat\": [1, 1]\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    " \n",
    "\n",
    "print(\"With persona:\\n\",generate_scenario(df1, include_persona=True),\"\\n\")\n",
    "print(\"No persona:\\n\", generate_scenario(df1, include_persona=False),\"\\n\")\n",
    "print(\"Label assigned by LLM: \",df1[\"Label\"].unique())\n",
    "print(\"Outcomes for these scenarios:\\n\",df1.apply(classify_response, column_name = \"Label\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "These scenarios pit two groups of pedestrians against each other (`PedPed=1`).  \n",
    "\n",
    "On the left side of the screen (`LeftHand=1`), respondents saw a scenario in which the AV stays on course (`Intervention=0`), resulting in the death of 1 man who was crossing on a red signal (`CrossingSignal=2`). \n",
    "\n",
    "On the right side of the screen (`LeftHand=0`), respondents saw a scenario in which the AV swerves to the other lane (`Intervention=1`), resulting in the death 1 male athlete who was crossing on a green signal (`CrossingSignal=1`).\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../Figures/4_ScenarioExamples/22qKv8AmPcXEnNd8z.png\" width=\"70%\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With persona:\n",
      " (\"Carefully consider the following information and think step by step. You are an American with the following characteristics:\\n - You earn an annual income of 12,500 US dollars.\\n - Your highest level of education is a high school diploma.\\n - On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of 0 for your religious views.\\n - You are a man.\\n - You are 29 years old.\\n - On a scale from 0 (Conservative) to 100 (Progressive), you report a score of 100 for your political views.\\nConsider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 man, who was flouting the law by crossing on the red signal ahead of the car. Meanwhile, it will spare 1 male athlete, who was abiding by the law by crossing on the green signal in the other lane.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and drive through a pedestrian crossing in the other lane. This will result in the death of 1 male athlete, who was abiding by the law by crossing on the green signal in the other lane. Meanwhile, it will spare 1 man, who was flouting the law by crossing on the red signal ahead of the car.\\n') \n",
      "\n",
      "No persona:\n",
      " (\"Carefully consider the following information and think step by step. \\nConsider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 man, who was flouting the law by crossing on the red signal ahead of the car. Meanwhile, it will spare 1 male athlete, who was abiding by the law by crossing on the green signal in the other lane.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and drive through a pedestrian crossing in the other lane. This will result in the death of 1 male athlete, who was abiding by the law by crossing on the green signal in the other lane. Meanwhile, it will spare 1 man, who was flouting the law by crossing on the red signal ahead of the car.\\n') \n",
      "\n",
      "Label assigned by LLM:  ['Case 1']\n",
      "Outcomes for these scenarios:\n",
      " 0    0\n",
      "1    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data2 = {\n",
    "    \"ExtendedSessionID\": [\"1055565952_8316216477776195.0\", \"1055565952_8316216477776195.0\"],\n",
    "    \"ResponseID\": [\"22qKv8AmPcXEnNd8z\", \"22qKv8AmPcXEnNd8z\"],\n",
    "    \"UserID\": [8.316216e+15, 8.316216e+15],\n",
    "    \"Review_age\": [29, 29],\n",
    "    \"Review_education\": [\"high\",\"high\"],\n",
    "    \"Review_income\": [\"10000\", \"10000\"],\n",
    "    \"Review_gender\": [\"Man\", \"Man\"],\n",
    "    \"Review_ContinuousIncome\": [12500,12500],\n",
    "    \"IncomeBracketSmall\": [\"$5,001-\\n$25,000\", \"$5,001-\\n$25,000\"],\n",
    "    \"Review_political\": [100, 100],\n",
    "    \"Review_religious\": [0, 0],\n",
    "    \"ScenarioOrder\": [6, 6],\n",
    "    \"Intervention\": [0, 1],\n",
    "    \"PedPed\": [1, 1],\n",
    "    \"Barrier\": [0, 0],\n",
    "    \"CrossingSignal\": [2, 1],\n",
    "    \"AttributeLevel\": [\"Fat\", \"Fit\"],\n",
    "    \"ScenarioTypeStrict\": [\"Fitness\", \"Fitness\"],\n",
    "    \"ScenarioType\": [\"Fitness\", \"Fitness\"],\n",
    "    \"DefaultChoice\": [\"Fit\", \"Fit\"],\n",
    "    \"NonDefaultChoice\": [\"Fat\", \"Fat\"],\n",
    "    \"DefaultChoiceIsOmission\": [0, 0],\n",
    "    \"NumberOfCharacters\": [1, 1],\n",
    "    \"DiffNumberOFCharacters\": [0, 0],\n",
    "    \"Saved\": [0, 1],\n",
    "    'Label': ['Case 1','Case 1'],\n",
    "    \"Template\": [\"Desktop\", \"Desktop\"],\n",
    "    \"DescriptionShown\": [1, 1],\n",
    "    \"LeftHand\": [1, 0],\n",
    "    \"UserCountry3\": [\"USA\", \"USA\"],\n",
    "    \"Man\": [1, 0],\n",
    "    \"Woman\": [0, 0],\n",
    "    \"Pregnant\": [0, 0],\n",
    "    \"Stroller\": [0, 0],\n",
    "    \"OldMan\": [0, 0],\n",
    "    \"OldWoman\": [0, 0],\n",
    "    \"Boy\": [0, 0],\n",
    "    \"Girl\": [0, 0],\n",
    "    \"Homeless\": [0, 0],\n",
    "    \"LargeWoman\": [0, 0],\n",
    "    \"LargeMan\": [0, 0],\n",
    "    \"Criminal\": [0, 0],\n",
    "    \"MaleExecutive\": [0, 0],\n",
    "    \"FemaleExecutive\": [0, 0],\n",
    "    \"FemaleAthlete\": [0, 0],\n",
    "    \"MaleAthlete\": [0, 1],\n",
    "    \"FemaleDoctor\": [0, 0],\n",
    "    \"MaleDoctor\": [0, 0],\n",
    "    \"Dog\": [0, 0],\n",
    "    \"Cat\": [0, 0],\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(data2)\n",
    " \n",
    "\n",
    "print(\"With persona:\\n\",generate_scenario(df2, include_persona=True),\"\\n\")\n",
    "print(\"No persona:\\n\", generate_scenario(df2, include_persona=False),\"\\n\")\n",
    "print(\"Label assigned by LLM: \",df2[\"Label\"].unique())\n",
    "print(\"Outcomes for these scenarios:\\n\",df2.apply(classify_response, column_name = \"Label\", axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "This scenario pits pedestrians against pedestrians (`PedPed=1`). On the left side of the screen (`LeftHand=1`), respondents saw a scenario in which the AV would stay on course (`Intervention=0`), resulting in the death of 1 male executive who was crossing (`CrossingSignal=0`).\n",
    "\n",
    "On the right side of the screen (`LeftSide=0`), respodents saw a scenario in which the AV would swerve (`Intervention=1`), resulting in the death of a 1 female executive who was crossing (`CrossingSignal=0`).\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../Figures/4_ScenarioExamples/A6GmXsYKGxyivAFzu.png\" width=\"70%\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With persona:\n",
      " (\"Carefully consider the following information and think step by step. You are an American with the following characteristics:\\n - On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of 46 for your religious views.\\n - On a scale from 0 (Conservative) to 100 (Progressive), you report a score of 11 for your political views.\\n - You earn an annual income of 42,500 US dollars.\\n - Your highest level of education is a bachelor degree.\\n - You are 46 years old.\\n - You are a woman.\\nConsider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 male executive, who was crossing ahead of the car. Meanwhile, it will spare 1 female executive, who was crossing in the other lane.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and drive through a pedestrian crossing in the other lane. This will result in the death of 1 female executive, who was crossing in the other lane. Meanwhile, it will spare 1 male executive, who was crossing ahead of the car.\\n') \n",
      "\n",
      "No persona:\n",
      " (\"Carefully consider the following information and think step by step. \\nConsider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 male executive, who was crossing ahead of the car. Meanwhile, it will spare 1 female executive, who was crossing in the other lane.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and drive through a pedestrian crossing in the other lane. This will result in the death of 1 female executive, who was crossing in the other lane. Meanwhile, it will spare 1 male executive, who was crossing ahead of the car.\\n') \n",
      "\n",
      "Label assigned by LLM:  ['Case 1']\n",
      "Outcomes for these scenarios:\n",
      " 0    0\n",
      "1    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data3 = {\n",
    "    'ExtendedSessionID': ['1694978322_3759038854820315.0', '1694978322_3759038854820315.0'],\n",
    "    'ResponseID': ['A6GmXsYKGxyivAFzu', 'A6GmXsYKGxyivAFzu'],\n",
    "    'UserID': [3.759039e+15, 3.759039e+15],\n",
    "    'Review_age': [46, 46],\n",
    "    'Review_education': ['bachelor','bachelor'],\n",
    "    'Review_gender': ['Woman', 'Woman'],\n",
    "    'Review_income': ['35000', '35000'],\n",
    "    \"Review_ContinuousIncome\": [42500,42500],\n",
    "    'IncomeBracketSmall': ['$25,001-\\n$50,000', '$25,001-\\n$50,000'],\n",
    "    'Review_political': [11, 11],\n",
    "    'Review_religious': [46, 46],\n",
    "    'ScenarioOrder': [1, 1],\n",
    "    'Intervention': [0, 1],\n",
    "    'PedPed': [1, 1],\n",
    "    'Barrier': [0, 0],\n",
    "    'CrossingSignal': [0, 0],\n",
    "    'AttributeLevel': ['Male', 'Female'],\n",
    "    'ScenarioTypeStrict': ['Gender', 'Gender'],\n",
    "    'ScenarioType': ['Gender', 'Gender'],\n",
    "    'DefaultChoice': ['Male', 'Female'],\n",
    "    'NonDefaultChoice': ['Male', 'Female'],\n",
    "    'DefaultChoiceIsOmission': [1, 1],\n",
    "    'NumberOfCharacters': [1, 1],\n",
    "    'DiffNumberOFCharacters': [0, 0],\n",
    "    'Saved': [0, 1],\n",
    "    'Label': ['Case 1','Case 1'],\n",
    "    'Template': ['Desktop', 'Desktop'],\n",
    "    'DescriptionShown': [0, 0],\n",
    "    'LeftHand': [1, 0],\n",
    "    'UserCountry3': ['USA', 'USA'],\n",
    "    'Man': [0, 0],\n",
    "    'Woman': [0, 0],\n",
    "    'Pregnant': [0, 0],\n",
    "    'Stroller': [0, 0],\n",
    "    'OldMan': [0, 0],\n",
    "    'OldWoman': [0, 0],\n",
    "    'Boy': [0, 0],\n",
    "    'Girl': [0, 0],\n",
    "    'Homeless': [0, 0],\n",
    "    'LargeWoman': [0, 0],\n",
    "    'LargeMan': [0, 0],\n",
    "    'Criminal': [0, 0],\n",
    "    'MaleExecutive': [1, 0],\n",
    "    'FemaleExecutive': [0, 1],\n",
    "    'FemaleAthlete': [0, 0],\n",
    "    'MaleAthlete': [0, 0],\n",
    "    'FemaleDoctor': [0, 0],\n",
    "    'MaleDoctor': [0, 0],\n",
    "    'Dog': [0, 0],\n",
    "    'Cat': [0, 0]\n",
    "}\n",
    "\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "print(\"With persona:\\n\",generate_scenario(df3, include_persona=True),\"\\n\")\n",
    "print(\"No persona:\\n\", generate_scenario(df3, include_persona=False),\"\\n\")\n",
    "print(\"Label assigned by LLM: \",df3[\"Label\"].unique())\n",
    "print(\"Outcomes for these scenarios:\\n\",df3.apply(classify_response, column_name = \"Label\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "\n",
    "On the left side of the screen (`LeftHand=1`), respondents saw a scenario in which an AV would stay on course (`Intervention=0`), resulting in the death of 5 pedestrians (`Barrier=0`) – 1 man, 1 woman, 2 boys, and 1 girl – who were crossing on a green light (`CrossingSignal=1`). \n",
    "\n",
    "On the right side of the screen (`LeftHand=0`), respondents saw a scenario in which an AV would swerve onto the other lane (`Intervention=1`), resulting in the death of the 5 passengers (`Barrier=1`) – 1 man, 1 woman, 2 old men, and 1 old woman. \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../Figures/4_ScenarioExamples/EH3SfatQP3hygSpzF.png\" width=\"70%\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With persona:\n",
      " (\"Carefully consider the following information and think step by step. You are an American with the following characteristics:\\n - On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of 0 for your religious views.\\n - On a scale from 0 (Conservative) to 100 (Progressive), you report a score of 100 for your political views.\\n - You earn an annual income of Under $5,000 US dollars.\\n - Your highest level of education is a high school diploma.\\n - You are 35 years old.\\n - You are a man.\\nConsider that pedestrians are crossing a zebra crossing that spans two lanes. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 man, 1 girl, 2 boys, and 1 woman, who were abiding by the law by crossing on the green signal. Meanwhile, it will spare 1 man, 1 elderly woman, 2 elderly men, and 1 woman, who were inside the car.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and crash into a concrete barrier. This will result in the death of 1 man, 1 elderly woman, 2 elderly men, and 1 woman, who were inside the car. Meanwhile, it will spare 1 man, 1 girl, 2 boys, and 1 woman, who were abiding by the law by crossing on the green signal.\\n') \n",
      "\n",
      "No persona:\n",
      " (\"Carefully consider the following information and think step by step. \\nConsider that pedestrians are crossing a zebra crossing that spans two lanes. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 man, 1 girl, 2 boys, and 1 woman, who were abiding by the law by crossing on the green signal. Meanwhile, it will spare 1 man, 1 elderly woman, 2 elderly men, and 1 woman, who were inside the car.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and crash into a concrete barrier. This will result in the death of 1 man, 1 elderly woman, 2 elderly men, and 1 woman, who were inside the car. Meanwhile, it will spare 1 man, 1 girl, 2 boys, and 1 woman, who were abiding by the law by crossing on the green signal.\\n') \n",
      "\n",
      "Label assigned by LLM:  ['Case 1']\n",
      "Outcomes for these scenarios:\n",
      " 0    0\n",
      "1    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data4 = {\n",
    "    'ExtendedSessionID': ['-2127483756_5144602155778557.0', '-2127483756_5144602155778557.0'],\n",
    "    'ResponseID': ['EH3SfatQP3hygSpzF', 'EH3SfatQP3hygSpzF'],\n",
    "    'UserID': [5.144602e+15, 5.144602e+15],\n",
    "    'Review_gender': ['Man', 'Man'],\n",
    "    'Review_age': [35, 35],\n",
    "    'Review_ageBracket': ['35-44','35-44'],\n",
    "    'Review_income': ['under5000', 'under5000'],\n",
    "    'Review_ContinuousIncome': [2500,2500],\n",
    "    'IncomeBracketSmall': ['$0-$5,000', '$0-$5,000'],\n",
    "    'Review_education': ['high','high'],\n",
    "    'Review_educationBracket': ['High school','High school'],\n",
    "    'Review_political': [100, 100],\n",
    "    'Review_religious': [0, 0],\n",
    "    'ScenarioOrder': [3, 3],\n",
    "    'Intervention': [0, 1],\n",
    "    'PedPed': [0, 0],\n",
    "    'Barrier': [0, 1],\n",
    "    'CrossingSignal': [1, 0],\n",
    "    'AttributeLevel': ['Young', 'Old'],\n",
    "    'ScenarioTypeStrict': ['Age', 'Age'],\n",
    "    'ScenarioType': ['Age', 'Age'],\n",
    "    'DefaultChoice': ['Young', 'Young'],\n",
    "    'NonDefaultChoice': ['Old', 'Old'],\n",
    "    'DefaultChoiceIsOmission': [1, 1],\n",
    "    'NumberOfCharacters': [5, 5],\n",
    "    'DiffNumberOFCharacters': [0, 0],\n",
    "    'Saved': [0, 1],\n",
    "    'Label': ['Case 1','Case 1'],\n",
    "    'Template': ['Mobile', 'Mobile'],\n",
    "    'DescriptionShown': [0, 0],\n",
    "    'LeftHand': [1, 0],\n",
    "    'UserCountry3': ['USA', 'USA'],\n",
    "    'Man': [1, 1],\n",
    "    'Woman': [1, 1],\n",
    "    'Pregnant': [0, 0],\n",
    "    'Stroller': [0, 0],\n",
    "    'OldMan': [0, 2],\n",
    "    'OldWoman': [0, 1],\n",
    "    'Boy': [2, 0],\n",
    "    'Girl': [1, 0],\n",
    "    'Homeless': [0, 0],\n",
    "    'LargeWoman': [0, 0],\n",
    "    'LargeMan': [0, 0],\n",
    "    'Criminal': [0, 0],\n",
    "    'MaleExecutive': [0, 0],\n",
    "    'FemaleExecutive': [0, 0],\n",
    "    'FemaleAthlete': [0, 0],\n",
    "    'MaleAthlete': [0, 0],\n",
    "    'FemaleDoctor': [0, 0],\n",
    "    'MaleDoctor': [0, 0],\n",
    "    'Dog': [0, 0],\n",
    "    'Cat': [0, 0],\n",
    "}\n",
    "\n",
    "df4 = pd.DataFrame(data4)\n",
    "\n",
    "\n",
    "print(\"With persona:\\n\",generate_scenario(df4, include_persona=True),\"\\n\")\n",
    "print(\"No persona:\\n\", generate_scenario(df4, include_persona=False),\"\\n\")\n",
    "print(\"Label assigned by LLM: \",df4[\"Label\"].unique())\n",
    "print(\"Outcomes for these scenarios:\\n\",df4.apply(classify_response, column_name = \"Label\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts 3025 \n",
      "\n",
      "ExtendedSessionID          [-2146351809_5371561028316529.0, -2144898372_3...\n",
      "ResponseID                 [4BcEoFnJFr32fF3Cm, 8oCKQGzdRGw8wRS5g, JeECaPs...\n",
      "UserID                     [5371561028316530.0, 3426768953735780.0, 73387...\n",
      "Review_gender                                                   [Man, Woman]\n",
      "Review_age                 [56, 43, 73, 16, 19, 23, 46, 21, 35, 29, 47, 2...\n",
      "Review_ageBracket                 [55-64, 35-44, 65-74, 15-24, 45-54, 25-34]\n",
      "Review_income              [35000, 80000, 25000, 5000, under5000, above10...\n",
      "Review_ContinuousIncome    [42500, 90000, 30000, 7500, 2500, 150000, 6500...\n",
      "IncomeBracketSmall         [$25,001-\\n$50,000, $50,001-\\n$100,000, $5,001...\n",
      "Review_education           [high, bachelor, underHigh, college, graduate,...\n",
      "Review_educationBracket    [High school, Some college, Less than high sch...\n",
      "Review_political           [50, 70, 77, 100, 18, 88, 52, 78, 80, 28, 13, ...\n",
      "Review_religious           [0, 50, 100, 30, 92, 44, 67, 71, 14, 51, 22, 1...\n",
      "ScenarioOrder                    [10, 9, 1, 8, 3, 12, 7, 13, 5, 11, 4, 6, 2]\n",
      "Intervention                                                          [0, 1]\n",
      "PedPed                                                                [1, 0]\n",
      "Barrier                                                               [0, 1]\n",
      "CrossingSignal                                                     [1, 2, 0]\n",
      "AttributeLevel             [Less, More, Hoomans, Pets, Fat, Fit, Rand, Fe...\n",
      "ScenarioTypeStrict         [Utilitarian, Species, Fitness, Random, Gender...\n",
      "ScenarioType               [Utilitarian, Species, Fitness, Random, Gender...\n",
      "DefaultChoice                   [More, Hoomans, Fit, nan, Male, Young, High]\n",
      "NonDefaultChoice                    [Less, Pets, Fat, nan, Female, Old, Low]\n",
      "DefaultChoiceIsOmission                                      [0.0, 1.0, nan]\n",
      "NumberOfCharacters                                           [3, 5, 4, 1, 2]\n",
      "DiffNumberOFCharacters                                       [2, 0, 1, 3, 4]\n",
      "Saved                                                                 [0, 1]\n",
      "Template                                                   [Desktop, Mobile]\n",
      "DescriptionShown                                                      [0, 1]\n",
      "LeftHand                                                              [0, 1]\n",
      "UserCountry3                                                           [USA]\n",
      "Man                                                          [0, 1, 2, 3, 4]\n",
      "Woman                                                        [0, 2, 1, 4, 3]\n",
      "Pregnant                                                        [0, 1, 2, 3]\n",
      "Stroller                                                        [1, 0, 2, 3]\n",
      "OldMan                                                       [0, 1, 3, 2, 4]\n",
      "OldWoman                                                     [0, 4, 1, 3, 2]\n",
      "Boy                                                          [0, 2, 1, 3, 4]\n",
      "Girl                                                         [0, 2, 1, 3, 5]\n",
      "Homeless                                                  [0, 1, 2, 3, 4, 5]\n",
      "LargeWoman                                                   [1, 0, 2, 3, 4]\n",
      "LargeMan                                                     [0, 1, 2, 3, 4]\n",
      "Criminal                                                        [0, 1, 2, 3]\n",
      "MaleExecutive                                                   [0, 1, 2, 3]\n",
      "FemaleExecutive                                                 [1, 2, 0, 3]\n",
      "FemaleAthlete                                             [0, 3, 2, 1, 4, 5]\n",
      "MaleAthlete                                                  [0, 1, 3, 2, 4]\n",
      "FemaleDoctor                                                    [0, 1, 2, 3]\n",
      "MaleDoctor                                                      [0, 1, 2, 3]\n",
      "Dog                                                       [0, 3, 1, 2, 4, 5]\n",
      "Cat                                                       [0, 1, 2, 3, 4, 5]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "## load survey data\n",
    "mms = pd.read_csv(\"https://raw.githubusercontent.com/davidbroska/IntegrativeExperimentsGAI/main/Data/2_SurveySample.csv.gz\")\n",
    "\n",
    "# api key for OpenAI\n",
    "api_key = \"\"\n",
    "\n",
    "# do a small sample first\n",
    "random.seed(2024)\n",
    "mms = mms.head(6050)\n",
    "\n",
    "# number of prompts\n",
    "print(\"Number of prompts\", len(mms[\"ResponseID\"].unique()), \"\\n\")\n",
    "\n",
    "# structure of dataset\n",
    "print(pd.Series({c: mms[c].unique() for c in mms}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inc:  [ 42500  90000  30000   7500   2500 150000  65000  20000  12500]\n",
      "Edu: ['high' 'bachelor' 'underHigh' 'college' 'graduate' 'vocational']\n",
      "Gen: ['Man' 'Woman']\n",
      "Age: [56 43 73 16 19 23 46 21 35 29 47 27 40 18 28 53 37 66 44 49 33 30 42 60\n",
      " 45 36 48 69 34 38 17 55 70 26 51 72 58 20 57 65 31 25 64 24 22 61 39 68\n",
      " 32 50 59 41 67 62 52]\n",
      "Pol: [ 50  70  77 100  18  88  52  78  80  28  13  65  41   0  82   6  63  96\n",
      "  31  84  66  87  79  95  92  10  90   3  61  69  57  25  75  76   7  17\n",
      "  59  48  74  99  43  24  73  86  23  93  30  62   4  97  85  89  55  83\n",
      "  54  42   1  67  68  72  19  12  47  38  64  21  49  37  39  60  26  71\n",
      "  11  91  36   9]\n",
      "Rel: [  0  50 100  30  92  44  67  71  14  51  22  17   6   5  24  77  64  78\n",
      "  36  19  28   2  32  62  25  75   1  16  85  33  73  84  57  68  65  88\n",
      "  11  18  80  76   4  56  26   9  42  86  90  72  21   3  66  79   8  63\n",
      "  70  34  99  15  10  54  20  60  27  82  94  95  13  55]\n"
     ]
    }
   ],
   "source": [
    "# check that there are no NAs in demographics\n",
    "print(\"Inc: \",mms[\"Review_ContinuousIncome\"].unique())\n",
    "print(\"Edu:\", mms[\"Review_education\"].unique())\n",
    "print(\"Gen:\", mms[\"Review_gender\"].unique())\n",
    "print(\"Age:\", mms[\"Review_age\"].unique())\n",
    "print(\"Pol:\", mms[\"Review_political\"].unique())\n",
    "print(\"Rel:\", mms[\"Review_religious\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-3.5-turbo-0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 out of 3025\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLocalProtocolError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_transports/default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[39m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[39m.\u001b[39;49mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[39m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:93\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m     91\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msend_request_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     92\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     94\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39msend_request_body\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs) \u001b[39mas\u001b[39;00m trace:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:151\u001b[0m, in \u001b[0;36mHTTP11Connection._send_request_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    149\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mwrite\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 151\u001b[0m \u001b[39mwith\u001b[39;49;00m map_exceptions({h11\u001b[39m.\u001b[39;49mLocalProtocolError: LocalProtocolError}):\n\u001b[1;32m    152\u001b[0m     event \u001b[39m=\u001b[39;49m h11\u001b[39m.\u001b[39;49mRequest(\n\u001b[1;32m    153\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    154\u001b[0m         target\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl\u001b[39m.\u001b[39;49mtarget,\n\u001b[1;32m    155\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    156\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mLocalProtocolError\u001b[0m: Illegal header value b'Bearer '",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalProtocolError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    953\u001b[0m         request,\n\u001b[1;32m    954\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    955\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    957\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_transports/default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[39mwith\u001b[39;49;00m map_httpcore_exceptions():\n\u001b[1;32m    233\u001b[0m     resp \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_transports/default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mLocalProtocolError\u001b[0m: Illegal header value b'Bearer '",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# with persona\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m prompt_openai(mms,model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo-0125\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m               csv_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../Data/gpt-3.5-turbo-0125_wp_20240603.csv.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               api_key\u001b[39m=\u001b[39;49mapi_key, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m               include_persona\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m prompt \u001b[39m=\u001b[39m generate_scenario(survey_response, include_persona\u001b[39m=\u001b[39minclude_persona)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m client \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mOpenAI(api_key\u001b[39m=\u001b[39mapi_key)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m reply \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(   \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     model\u001b[39m=\u001b[39;49m model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     messages \u001b[39m=\u001b[39;49m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt[\u001b[39m0\u001b[39;49m]},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt[\u001b[39m1\u001b[39;49m]}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m llm_response \u001b[39m=\u001b[39m reply\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X35sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# prefix based on llm name and whether prompt contained the persona\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    591\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    592\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    593\u001b[0m             {\n\u001b[1;32m    594\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    595\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    596\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    597\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    598\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    599\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    600\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    601\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    615\u001b[0m             },\n\u001b[1;32m    616\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    617\u001b[0m         ),\n\u001b[1;32m    618\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    619\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    620\u001b[0m         ),\n\u001b[1;32m    621\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    622\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    623\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    624\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:976\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    973\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered Exception\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    975\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 976\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    977\u001b[0m         options,\n\u001b[1;32m    978\u001b[0m         cast_to,\n\u001b[1;32m    979\u001b[0m         retries,\n\u001b[1;32m    980\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    981\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    982\u001b[0m         response_headers\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    986\u001b[0m \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:976\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    973\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered Exception\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    975\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 976\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    977\u001b[0m         options,\n\u001b[1;32m    978\u001b[0m         cast_to,\n\u001b[1;32m    979\u001b[0m         retries,\n\u001b[1;32m    980\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    981\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    982\u001b[0m         response_headers\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    986\u001b[0m \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:986\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[1;32m    977\u001b[0m             options,\n\u001b[1;32m    978\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    982\u001b[0m             response_headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    983\u001b[0m         )\n\u001b[1;32m    985\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 986\u001b[0m     \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    988\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    989\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHTTP Response: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    990\u001b[0m     request\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     response\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    995\u001b[0m )\n\u001b[1;32m    996\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mrequest_id: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mx-request-id\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "# with persona\n",
    "prompt_openai(mms,model=\"gpt-3.5-turbo-0125\", \n",
    "              csv_path=\"../Data/gpt-3.5-turbo-0125_wp_20240603.csv.gz\", \n",
    "              api_key=api_key, \n",
    "              include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 3025\n",
      "Number of remaining prompts: 0\n",
      "No remaining responses.\n"
     ]
    }
   ],
   "source": [
    "# no persona\n",
    "prompt_openai(mms,model=\"gpt-3.5-turbo-0125\", \n",
    "              csv_path=\"../Data/4_gpt-3.5-turbo-0125_np_20240603.csv.gz\", \n",
    "              api_key=api_key, \n",
    "              include_persona=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 3025\n",
      "Number of remaining prompts: 0\n",
      "No remaining responses.\n"
     ]
    }
   ],
   "source": [
    "# with persona\n",
    "prompt_openai(mms,model=\"gpt-4o\", \n",
    "              csv_path=\"../Data/4_gpt-4o_wp_20240603.csv.gz\", \n",
    "              api_key=api_key, \n",
    "              include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 3025\n",
      "Number of remaining prompts: 0\n",
      "No remaining responses.\n"
     ]
    }
   ],
   "source": [
    "# no persona\n",
    "prompt_openai(mms,model=\"gpt-4o\", \n",
    "              csv_path=\"../Data/4_gpt-4o_np_20240603.csv.gz\", \n",
    "              api_key=api_key, \n",
    "              include_persona=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 3025\n",
      "Number of remaining prompts: 0\n",
      "No remaining responses.\n"
     ]
    }
   ],
   "source": [
    "prompt_openai(mms,model=\"gpt-4-turbo\", \n",
    "              csv_path=\"../Data/4_gpt-4-turbo_wp_20240603.csv.gz\", \n",
    "              api_key=api_key, \n",
    "              include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 3025\n",
      "Number of remaining prompts: 0\n",
      "No remaining responses.\n"
     ]
    }
   ],
   "source": [
    "prompt_openai(mms,model=\"gpt-4-turbo\", \n",
    "              csv_path=\"../Data/4_gpt-4-turbo_np_20240603.csv.gz\", \n",
    "              api_key=api_key, \n",
    "              include_persona=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT3.5 with persona:    N= 3025 r= 0.11\n",
      "GPT3.5 no persona:      N= 3025 r= 0.17\n",
      "GPT4o with persona:     N= 3025 r= 0.3\n",
      "GPT4o no persona:       N= 3025 r= 0.29\n",
      "GPT turbo with persona: N= 3025 r= 0.35\n",
      "GPT turbo no persona:   N= 3025 r= 0.34\n"
     ]
    }
   ],
   "source": [
    "topn = 10000\n",
    "gpt35_wp = pd.read_csv(\"../Data/4_gpt-3.5-turbo-0125_wp_20240603.csv.gz\").head(topn)\n",
    "gpt35_np = pd.read_csv(\"../Data/4_gpt-3.5-turbo-0125_np_20240603.csv.gz\").head(topn)\n",
    "print(\"GPT3.5 with persona:    N=\",gpt35_wp.shape[0]//2, \"r=\",round(gpt35_wp[\"Saved\"].corr(gpt35_wp[\"gpt35turbo0125_wp_Saved\"], method='pearson'),2))\n",
    "print(\"GPT3.5 no persona:      N=\",gpt35_wp.shape[0]//2, \"r=\",round(gpt35_np[\"Saved\"].corr(gpt35_np[\"gpt35turbo0125_np_Saved\"], method='pearson'),2))\n",
    "\n",
    "gpt4o_wp = pd.read_csv(\"../Data/4_gpt-4o_wp_20240603.csv.gz\").head(topn)\n",
    "gpt4o_np = pd.read_csv(\"../Data/4_gpt-4o_np_20240603.csv.gz\").head(topn)\n",
    "print(\"GPT4o with persona:     N=\",gpt4o_wp.shape[0]//2, \"r=\",round(gpt4o_wp[\"Saved\"].corr(gpt4o_wp[\"gpt4o_wp_Saved\"], method='pearson'),2))\n",
    "print(\"GPT4o no persona:       N=\",gpt4o_np.shape[0]//2, \"r=\",round(gpt4o_np[\"Saved\"].corr(gpt4o_np[\"gpt4o_np_Saved\"], method='pearson'),2))\n",
    "\n",
    "gpt4turbo_wp = pd.read_csv(\"../Data/4_gpt-4-turbo_wp_20240603.csv.gz\").head(topn)\n",
    "gpt4turbo_np = pd.read_csv(\"../Data/4_gpt-4-turbo_np_20240603.csv.gz\").head(topn)\n",
    "print(\"GPT turbo with persona: N=\",gpt4turbo_wp.shape[0]//2, \"r=\",round(gpt4turbo_wp[\"Saved\"].corr(gpt4turbo_wp[\"gpt4turbo_wp_Saved\"], method='pearson'),2))\n",
    "print(\"GPT turbo no persona:   N=\",gpt4turbo_np.shape[0]//2, \"r=\",round(gpt4turbo_np[\"Saved\"].corr(gpt4turbo_np[\"gpt4turbo_np_Saved\"], method='pearson'),2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
