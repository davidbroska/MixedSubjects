{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prompt LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates a prompt for the LLM with a demographic profile of a survey respondents and a description of the two scenarios that the respondents encountered during the experiments. \n",
    "\n",
    "The demographic profile on the age, education, gender, and income that survey respondents reported after the Moral Machine experiment. The order in which these characteristics appear is randomized. It was up to the respondents to decide whether to take the survey or not.\n",
    "\n",
    "The description of the scenarios is generated from the data matrix. One dilemma consists of two scenarios, presented side by side during the experiment. If respondents click on the left side, they opt for the death of the characters represented in that outcome while the characters on the right side will be saved (and vice versa). \n",
    "\n",
    "To generate the descriptions of the dilemmas, we extended code written by Takemoto (2024). This study generates new dilemmas by randomly combining features of scenarios (e.g. the composition of characters) and prompts LLMs to evaluate these dilemmas. In contrast, our study takes the existing dilemmas from Awad et al. (2018) and prompts LLMs to predict how survey respondents evluated the dilemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/md/0h30crhs7xb714g8zy7dzj1m0000gn/T/ipykernel_67959/1965502107.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import random\n",
    "import openai\n",
    "import anthropic\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scenario(response, include_persona=True):\n",
    "\n",
    "    if include_persona:\n",
    "\n",
    "        # \"Annual income, including tips, dividends, interest, etc (in US dollars)\" [dropdown menu]\n",
    "        inc_descriptions = {\n",
    "            2500:   \"under 5,000\",                               # \"Under $5,000\"\n",
    "            7500:   \"7,500\",                                     # \"$5,000-$10,000\"\n",
    "            12500:  \"12,500\",                                    # \"$10,001-$15,000\"\n",
    "            20000:  \"20,000\",                                    # \"$15,001-$25,000\"\n",
    "            30000:  \"30,000\",                                    # \"$25,001-$35,000\" \n",
    "            42500:  \"42,500\",                                    # \"$35,001-$50,000\"\n",
    "            65000:  \"65,000\",                                    # \"$50,000-$85,000\"\n",
    "            90000:  \"90,000\",                                    # \"$80,001-$100,000\"\n",
    "            150000: \"more than 100,000\"                          # \"Over $100,000\"\n",
    "        }\n",
    "        inc_val = response[\"Review_ContinuousIncome\"].unique()[0]\n",
    "        inc_des = \"You earn an annual income of {} US dollars.\".format(inc_descriptions[inc_val])\n",
    "\n",
    "\n",
    "        # \"How old are you?\" [text box]\n",
    "        age_val = response[\"Review_age\"].unique()[0]\n",
    "        age_des = \"You are {} years old.\".format(age_val)\n",
    "\n",
    "        # \"Highest level of education\" [dropdown menu]\n",
    "        edu_descriptions = {\n",
    "            'underHigh': \"less than a high school diploma\",      # \"Less than a High School Diploma\"\n",
    "            'high': \"a high school diploma\",                     # \"High School Diploma\"\n",
    "            'vocational': \"vocational training\",                 # \"Vocational training\"\n",
    "            'college': \"that you attended college\",              # \"Attended College\"\n",
    "            'bachelor': \"a bachelor degree\",                     # \"Bachelor Degree\"\n",
    "            'graduate': \"graduate degree\"                        # \"Graduate Degree\"\n",
    "        }\n",
    "        edu_val = response[\"Review_education\"].unique()[0]\n",
    "        edu_des = \"Your highest level of education is {}.\".format(edu_descriptions[edu_val])\n",
    "\n",
    "\n",
    "        # \"What is your gender?\" [dropdown menu]\n",
    "        gen_descriptions = {\n",
    "            'Man':   \"You are a man.\",                           # \"Male\"\n",
    "            'Woman': \"You are a woman.\",                         # \"Female\"\n",
    "            \"Other\": \"You do not identify as a woman or a man.\"  # \"Other\"\n",
    "        }\n",
    "        gen_val = response[\"Review_gender\"].unique()[0]\n",
    "        gen_des = gen_descriptions[gen_val]\n",
    "\n",
    "        # \"What are your religious views?\" [slider scale]\n",
    "        rel_val = response[\"Review_religious\"].unique()[0]\n",
    "        rel_des = \"On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of {} for your religious views.\".format(rel_val)\n",
    "\n",
    "        # \"What are your political views?\" [slider scale]\n",
    "        pol_val = response[\"Review_political\"].unique()[0]\n",
    "        pol_des = \"On a scale from 0 (Conservative) to 100 (Progressive), you report a score of {} for your political views.\".format(pol_val)\n",
    "\n",
    "        # reshuffle order of persona characterists\n",
    "        persona_characteristics = [inc_des, age_des, edu_des, gen_des, pol_des, rel_des]\n",
    "        random.shuffle(persona_characteristics)\n",
    "    \n",
    "        persona = '\\n'.join([f' - {c}' for c in persona_characteristics])\n",
    "    \n",
    "    # set seed for order of characters in description of scenarios\n",
    "    random.seed(a=2024)\n",
    "\n",
    "    # column names in survey data\n",
    "    cols = ['Man','Woman','Pregnant','Stroller','OldMan','OldWoman','Boy','Girl','Homeless','LargeWoman','LargeMan','Criminal',\n",
    "            'MaleExecutive','FemaleExecutive','FemaleAthlete','MaleAthlete','FemaleDoctor','MaleDoctor','Dog','Cat']\n",
    "    \n",
    "    # corresponding words used in the descriptions\n",
    "    character = [\"man\", \"woman\", \"pregnant woman\", \"baby\", \"elderly man\", \"elderly woman\", \"boy\", \"girl\", \"homeless person\", \"large woman\", \"large man\",\n",
    "                 \"criminal\", \"male executive\", \"female executive\", \"female athlete\", \"male athlete\", \"female doctor\", \"male doctor\", \"dog\",\"cat\"]\n",
    "\n",
    "    # plural conversion\n",
    "    characters_plural = [\"men\", \"women\", \"pregnant women\", \"babies\", \"elderly men\", \"elderly women\", \"boys\", \"girls\", \"homeless persons\", \"large women\", \"large men\", \n",
    "                         \"criminals\", \"male executives\", \"female executives\", \"female athletes\", \"male athletes\", \"female doctors\", \"male doctors\", \"dogs\", \"cats\"]\n",
    "    def plural(elem, cnt):\n",
    "        if cnt > 1:\n",
    "            return characters_plural[character.index(elem)]\n",
    "        else:\n",
    "            return elem\n",
    "        \n",
    "    # baseline description\n",
    "    base_description = \"A self-driving car with sudden brake failure will \"\n",
    "\n",
    "    # shortcuts for text\n",
    "    traffic_light_pattern_descriptions = {\n",
    "        'red': \"flouting the law by crossing on the red signal\",\n",
    "        'green': \"abiding by the law by crossing on the green signal\" \n",
    "    }\n",
    "\n",
    "    # whether scenarios involve a car and a pedestrian\n",
    "    is_pasvsped = all(response[\"PedPed\"] == 0)\n",
    "\n",
    "    # passengers vs pedestrians\n",
    "    if is_pasvsped:\n",
    "\n",
    "        pas_description = \"\"\n",
    "        ped_description = \"\"\n",
    "        \n",
    "        # passengers died if Barrier=1\n",
    "        pas = response[ response[\"Barrier\"]==1 ]\n",
    "        # long format to get characters and frequency\n",
    "        pas_T = pas[cols].T.reset_index()\n",
    "        pas_ind = pas_T[pas_T.iloc[:,1] >= 1 ].index\n",
    "        # get list of unique characters in scenario\n",
    "        pas_char = [character[i] for i in pas_ind]\n",
    "        # get number of times each one of them appears\n",
    "        pas_numchar = list(pas_T[pas_T.iloc[:,1] >= 1 ].iloc[:,1])\n",
    "        # repeat characters if they occur multiple times\n",
    "        passengers  = [elem for count, elem in zip(pas_numchar, pas_char) for _ in repeat(None, count)]\n",
    "        # reshuffle order\n",
    "        random.shuffle(passengers)\n",
    "\n",
    "        # pedestrians died if Barrier=0\n",
    "        ped = response[ response[\"Barrier\"]==0 ]\n",
    "        # long format to get characters and frequency\n",
    "        ped_T = ped[cols].T.reset_index()\n",
    "        ped_ind = ped_T[ped_T.iloc[:,1] >= 1 ].index\n",
    "        # get list of unique characters in scenario\n",
    "        ped_char = [character[i] for i in ped_ind]\n",
    "        # get number of times each one of them appears\n",
    "        ped_numchar = list(ped_T[ped_T.iloc[:,1] >= 1 ].iloc[:,1])\n",
    "        # repeat characters if they occur multiple times\n",
    "        pedestrians = [elem for count, elem in zip(ped_numchar, ped_char) for _ in repeat(None, count)]\n",
    "        # reshuffle order\n",
    "        random.shuffle(pedestrians)\n",
    "\n",
    "        pas_dict = Counter(passengers)\n",
    "        ped_dict = Counter(pedestrians)\n",
    "\n",
    "        passengers_set = \"\"\n",
    "        nb_passengers_set = 0\n",
    "        for i, (element, count) in enumerate(pas_dict.items()):\n",
    "            nb_passengers_set += count\n",
    "            passengers_set += \"{} {}\".format(count, plural(element, count))\n",
    "            if i == len(pas_dict) - 2:\n",
    "                if len(pas_dict) == 2:\n",
    "                    passengers_set += \" and \"\n",
    "                else:\n",
    "                    passengers_set += \", and \"\n",
    "            elif i < len(pas_dict) - 1:\n",
    "                passengers_set += \", \"\n",
    "\n",
    "        pedestrians_set = \"\"\n",
    "        nb_pedestrians_set = 0\n",
    "        for i, (element, count) in enumerate(ped_dict.items()):\n",
    "            nb_pedestrians_set += count\n",
    "            pedestrians_set += \"{} {}\".format(count, plural(element, count))\n",
    "            if i == len(ped_dict) - 2:\n",
    "                if len(ped_dict) == 2:\n",
    "                    pedestrians_set += \" and \"\n",
    "                else:\n",
    "                    pedestrians_set += \", and \"\n",
    "            elif i < len(ped_dict) - 1:\n",
    "                pedestrians_set += \", \"\n",
    "\n",
    "        # code legality for pedestrians\n",
    "        if ped.iloc[0][\"CrossingSignal\"] == 0: \n",
    "            ped_signal = \"NA\" \n",
    "        elif ped.iloc[0][\"CrossingSignal\"] == 1:\n",
    "            ped_signal = \"green\"\n",
    "        elif ped.iloc[0][\"CrossingSignal\"] == 2:\n",
    "            ped_signal  = \"red\"\n",
    "        \n",
    "        # passengers died as the result of swerving\n",
    "        if pas.iloc[0][\"Intervention\"] == 1:\n",
    "            pas_description = pas_description + base_description + \"swerve and crash into a concrete barrier. This will result in the death of {}, who {} inside the car.\".format(\n",
    "                passengers_set,\n",
    "                'were' if nb_passengers_set >= 2 else 'was',\n",
    "            )\n",
    "            if ped_signal != \"NA\":\n",
    "                ped_description = ped_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} {}.\".format(\n",
    "                    pedestrians_set,\n",
    "                    'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped_signal],\n",
    "                )\n",
    "            else:\n",
    "                ped_description = ped_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} crossing.\".format(\n",
    "                    pedestrians_set,\n",
    "                    'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                )\n",
    "        # passengers died from continuing ahead and crashing into a barrier\n",
    "        else:\n",
    "            pas_description = pas_description + base_description + \"continue ahead and crash into a concrete barrier. This will result in the death of {}, who {} inside the car.\".format(\n",
    "                passengers_set,\n",
    "                'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "            )\n",
    "            if ped_signal != \"NA\":\n",
    "                ped_description = ped_description + base_description + \"swerve to avoid crashing into a concrete barrier and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} {}.\".format(\n",
    "                    pedestrians_set,\n",
    "                    'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped_signal],\n",
    "                )\n",
    "            else:\n",
    "                ped_description = ped_description + base_description + \"swerve to avoid crashing into a concrete barrier and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} crossing.\".format(\n",
    "                    pedestrians_set,\n",
    "                    'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                )\n",
    "\n",
    "        pas_description = pas_description + \" Meanwhile, it will spare {}\".format(pedestrians_set)\n",
    "        if ped_signal != \"NA\":\n",
    "            pas_description = pas_description + \", who {} {}.\\n\".format(\n",
    "                'were' if nb_pedestrians_set >= 2 else 'was',\n",
    "                traffic_light_pattern_descriptions[ped_signal],\n",
    "            )\n",
    "        else:\n",
    "            pas_description = pas_description + \", who {} crossing.\\n\".format('were' if nb_pedestrians_set >= 2 else 'was')\n",
    "\n",
    "        ped_description = ped_description + \" Meanwhile, it will spare {}\".format(passengers_set)\n",
    "        ped_description = ped_description + \", who {} inside the car.\\n\".format('were' if nb_passengers_set >= 2 else 'was')\n",
    "\n",
    "        # order descriptions of scenarios as presented to respondent \n",
    "        if pas.iloc[0][\"LeftHand\"] == 1: \n",
    "            c1_description = \"Case 1.\\n\" + pas_description\n",
    "            c2_description = \"Case 2.\\n\" + ped_description\n",
    "        else:\n",
    "            c1_description = \"Case 1.\\n\" + ped_description\n",
    "            c2_description = \"Case 2.\\n\" + pas_description\n",
    "\n",
    "    \n",
    "    # pedestrians vs pedestrians\n",
    "    else:\n",
    "            \n",
    "        ped1_description = \"\"\n",
    "        ped2_description = \"\"\n",
    "        \n",
    "        # pedestrians 1 mentioned first if LeftHand=1\n",
    "        ped1 = response[ response[\"LeftHand\"]==1 ]\n",
    "        # long format to get characters and frequency\n",
    "        ped1_T = ped1[cols].T.reset_index()\n",
    "        ped1_ind = ped1_T[ped1_T.iloc[:,1] >= 1 ].index\n",
    "        # get list of unique characters in scenario\n",
    "        ped1_char = [character[i] for i in ped1_ind]\n",
    "        # get number of times each one of them appears\n",
    "        ped1_numchar = list(ped1_T[ped1_T.iloc[:,1] >= 1 ].iloc[:,1])\n",
    "        # repeat characters if they occur multiple times\n",
    "        pedestrians1 = [elem for count, elem in zip(ped1_numchar, ped1_char) for _ in repeat(None, count)]\n",
    "        # reshuffle order\n",
    "        random.shuffle(pedestrians1)\n",
    "\n",
    "        # pedestrians died if Barrier=0\n",
    "        ped2 = response[ response[\"LeftHand\"]==0 ]\n",
    "        # long format to get characters and frequency\n",
    "        ped2_T = ped2[cols].T.reset_index()\n",
    "        ped2_ind = ped2_T[ped2_T.iloc[:,1] >= 1 ].index\n",
    "        # get list of unique characters in scenario\n",
    "        ped2_char = [character[i] for i in ped2_ind]\n",
    "        # get number of times each one of them appears\n",
    "        ped2_numchar = list(ped2_T[ped2_T.iloc[:,1] >= 1 ].iloc[:,1])\n",
    "        # repeat characters if they occur multiple times\n",
    "        pedestrians2 = [elem for count, elem in zip(ped2_numchar, ped2_char) for _ in repeat(None, count)]\n",
    "        # reshuffle order\n",
    "        random.shuffle(pedestrians2)\n",
    "\n",
    "        ped1_dict = Counter(pedestrians1)\n",
    "        ped2_dict = Counter(pedestrians2)\n",
    "\n",
    "        pedestrians1_set = \"\"\n",
    "        nb_pedestrians1_set = 0\n",
    "        for i, (element, count) in enumerate(ped1_dict.items()):\n",
    "            nb_pedestrians1_set += count\n",
    "            pedestrians1_set += \"{} {}\".format(count, plural(element, count))\n",
    "            if i == len(ped1_dict) - 2:\n",
    "                if len(ped1_dict) == 2:\n",
    "                    pedestrians1_set += \" and \"\n",
    "                else:\n",
    "                    pedestrians1_set += \", and \"\n",
    "            elif i < len(ped1_dict) - 1:\n",
    "                pedestrians1_set += \", \"\n",
    "\n",
    "        pedestrians2_set = \"\"\n",
    "        nb_pedestrians2_set = 0\n",
    "        for i, (element, count) in enumerate(ped2_dict.items()):\n",
    "            nb_pedestrians2_set += count\n",
    "            pedestrians2_set += \"{} {}\".format(count, plural(element, count))\n",
    "            if i == len(ped2_dict) - 2:\n",
    "                if len(ped2_dict) == 2:\n",
    "                    pedestrians2_set += \" and \"\n",
    "                else:\n",
    "                    pedestrians2_set += \", and \"\n",
    "            elif i < len(ped2_dict) - 1:\n",
    "                pedestrians2_set += \", \"\n",
    "\n",
    "        # code legality for pedestrians 1\n",
    "        if ped1.iloc[0][\"CrossingSignal\"] == 0: \n",
    "            ped1_signal = \"NA\" \n",
    "        elif ped1.iloc[0][\"CrossingSignal\"] == 1:\n",
    "            ped1_signal = \"green\"\n",
    "        elif ped1.iloc[0][\"CrossingSignal\"] == 2:\n",
    "            ped1_signal  = \"red\"\n",
    "\n",
    "        # code legality for pedestrians 2 \n",
    "        if ped2.iloc[0][\"CrossingSignal\"] == 0: \n",
    "            ped2_signal = \"NA\" \n",
    "        elif ped2.iloc[0][\"CrossingSignal\"] == 1:\n",
    "            ped2_signal = \"green\"\n",
    "        elif ped2.iloc[0][\"CrossingSignal\"] == 2:\n",
    "            ped2_signal  = \"red\"\n",
    "        \n",
    "        # pedestrians 1 died because respondent let the AV swerve\n",
    "        if ped1.iloc[0][\"Intervention\"] == 1:\n",
    "            if ped1_signal != \"NA\":\n",
    "                ped1_description = ped1_description + base_description + \"swerve and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} {} in the other lane.\".format(\n",
    "                    pedestrians1_set,\n",
    "                    'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped1_signal],\n",
    "                )\n",
    "                ped2_description = ped2_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} {} ahead of the car.\".format(\n",
    "                    pedestrians2_set,\n",
    "                    'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped2_signal],\n",
    "                )\n",
    "            else:\n",
    "                ped1_description = ped1_description + base_description + \"swerve and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} crossing in the other lane.\".format(\n",
    "                    pedestrians1_set,\n",
    "                    'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                )\n",
    "                ped2_description = ped2_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} crossing ahead of the car.\".format(\n",
    "                    pedestrians2_set,\n",
    "                    'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                )\n",
    "        # pedestrians 2 died because respondet let the AV stay on course\n",
    "        else:\n",
    "            if ped1_signal != \"NA\":\n",
    "                ped1_description = ped1_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} {} ahead of the car.\".format(\n",
    "                    pedestrians1_set,\n",
    "                    'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped1_signal],\n",
    "                )\n",
    "                ped2_description = ped2_description + base_description + \"swerve and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} {} in the other lane.\".format(\n",
    "                    pedestrians2_set,\n",
    "                    'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                    traffic_light_pattern_descriptions[ped2_signal],\n",
    "                )\n",
    "            else:\n",
    "                ped1_description = ped1_description + base_description + \"continue ahead and drive through a pedestrian crossing ahead. This will result in the death of {}, who {} crossing ahead of the car.\".format(\n",
    "                    pedestrians1_set,\n",
    "                    'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                )\n",
    "                ped2_description = ped2_description + base_description + \"swerve and drive through a pedestrian crossing in the other lane. This will result in the death of {}, who {} crossing in the other lane.\".format(\n",
    "                    pedestrians2_set,\n",
    "                    'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                )\n",
    "\n",
    "        ped1_description = ped1_description + \" Meanwhile, it will spare {}\".format(pedestrians2_set)\n",
    "        if ped2_signal != \"NA\":\n",
    "            ped1_description = ped1_description + \", who {} {} \".format(\n",
    "                'were' if nb_pedestrians2_set >= 2 else 'was',\n",
    "                traffic_light_pattern_descriptions[ped2_signal],\n",
    "            )\n",
    "        else:\n",
    "            ped1_description = ped1_description + \", who {} crossing \".format('were' if nb_pedestrians2_set >= 2 else 'was')\n",
    "\n",
    "        if ped1.iloc[0][\"Intervention\"] == 1:\n",
    "            ped1_description = ped1_description + \"ahead of the car.\\n\"\n",
    "        else:\n",
    "            ped1_description = ped1_description + \"in the other lane.\\n\"\n",
    "        \n",
    "        ped2_description = ped2_description + \" Meanwhile, it will spare {}\".format(pedestrians1_set)\n",
    "        if ped1_signal != \"NA\":\n",
    "            ped2_description = ped2_description + \", who {} {} \".format(\n",
    "                'were' if nb_pedestrians1_set >= 2 else 'was',\n",
    "                traffic_light_pattern_descriptions[ped1_signal],\n",
    "            )\n",
    "        else:\n",
    "            ped2_description = ped2_description + \", who {} crossing \".format('were' if nb_pedestrians1_set >= 2 else 'was')\n",
    "\n",
    "        if ped1.iloc[0][\"Intervention\"] == 1:\n",
    "            ped2_description = ped2_description + \"in the other lane.\\n\"\n",
    "        else:\n",
    "            ped2_description = ped2_description + \"ahead of the car.\\n\"\n",
    "\n",
    "        c1_description = \"Case 1.\\n\" + ped1_description\n",
    "        c2_description = \"Case 2.\\n\" + ped2_description\n",
    "\n",
    "    # system content base description\n",
    "    system_content = \"Carefully consider the following information and think step by step. \"\n",
    "\n",
    "    # add information about survey respondent\n",
    "    if include_persona:\n",
    "        system_content += \"You are an American with the following characteristics:\\n\" + persona + \"\\n\"\n",
    "    else: \n",
    "        system_content += \"\\n\"\n",
    "\n",
    "    # add information about the structure of the scenario      \n",
    "    if is_pasvsped:\n",
    "        system_content += \"Consider that pedestrians are crossing a zebra crossing that spans two lanes. \"\n",
    "    else:\n",
    "        system_content += \"Consider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. \"\n",
    "    \n",
    "    # add task description and requested output format\n",
    "    system_content += \"Please indicate which case is better for autonomous driving by choosing either \\'Case 1\\' or \\'Case 2\\'. Respond with one word only.\\n\"\n",
    "    \n",
    "    # user content contains description of scenario\n",
    "    user_content = c1_description + \"\\n\" + c2_description\n",
    "\n",
    "    return system_content, user_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_response(row, column_name):\n",
    "\n",
    "    # row refers to one row in the dataframe with llm reponses\n",
    "    # column_name refers to the column with llm text output\n",
    "    text = row[ column_name ].lower()\n",
    "\n",
    "    # define regular expression for whether llm opted for 1 or 2\n",
    "    pattern_case1 = re.compile(r\"case\\s?1.?\")\n",
    "    pattern_case2 = re.compile(r\"case\\s?2.?\")\n",
    "\n",
    "    # whether text matches case1 and case2 pattern, respectively\n",
    "    match_case1 = pattern_case1.search(text) is not None\n",
    "    match_case2 = pattern_case2.search(text) is not None\n",
    "\n",
    "    if match_case1 and not match_case2:\n",
    "        case = 1\n",
    "    elif not match_case1 and match_case2:\n",
    "        case = 2\n",
    "    else:\n",
    "        case = np.NaN\n",
    "\n",
    "    # row refers to case 1 and llm chose case 1, so people described in case 1 are not saved\n",
    "    if  row[\"LeftHand\"]==1 and case==1: \n",
    "        saved = 0\n",
    "    # row refers to case 1 but llm chose case 2, so people described in case 2 are saved\n",
    "    elif row[\"LeftHand\"]==1 and case==2:\n",
    "        saved = 1\n",
    "    # row refers to case 2 and llm chose case 2, so people described in case 2 are not saved\n",
    "    elif row[\"LeftHand\"]==0 and case==2:\n",
    "        saved = 0\n",
    "    # row refers to case 2 but llm chose case 1, so people described in case 2 are saved\n",
    "    elif row[\"LeftHand\"]==0 and case==1:\n",
    "        saved = 1\n",
    "    else: \n",
    "        saved = np.NaN\n",
    "    \n",
    "\n",
    "    return saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_llm(data, model, api_key, csv_path, include_persona=True, verbose=False, sleep=0, temperature=None):\n",
    "\n",
    "    # data:            DataFrame with scenario descriptions and profiles of survey respondents\n",
    "    # model:           Name of model, e.g. gpt-4o\n",
    "    # api_key:         API key \n",
    "    # csv_path:        File path to existing .csv for saving output of API calls, creates .csv if not exists\n",
    "    # include_persona: If true, the prompt includes a demographic description of the survey respondent\n",
    "    # verbose:         If true, the LLM response is printed\n",
    "    # sleep:           Time in seconds between API calls\n",
    "\n",
    "    # prompt \n",
    "    if os.path.exists(csv_path): \n",
    "\n",
    "        # get existing reponses\n",
    "        existing = pd.read_csv(csv_path, usecols = [\"ResponseID\"])\n",
    "        print(\"Existing responses:\", existing[\"ResponseID\"].unique().shape[0])\n",
    "\n",
    "        # define column indicating in which dataframe ResponseID is present \n",
    "        toprompt = pd.merge(data, existing, indicator=True, on=\"ResponseID\", how=\"left\")\n",
    "\n",
    "        # keep rows that haven't been used for prompting\n",
    "        ids_toprompt = toprompt.loc[toprompt['_merge'] == 'left_only', 'ResponseID'].unique()\n",
    "        random.shuffle(ids_toprompt)\n",
    "\n",
    "        print(\"Number of remaining prompts:\", len(ids_toprompt))\n",
    "\n",
    "    else:\n",
    "        ids_toprompt = data[\"ResponseID\"].unique()\n",
    "        random.shuffle(ids_toprompt)\n",
    "\n",
    "\n",
    "    if len(ids_toprompt) > 0: \n",
    "        \n",
    "        i = 1\n",
    "        for id in ids_toprompt: \n",
    "\n",
    "            # track progress\n",
    "            if i==1 or i % 50 == 0: \n",
    "                print(f\"Prompt {i} out of {len(ids_toprompt)}\")\n",
    "            i = i+1\n",
    "            \n",
    "            survey_response = data[ data[\"ResponseID\"]== id ]\n",
    "\n",
    "            prompt = generate_scenario(survey_response, include_persona=include_persona)\n",
    "\n",
    "            \n",
    "            # check the model and assign the appropriate client\n",
    "            if model in [\"gpt-4o\",\"gpt-4-turbo\",\"o1-mini\",\"o1-preview\"]:\n",
    "                \n",
    "                client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": prompt[0]},\n",
    "                        {\"role\": \"user\",   \"content\": prompt[1]}\n",
    "                ]\n",
    "\n",
    "                if model in [\"o1-mini\", \"o1-preview\"]:\n",
    "                    messages = [{\"role\": \"user\",   \"content\": prompt[0] + \" \" + prompt[1]}]\n",
    "\n",
    "                if temperature is None:\n",
    "                    reply = client.chat.completions.create(\n",
    "                        model=model, \n",
    "                        messages=messages\n",
    "                    ) \n",
    "                elif temperature is not None:\n",
    "                    reply = client.chat.completions.create(\n",
    "                        model=model, \n",
    "                        messages=messages,\n",
    "                        temperature=temperature\n",
    "                    ) \n",
    "                \n",
    "                llm_response = reply.choices[0].message.content\n",
    "\n",
    "\n",
    "            elif model == \"claude-3-5-sonnet-20241022\":\n",
    "\n",
    "                client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "                reply = client.messages.create(\n",
    "                    model=model, \n",
    "                    max_tokens=4,\n",
    "                    system=prompt[0],\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\",   \"content\": prompt[1]}\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                llm_response = reply.content[0].text\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model: {model}\")\n",
    "            \n",
    "            # print LLM response if verbose\n",
    "            if verbose: print(llm_response)\n",
    "\n",
    "            # loop with a 1-second pause between iterations\n",
    "            time.sleep(sleep)\n",
    "\n",
    "            # prefix based on llm name and whether prompt contained the persona\n",
    "            column_prefix = re.sub('[-._ ]', '', model) + (\"_wp_\" if include_persona else \"_np_\")\n",
    "\n",
    "            # Create a dictionary for the new row\n",
    "            new_values = {\n",
    "                column_prefix+'Timestamp': datetime.now().isoformat(),\n",
    "                column_prefix+'SystemPrompt': prompt[0],\n",
    "                column_prefix+'UserPrompt': prompt[1],\n",
    "                column_prefix+'Persona': int(include_persona),\n",
    "                column_prefix+'Label': llm_response}        \n",
    "\n",
    "            survey_response = survey_response.assign(**new_values)\n",
    "\n",
    "            survey_response[column_prefix+\"Saved\"] = survey_response.apply(classify_response, column_name=column_prefix+\"Label\", axis=1)\n",
    "\n",
    "            if os.path.isfile(csv_path): \n",
    "                survey_response.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "            else: \n",
    "                survey_response.to_csv(csv_path, index = False)\n",
    "\n",
    "    else:\n",
    "        print(\"No remaining responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Dilemmas\n",
    "\n",
    "Below we illustrate how the entries in the data matrix describe a scenario. We created the images with the design functionalities of [moralmachine.net](https://www.moralmachine.net/). These images illustrate the examples but play no further role in the study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "On the left side (`LeftHand=1`), respondents saw an AV that swerves to the other lane (`Intervention=1`) and kills 5 pedestrians (`Barrier=0`) – 1 baby, 1 female athlete, 1 male athlete, 1 female doctor, and 1 cat – who were crossing on a green light (`CrossingSignal=1`).\n",
    "\n",
    "On the right side (`LeftHand=0`), respondents saw an AV that would continue ahead and crash into a barrier, resulting in the dealth of the 4 passengers (`Barrier=1`) - 1 baby, 1 female athlete, 1 female doctor, and a cat.\n",
    "\n",
    "This example is taken from the [supplementary material](https://osf.io/wt6mc?view_only=4bb49492edee4a8eb1758552a362a2cf) in Awad et al. (2018). \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../Figures/4_ScenarioExamples/2224g4ytARX4QT5rB.png\" alt=\"Example 1 2224g4ytARX4QT5rB (SI, Awad et al. 2018)\" width=\"70%\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With persona:\n",
      " (\"Carefully consider the following information and think step by step. You are an American with the following characteristics:\\n - On a scale from 0 (Conservative) to 100 (Progressive), you report a score of 54 for your political views.\\n - Your highest level of education is a bachelor degree.\\n - You are 36 years old.\\n - You are a man.\\n - On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of 24 for your religious views.\\n - You earn an annual income of 30,000 US dollars.\\nConsider that pedestrians are crossing a zebra crossing that spans two lanes. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will swerve to avoid crashing into a concrete barrier and drive through a pedestrian crossing in the other lane. This will result in the death of 1 baby, 1 cat, 1 male athlete, 1 female doctor, and 1 female athlete, who were abiding by the law by crossing on the green signal. Meanwhile, it will spare 1 female doctor, 1 female athlete, 1 baby, and 1 cat, who were inside the car.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will continue ahead and crash into a concrete barrier. This will result in the death of 1 female doctor, 1 female athlete, 1 baby, and 1 cat, who were inside the car. Meanwhile, it will spare 1 baby, 1 cat, 1 male athlete, 1 female doctor, and 1 female athlete, who were abiding by the law by crossing on the green signal.\\n') \n",
      "\n",
      "No persona:\n",
      " (\"Carefully consider the following information and think step by step. \\nConsider that pedestrians are crossing a zebra crossing that spans two lanes. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will swerve to avoid crashing into a concrete barrier and drive through a pedestrian crossing in the other lane. This will result in the death of 1 baby, 1 cat, 1 male athlete, 1 female doctor, and 1 female athlete, who were abiding by the law by crossing on the green signal. Meanwhile, it will spare 1 female doctor, 1 female athlete, 1 baby, and 1 cat, who were inside the car.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will continue ahead and crash into a concrete barrier. This will result in the death of 1 female doctor, 1 female athlete, 1 baby, and 1 cat, who were inside the car. Meanwhile, it will spare 1 baby, 1 cat, 1 male athlete, 1 female doctor, and 1 female athlete, who were abiding by the law by crossing on the green signal.\\n') \n",
      "\n",
      "Label assigned by LLM:  ['Case 2']\n",
      "Outcomes for these scenarios:\n",
      " 0    1\n",
      "1    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data1 = {\n",
    "    \"ResponseID\": [\"2224g4ytARX4QT5rB\", \"2224g4ytARX4QT5rB\"],\n",
    "    \"ExtendedSessionID\": [\"213978760_9992828917431898.0\", \"213978760_9992828917431898.0\"],\n",
    "    \"UserID\": [9.992829e+15, 9.992829e+15],\n",
    "    # Imputed demographics just for this illustration\n",
    "    \"Review_age\": [36,36],                     \n",
    "    \"Review_education\": [\"bachelor\",\"bachelor\"],\n",
    "    \"Review_gender\": [\"Man\",\"Man\"],\n",
    "    \"Review_ContinuousIncome\": [30000,30000],\n",
    "    \"Review_political\": [54,54],\n",
    "    \"Review_religious\": [24,24],\n",
    "    \"ScenarioOrder\": [7, 7],\n",
    "    \"Intervention\": [1, 0],\n",
    "    \"PedPed\": [0, 0],\n",
    "    \"Barrier\": [0, 1],\n",
    "    \"CrossingSignal\": [1, 0],\n",
    "    \"AttributeLevel\": [\"More\", \"Less\"],\n",
    "    \"ScenarioTypeStrict\": [\"Utilitarian\", \"Utilitarian\"],\n",
    "    \"ScenarioType\": [\"Utilitarian\", \"Utilitarian\"],\n",
    "    \"DefaultChoice\": [\"More\", \"More\"],\n",
    "    \"NonDefaultChoice\": [\"Less\", \"Less\"],\n",
    "    \"DefaultChoiceIsOmission\": [0, 0],\n",
    "    \"NumberOfCharacters\": [5, 4],\n",
    "    \"DiffNumberOFCharacters\": [1, 1],\n",
    "    \"Saved\": [0, 1],\n",
    "    'Label': ['Case 2','Case 2'],\n",
    "    \"Template\": [\"Desktop\", \"Desktop\"],\n",
    "    \"DescriptionShown\": [1, 1],\n",
    "    \"LeftHand\": [1, 0],\n",
    "    \"UserCountry3\": [\"USA\", \"USA\"],\n",
    "    \"Man\": [0, 0],\n",
    "    \"Woman\": [0, 0],\n",
    "    \"Pregnant\": [0, 0],\n",
    "    \"Stroller\": [1, 1],\n",
    "    \"OldMan\": [0, 0],\n",
    "    \"OldWoman\": [0, 0],\n",
    "    \"Boy\": [0, 0],\n",
    "    \"Girl\": [0, 0],\n",
    "    \"Homeless\": [0, 0],\n",
    "    \"LargeWoman\": [0, 0],\n",
    "    \"LargeMan\": [0, 0],\n",
    "    \"Criminal\": [0, 0],\n",
    "    \"MaleExecutive\": [0, 0],\n",
    "    \"FemaleExecutive\": [0, 0],\n",
    "    \"FemaleAthlete\": [1, 1],\n",
    "    \"MaleAthlete\": [1, 0],\n",
    "    \"FemaleDoctor\": [1, 1],\n",
    "    \"MaleDoctor\": [0, 0],\n",
    "    \"Dog\": [0, 0],\n",
    "    \"Cat\": [1, 1]\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    " \n",
    "\n",
    "print(\"With persona:\\n\",generate_scenario(df1, include_persona=True),\"\\n\")\n",
    "print(\"No persona:\\n\", generate_scenario(df1, include_persona=False),\"\\n\")\n",
    "print(\"Label assigned by LLM: \",df1[\"Label\"].unique())\n",
    "print(\"Outcomes for these scenarios:\\n\",df1.apply(classify_response, column_name = \"Label\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "These scenarios pit two groups of pedestrians against each other (`PedPed=1`).  \n",
    "\n",
    "On the left side of the screen (`LeftHand=1`), respondents saw a scenario in which the AV stays on course (`Intervention=0`), resulting in the death of 1 man who was crossing on a red signal (`CrossingSignal=2`). \n",
    "\n",
    "On the right side of the screen (`LeftHand=0`), respondents saw a scenario in which the AV swerves to the other lane (`Intervention=1`), resulting in the death 1 male athlete who was crossing on a green signal (`CrossingSignal=1`).\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../Figures/4_ScenarioExamples/22qKv8AmPcXEnNd8z.png\" width=\"70%\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With persona:\n",
      " (\"Carefully consider the following information and think step by step. You are an American with the following characteristics:\\n - You earn an annual income of 12,500 US dollars.\\n - Your highest level of education is a high school diploma.\\n - On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of 0 for your religious views.\\n - You are a man.\\n - You are 29 years old.\\n - On a scale from 0 (Conservative) to 100 (Progressive), you report a score of 100 for your political views.\\nConsider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 man, who was flouting the law by crossing on the red signal ahead of the car. Meanwhile, it will spare 1 male athlete, who was abiding by the law by crossing on the green signal in the other lane.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and drive through a pedestrian crossing in the other lane. This will result in the death of 1 male athlete, who was abiding by the law by crossing on the green signal in the other lane. Meanwhile, it will spare 1 man, who was flouting the law by crossing on the red signal ahead of the car.\\n') \n",
      "\n",
      "No persona:\n",
      " (\"Carefully consider the following information and think step by step. \\nConsider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 man, who was flouting the law by crossing on the red signal ahead of the car. Meanwhile, it will spare 1 male athlete, who was abiding by the law by crossing on the green signal in the other lane.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and drive through a pedestrian crossing in the other lane. This will result in the death of 1 male athlete, who was abiding by the law by crossing on the green signal in the other lane. Meanwhile, it will spare 1 man, who was flouting the law by crossing on the red signal ahead of the car.\\n') \n",
      "\n",
      "Label assigned by LLM:  ['Case 1']\n",
      "Outcomes for these scenarios:\n",
      " 0    0\n",
      "1    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data2 = {\n",
    "    \"ExtendedSessionID\": [\"1055565952_8316216477776195.0\", \"1055565952_8316216477776195.0\"],\n",
    "    \"ResponseID\": [\"22qKv8AmPcXEnNd8z\", \"22qKv8AmPcXEnNd8z\"],\n",
    "    \"UserID\": [8.316216e+15, 8.316216e+15],\n",
    "    \"Review_age\": [29, 29],\n",
    "    \"Review_education\": [\"high\",\"high\"],\n",
    "    \"Review_income\": [\"10000\", \"10000\"],\n",
    "    \"Review_gender\": [\"Man\", \"Man\"],\n",
    "    \"Review_ContinuousIncome\": [12500,12500],\n",
    "    \"IncomeBracketSmall\": [\"$5,001-\\n$25,000\", \"$5,001-\\n$25,000\"],\n",
    "    \"Review_political\": [100, 100],\n",
    "    \"Review_religious\": [0, 0],\n",
    "    \"ScenarioOrder\": [6, 6],\n",
    "    \"Intervention\": [0, 1],\n",
    "    \"PedPed\": [1, 1],\n",
    "    \"Barrier\": [0, 0],\n",
    "    \"CrossingSignal\": [2, 1],\n",
    "    \"AttributeLevel\": [\"Fat\", \"Fit\"],\n",
    "    \"ScenarioTypeStrict\": [\"Fitness\", \"Fitness\"],\n",
    "    \"ScenarioType\": [\"Fitness\", \"Fitness\"],\n",
    "    \"DefaultChoice\": [\"Fit\", \"Fit\"],\n",
    "    \"NonDefaultChoice\": [\"Fat\", \"Fat\"],\n",
    "    \"DefaultChoiceIsOmission\": [0, 0],\n",
    "    \"NumberOfCharacters\": [1, 1],\n",
    "    \"DiffNumberOFCharacters\": [0, 0],\n",
    "    \"Saved\": [0, 1],\n",
    "    'Label': ['Case 1','Case 1'],\n",
    "    \"Template\": [\"Desktop\", \"Desktop\"],\n",
    "    \"DescriptionShown\": [1, 1],\n",
    "    \"LeftHand\": [1, 0],\n",
    "    \"UserCountry3\": [\"USA\", \"USA\"],\n",
    "    \"Man\": [1, 0],\n",
    "    \"Woman\": [0, 0],\n",
    "    \"Pregnant\": [0, 0],\n",
    "    \"Stroller\": [0, 0],\n",
    "    \"OldMan\": [0, 0],\n",
    "    \"OldWoman\": [0, 0],\n",
    "    \"Boy\": [0, 0],\n",
    "    \"Girl\": [0, 0],\n",
    "    \"Homeless\": [0, 0],\n",
    "    \"LargeWoman\": [0, 0],\n",
    "    \"LargeMan\": [0, 0],\n",
    "    \"Criminal\": [0, 0],\n",
    "    \"MaleExecutive\": [0, 0],\n",
    "    \"FemaleExecutive\": [0, 0],\n",
    "    \"FemaleAthlete\": [0, 0],\n",
    "    \"MaleAthlete\": [0, 1],\n",
    "    \"FemaleDoctor\": [0, 0],\n",
    "    \"MaleDoctor\": [0, 0],\n",
    "    \"Dog\": [0, 0],\n",
    "    \"Cat\": [0, 0],\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(data2)\n",
    " \n",
    "\n",
    "print(\"With persona:\\n\",generate_scenario(df2, include_persona=True),\"\\n\")\n",
    "print(\"No persona:\\n\", generate_scenario(df2, include_persona=False),\"\\n\")\n",
    "print(\"Label assigned by LLM: \",df2[\"Label\"].unique())\n",
    "print(\"Outcomes for these scenarios:\\n\",df2.apply(classify_response, column_name = \"Label\", axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "This scenario pits pedestrians against pedestrians (`PedPed=1`). On the left side of the screen (`LeftHand=1`), respondents saw a scenario in which the AV would stay on course (`Intervention=0`), resulting in the death of 1 male executive who was crossing (`CrossingSignal=0`).\n",
    "\n",
    "On the right side of the screen (`LeftSide=0`), respodents saw a scenario in which the AV would swerve (`Intervention=1`), resulting in the death of a 1 female executive who was crossing (`CrossingSignal=0`).\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../Figures/4_ScenarioExamples/A6GmXsYKGxyivAFzu.png\" width=\"70%\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With persona:\n",
      " (\"Carefully consider the following information and think step by step. You are an American with the following characteristics:\\n - On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of 46 for your religious views.\\n - On a scale from 0 (Conservative) to 100 (Progressive), you report a score of 11 for your political views.\\n - You earn an annual income of 42,500 US dollars.\\n - Your highest level of education is a bachelor degree.\\n - You are 46 years old.\\n - You are a woman.\\nConsider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 male executive, who was crossing ahead of the car. Meanwhile, it will spare 1 female executive, who was crossing in the other lane.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and drive through a pedestrian crossing in the other lane. This will result in the death of 1 female executive, who was crossing in the other lane. Meanwhile, it will spare 1 male executive, who was crossing ahead of the car.\\n') \n",
      "\n",
      "No persona:\n",
      " (\"Carefully consider the following information and think step by step. \\nConsider that pedestrians are crossing a zebra crossing that spans two lanes, divided by an island. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 male executive, who was crossing ahead of the car. Meanwhile, it will spare 1 female executive, who was crossing in the other lane.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and drive through a pedestrian crossing in the other lane. This will result in the death of 1 female executive, who was crossing in the other lane. Meanwhile, it will spare 1 male executive, who was crossing ahead of the car.\\n') \n",
      "\n",
      "Label assigned by LLM:  ['Case 1']\n",
      "Outcomes for these scenarios:\n",
      " 0    0\n",
      "1    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data3 = {\n",
    "    'ExtendedSessionID': ['1694978322_3759038854820315.0', '1694978322_3759038854820315.0'],\n",
    "    'ResponseID': ['A6GmXsYKGxyivAFzu', 'A6GmXsYKGxyivAFzu'],\n",
    "    'UserID': [3.759039e+15, 3.759039e+15],\n",
    "    'Review_age': [46, 46],\n",
    "    'Review_education': ['bachelor','bachelor'],\n",
    "    'Review_gender': ['Woman', 'Woman'],\n",
    "    'Review_income': ['35000', '35000'],\n",
    "    \"Review_ContinuousIncome\": [42500,42500],\n",
    "    'IncomeBracketSmall': ['$25,001-\\n$50,000', '$25,001-\\n$50,000'],\n",
    "    'Review_political': [11, 11],\n",
    "    'Review_religious': [46, 46],\n",
    "    'ScenarioOrder': [1, 1],\n",
    "    'Intervention': [0, 1],\n",
    "    'PedPed': [1, 1],\n",
    "    'Barrier': [0, 0],\n",
    "    'CrossingSignal': [0, 0],\n",
    "    'AttributeLevel': ['Male', 'Female'],\n",
    "    'ScenarioTypeStrict': ['Gender', 'Gender'],\n",
    "    'ScenarioType': ['Gender', 'Gender'],\n",
    "    'DefaultChoice': ['Male', 'Female'],\n",
    "    'NonDefaultChoice': ['Male', 'Female'],\n",
    "    'DefaultChoiceIsOmission': [1, 1],\n",
    "    'NumberOfCharacters': [1, 1],\n",
    "    'DiffNumberOFCharacters': [0, 0],\n",
    "    'Saved': [0, 1],\n",
    "    'Label': ['Case 1','Case 1'],\n",
    "    'Template': ['Desktop', 'Desktop'],\n",
    "    'DescriptionShown': [0, 0],\n",
    "    'LeftHand': [1, 0],\n",
    "    'UserCountry3': ['USA', 'USA'],\n",
    "    'Man': [0, 0],\n",
    "    'Woman': [0, 0],\n",
    "    'Pregnant': [0, 0],\n",
    "    'Stroller': [0, 0],\n",
    "    'OldMan': [0, 0],\n",
    "    'OldWoman': [0, 0],\n",
    "    'Boy': [0, 0],\n",
    "    'Girl': [0, 0],\n",
    "    'Homeless': [0, 0],\n",
    "    'LargeWoman': [0, 0],\n",
    "    'LargeMan': [0, 0],\n",
    "    'Criminal': [0, 0],\n",
    "    'MaleExecutive': [1, 0],\n",
    "    'FemaleExecutive': [0, 1],\n",
    "    'FemaleAthlete': [0, 0],\n",
    "    'MaleAthlete': [0, 0],\n",
    "    'FemaleDoctor': [0, 0],\n",
    "    'MaleDoctor': [0, 0],\n",
    "    'Dog': [0, 0],\n",
    "    'Cat': [0, 0]\n",
    "}\n",
    "\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "print(\"With persona:\\n\",generate_scenario(df3, include_persona=True),\"\\n\")\n",
    "print(\"No persona:\\n\", generate_scenario(df3, include_persona=False),\"\\n\")\n",
    "print(\"Label assigned by LLM: \",df3[\"Label\"].unique())\n",
    "print(\"Outcomes for these scenarios:\\n\",df3.apply(classify_response, column_name = \"Label\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "\n",
    "On the left side of the screen (`LeftHand=1`), respondents saw a scenario in which an AV would stay on course (`Intervention=0`), resulting in the death of 5 pedestrians (`Barrier=0`) – 1 man, 1 woman, 2 boys, and 1 girl – who were crossing on a green light (`CrossingSignal=1`). \n",
    "\n",
    "On the right side of the screen (`LeftHand=0`), respondents saw a scenario in which an AV would swerve onto the other lane (`Intervention=1`), resulting in the death of the 5 passengers (`Barrier=1`) – 1 man, 1 woman, 2 old men, and 1 old woman. \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../Figures/4_ScenarioExamples/EH3SfatQP3hygSpzF.png\" width=\"70%\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With persona:\n",
      " (\"Carefully consider the following information and think step by step. You are an American with the following characteristics:\\n - On a scale from 0 (Not Religious) to 100 (Very Religious), you report a score of 0 for your religious views.\\n - On a scale from 0 (Conservative) to 100 (Progressive), you report a score of 100 for your political views.\\n - You earn an annual income of under 5,000 US dollars.\\n - Your highest level of education is a high school diploma.\\n - You are 35 years old.\\n - You are a man.\\nConsider that pedestrians are crossing a zebra crossing that spans two lanes. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 man, 1 girl, 2 boys, and 1 woman, who were abiding by the law by crossing on the green signal. Meanwhile, it will spare 1 man, 1 elderly woman, 2 elderly men, and 1 woman, who were inside the car.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and crash into a concrete barrier. This will result in the death of 1 man, 1 elderly woman, 2 elderly men, and 1 woman, who were inside the car. Meanwhile, it will spare 1 man, 1 girl, 2 boys, and 1 woman, who were abiding by the law by crossing on the green signal.\\n') \n",
      "\n",
      "No persona:\n",
      " (\"Carefully consider the following information and think step by step. \\nConsider that pedestrians are crossing a zebra crossing that spans two lanes. Please indicate which case is better for autonomous driving by choosing either 'Case 1' or 'Case 2'. Respond with one word only.\\n\", 'Case 1.\\nA self-driving car with sudden brake failure will continue ahead and drive through a pedestrian crossing ahead. This will result in the death of 1 man, 1 girl, 2 boys, and 1 woman, who were abiding by the law by crossing on the green signal. Meanwhile, it will spare 1 man, 1 elderly woman, 2 elderly men, and 1 woman, who were inside the car.\\n\\nCase 2.\\nA self-driving car with sudden brake failure will swerve and crash into a concrete barrier. This will result in the death of 1 man, 1 elderly woman, 2 elderly men, and 1 woman, who were inside the car. Meanwhile, it will spare 1 man, 1 girl, 2 boys, and 1 woman, who were abiding by the law by crossing on the green signal.\\n') \n",
      "\n",
      "Label assigned by LLM:  ['Case 1']\n",
      "Outcomes for these scenarios:\n",
      " 0    0\n",
      "1    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data4 = {\n",
    "    'ExtendedSessionID': ['-2127483756_5144602155778557.0', '-2127483756_5144602155778557.0'],\n",
    "    'ResponseID': ['EH3SfatQP3hygSpzF', 'EH3SfatQP3hygSpzF'],\n",
    "    'UserID': [5.144602e+15, 5.144602e+15],\n",
    "    'Review_gender': ['Man', 'Man'],\n",
    "    'Review_age': [35, 35],\n",
    "    'Review_ageBracket': ['35-44','35-44'],\n",
    "    'Review_income': ['under5000', 'under5000'],\n",
    "    'Review_ContinuousIncome': [2500,2500],\n",
    "    'IncomeBracketSmall': ['$0-$5,000', '$0-$5,000'],\n",
    "    'Review_education': ['high','high'],\n",
    "    'Review_educationBracket': ['High school','High school'],\n",
    "    'Review_political': [100, 100],\n",
    "    'Review_religious': [0, 0],\n",
    "    'ScenarioOrder': [3, 3],\n",
    "    'Intervention': [0, 1],\n",
    "    'PedPed': [0, 0],\n",
    "    'Barrier': [0, 1],\n",
    "    'CrossingSignal': [1, 0],\n",
    "    'AttributeLevel': ['Young', 'Old'],\n",
    "    'ScenarioTypeStrict': ['Age', 'Age'],\n",
    "    'ScenarioType': ['Age', 'Age'],\n",
    "    'DefaultChoice': ['Young', 'Young'],\n",
    "    'NonDefaultChoice': ['Old', 'Old'],\n",
    "    'DefaultChoiceIsOmission': [1, 1],\n",
    "    'NumberOfCharacters': [5, 5],\n",
    "    'DiffNumberOFCharacters': [0, 0],\n",
    "    'Saved': [0, 1],\n",
    "    'Label': ['Case 1','Case 1'],\n",
    "    'Template': ['Mobile', 'Mobile'],\n",
    "    'DescriptionShown': [0, 0],\n",
    "    'LeftHand': [1, 0],\n",
    "    'UserCountry3': ['USA', 'USA'],\n",
    "    'Man': [1, 1],\n",
    "    'Woman': [1, 1],\n",
    "    'Pregnant': [0, 0],\n",
    "    'Stroller': [0, 0],\n",
    "    'OldMan': [0, 2],\n",
    "    'OldWoman': [0, 1],\n",
    "    'Boy': [2, 0],\n",
    "    'Girl': [1, 0],\n",
    "    'Homeless': [0, 0],\n",
    "    'LargeWoman': [0, 0],\n",
    "    'LargeMan': [0, 0],\n",
    "    'Criminal': [0, 0],\n",
    "    'MaleExecutive': [0, 0],\n",
    "    'FemaleExecutive': [0, 0],\n",
    "    'FemaleAthlete': [0, 0],\n",
    "    'MaleAthlete': [0, 0],\n",
    "    'FemaleDoctor': [0, 0],\n",
    "    'MaleDoctor': [0, 0],\n",
    "    'Dog': [0, 0],\n",
    "    'Cat': [0, 0],\n",
    "}\n",
    "\n",
    "df4 = pd.DataFrame(data4)\n",
    "\n",
    "\n",
    "print(\"With persona:\\n\",generate_scenario(df4, include_persona=True),\"\\n\")\n",
    "print(\"No persona:\\n\", generate_scenario(df4, include_persona=False),\"\\n\")\n",
    "print(\"Label assigned by LLM: \",df4[\"Label\"].unique())\n",
    "print(\"Outcomes for these scenarios:\\n\",df4.apply(classify_response, column_name = \"Label\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts in small sample 5000 \n",
      "\n",
      "Number of prompts in full sample 581981 \n",
      "\n",
      "ExtendedSessionID          [-2147422537_8735659664189848.0, -2147421004_6...\n",
      "ResponseID                 [7YhXvwFC7xwp8PCLg, 9baC2PcmDRyCsqR3n, KuP7HL2...\n",
      "UserID                     [8735659664189850.0, 61324042610620.0, 4765991...\n",
      "Review_gender                                                   [Man, Woman]\n",
      "Review_age                 [23, 17, 16, 25, 57, 18, 19, 22, 34, 21, 56, 4...\n",
      "Review_ageBracket                 [15-24, 25-34, 55-64, 35-44, 45-54, 65-74]\n",
      "Review_income              [50000, 15000, 10000, under5000, 5000, 80000, ...\n",
      "Review_ContinuousIncome    [65000, 20000, 12500, 2500, 7500, 90000, 15000...\n",
      "IncomeBracketSmall         [$50,001-\\n$100,000, $5,001-\\n$25,000, $0-$5,0...\n",
      "Review_education           [bachelor, high, underHigh, graduate, college,...\n",
      "Review_educationBracket    [Some college, High school, Less than\\nhigh sc...\n",
      "Review_political           [100, 93, 83, 38, 0, 50, 19, 61, 86, 62, 77, 5...\n",
      "Review_religious           [0, 89, 62, 50, 65, 7, 49, 9, 83, 18, 100, 41,...\n",
      "ScenarioOrder                    [8, 9, 4, 7, 12, 6, 1, 11, 2, 5, 3, 13, 10]\n",
      "Intervention                                                          [0, 1]\n",
      "PedPed                                                                [0, 1]\n",
      "Barrier                                                               [0, 1]\n",
      "CrossingSignal                                                     [0, 1, 2]\n",
      "AttributeLevel             [Less, More, Fit, Fat, Hoomans, Pets, Young, O...\n",
      "ScenarioTypeStrict         [Utilitarian, Fitness, Species, Age, Random, G...\n",
      "ScenarioType               [Utilitarian, Fitness, Species, Age, Random, G...\n",
      "DefaultChoice                   [More, Fit, Hoomans, Young, nan, Male, High]\n",
      "NonDefaultChoice                    [Less, Fat, Pets, Old, nan, Female, Low]\n",
      "DefaultChoiceIsOmission                                      [0.0, 1.0, nan]\n",
      "NumberOfCharacters                                           [2, 5, 4, 1, 3]\n",
      "DiffNumberOFCharacters                                       [3, 1, 0, 2, 4]\n",
      "Saved                                                                 [0, 1]\n",
      "Template                                                   [Desktop, Mobile]\n",
      "DescriptionShown                                                      [0, 1]\n",
      "LeftHand                                                              [0, 1]\n",
      "UserCountry3                                                           [USA]\n",
      "Man                                                          [0, 1, 2, 3, 4]\n",
      "Woman                                                        [0, 1, 2, 3, 4]\n",
      "Pregnant                                                        [1, 0, 2, 3]\n",
      "Stroller                                                        [0, 1, 2, 3]\n",
      "OldMan                                                       [0, 3, 1, 2, 4]\n",
      "OldWoman                                                     [0, 1, 3, 2, 4]\n",
      "Boy                                                          [0, 1, 3, 2, 4]\n",
      "Girl                                                         [0, 1, 2, 3, 4]\n",
      "Homeless                                                  [0, 1, 2, 3, 4, 5]\n",
      "LargeWoman                                                   [0, 2, 1, 3, 4]\n",
      "LargeMan                                                     [0, 1, 2, 4, 3]\n",
      "Criminal                                                        [0, 1, 2, 3]\n",
      "MaleExecutive                                                   [0, 1, 2, 3]\n",
      "FemaleExecutive                                                 [0, 1, 2, 3]\n",
      "FemaleAthlete                                             [0, 1, 2, 4, 3, 5]\n",
      "MaleAthlete                                                  [0, 1, 2, 3, 4]\n",
      "FemaleDoctor                                                    [0, 1, 2, 3]\n",
      "MaleDoctor                                                      [0, 1, 2, 3]\n",
      "Dog                                                       [0, 1, 4, 2, 3, 5]\n",
      "Cat                                                       [1, 0, 3, 2, 4, 5]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# load survey data\n",
    "mms_all = pd.read_csv(\"../Data/2_SurveySample.csv.gz\")\n",
    "\n",
    "\n",
    "# David's API key for OpenAI\n",
    "#oai_api_key = \"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\"\n",
    "\n",
    "# lab\n",
    "#oai_api_key = \"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\"\n",
    "\n",
    "# Austin API key for OpenAI\n",
    "oai_api_key = \"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\"\n",
    "\n",
    "# API key for Anthropic\n",
    "ant_api_key = \"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\"\n",
    "\n",
    "# small sample\n",
    "random.seed(2024)\n",
    "mms = mms_all.head(10000)\n",
    "\n",
    "# number of prompts\n",
    "print(\"Number of prompts in small sample\", len(mms[\"ResponseID\"].unique()), \"\\n\")\n",
    "print(\"Number of prompts in full sample\", len(mms_all[\"ResponseID\"].unique()), \"\\n\")\n",
    "\n",
    "# structure of dataset\n",
    "print(pd.Series({c: mms[c].unique() for c in mms}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inc:  [ 65000  20000  12500   2500   7500  90000 150000  42500  30000]\n",
      "\n",
      "Edu: ['bachelor' 'high' 'underHigh' 'graduate' 'college' 'vocational']\n",
      "\n",
      "Gen: ['Man' 'Woman']\n",
      "\n",
      "Age: [23 17 16 25 57 18 19 22 34 21 56 44 30 47 29 33 40 43 31 20 26 73 28 42\n",
      " 35 36 24 46 50 37 45 27 51 39 32 67 59 38 41 49 48 64 60 52]\n",
      "\n",
      "Pol: [100  93  83  38   0  50  19  61  86  62  77  56  37  99  85  94  92  91\n",
      "  60  59  79  70  13  72  76  42  81  21  84  49  74  71  30  44  69  82\n",
      "  57  36  23  75  15  67  45  55   6  98  24  89  32  97  80  47  31  90\n",
      "  34  78   9  18  65  17  39  64  14  28  25  35  68  73  40  95  22  88\n",
      "   7  46  87  11  26  12  66  54  52  43]\n",
      "\n",
      "Rel: [  0  89  62  50  65   7  49   9  83  18 100  41   2  20  85  10  11  56\n",
      "  15   6  12  78  13  39  25   3  30  68  37  76  32  22  82  28  17   1\n",
      "  79  95  27  86  69  31  53  72  24  96  61  52  21  38  23   5  92  60\n",
      "  73  26  44  90  66  64  16  84   8  77  93  81  14  55  74  80  19  71\n",
      "  75  36  51  70  33  35  67  57  97  63]\n"
     ]
    }
   ],
   "source": [
    "# check that there are no NAs in demographics\n",
    "print(\"Inc: \",  mms[\"Review_ContinuousIncome\"].unique())\n",
    "print(\"\\nEdu:\", mms[\"Review_education\"].unique())\n",
    "print(\"\\nGen:\", mms[\"Review_gender\"].unique())\n",
    "print(\"\\nAge:\", mms[\"Review_age\"].unique())\n",
    "print(\"\\nPol:\", mms[\"Review_political\"].unique())\n",
    "print(\"\\nRel:\", mms[\"Review_religious\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-3.5-turbo-0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 22315\n",
      "Number of remaining prompts: 559666\n",
      "Prompt 1 out of 559666\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported model: gpt-3.5-turbo-0125",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 22,315 predictions with persona\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m prompt_llm(mms_all,model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo-0125\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m            csv_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../Data/4_gpt35turbo0125_wp_20240603.csv.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m            api_key\u001b[39m=\u001b[39;49moai_api_key, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m            include_persona\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     llm_response \u001b[39m=\u001b[39m reply\u001b[39m.\u001b[39mcontent[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported model: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# print LLM response if verbose\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X25sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(llm_response)\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported model: gpt-3.5-turbo-0125"
     ]
    }
   ],
   "source": [
    "# 22,315 predictions with persona\n",
    "prompt_llm(mms_all,model=\"gpt-3.5-turbo-0125\", \n",
    "           csv_path=\"../Data/4_gpt35turbo0125_wp_20240603.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 replicates with persona\n",
    "prompt_llm(mms,model=\"gpt-3.5-turbo-0125\", \n",
    "           csv_path=\"../Data/4_gpt35turbo0125_wp_20240605.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 replicate with persona\n",
    "prompt_llm(mms,model=\"gpt-3.5-turbo-0125\", \n",
    "           csv_path=\"../Data/4_gpt35turbo0125_wp_20240606.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 prediction without persona\n",
    "prompt_llm(mms,model=\"gpt-3.5-turbo-0125\", \n",
    "              csv_path=\"../Data/4_gpt35turbo0125_np_20240603.csv.gz\", \n",
    "              api_key=oai_api_key, \n",
    "              include_persona=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22,315 predictions with persona\n",
    "prompt_llm(mms_all,model=\"gpt-4o\", \n",
    "           csv_path=\"../Data/4_gpt4o_wp_20241116.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22,315 predictions with persona\n",
    "prompt_llm(mms_all,model=\"gpt-4o\", \n",
    "           csv_path=\"../Data/4_gpt4o_wp_20240603.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 replicates with persona\n",
    "prompt_llm(mms,model=\"gpt-4o\", \n",
    "           csv_path=\"../Data/4_gpt4o_wp_20240605.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 replicates with persona\n",
    "prompt_llm(mms,model=\"gpt-4o\", \n",
    "           csv_path=\"../Data/4_gpt4o_wp_20240606.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 predictions without persona\n",
    "prompt_llm(mms,model=\"gpt-4o\", \n",
    "           csv_path=\"../Data/4_gpt4o_np_20240603.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 23902\n",
      "Number of remaining prompts: 558079\n",
      "Prompt 1 out of 558079\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# predictions with persona\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m prompt_llm(mms_all,model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m            csv_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../Data/4_gpt4turbo_wp_20241118.csv.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m            api_key\u001b[39m=\u001b[39;49moai_api_key, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m            include_persona\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb Cell 31\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m,   \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m prompt[\u001b[39m1\u001b[39m]}]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m temperature \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     reply \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     ) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39melif\u001b[39;00m temperature \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     reply \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         messages\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X66sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     ) \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    591\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    592\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    593\u001b[0m             {\n\u001b[1;32m    594\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    595\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    596\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    597\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    598\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    599\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    600\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    601\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    615\u001b[0m             },\n\u001b[1;32m    616\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    617\u001b[0m         ),\n\u001b[1;32m    618\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    619\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    620\u001b[0m         ),\n\u001b[1;32m    621\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    622\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    623\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    624\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1006\u001b[0m         options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries,\n\u001b[1;32m   1009\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1006\u001b[0m         options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries,\n\u001b[1;32m   1009\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# predictions with persona\n",
    "prompt_llm(mms_all,model=\"gpt-4-turbo\", \n",
    "           csv_path=\"../Data/4_gpt4turbo_wp_20241118.csv.gz\", \n",
    "           api_key=oai_api_key,\n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseID    23902\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompted = pd.read_csv(\"../Data/4_gpt4turbo_wp_20241118.csv.gz\")\n",
    "prompted[\"gpt4turbo_wp_Label\"].unique()\n",
    "prompted[[\"ResponseID\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 5000\n",
      "Number of remaining prompts: 0\n",
      "No remaining responses.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gpt4turbo_wp_Label\n",
       "Case 2.    6664\n",
       "Case 1     2490\n",
       "Case 1.     846\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions with persona\n",
    "prompt_llm(mms,model=\"gpt-4-turbo\", \n",
    "           csv_path=\"../Data/4_gpt4turbo_t0wp_20241119.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True,\n",
    "           temperature=0.0)\n",
    "prompted = pd.read_csv(\"../Data/4_gpt4turbo_t0wp_20241119.csv.gz\")\n",
    "prompted[\"ResponseID\"].nunique()\n",
    "prompted[\"gpt4turbo_wp_Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22,315 predictions with persona\n",
    "prompt_llm(mms_all,model=\"gpt-4-turbo\", \n",
    "           csv_path=\"../Data/4_gpt4turbo_wp_20240603.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 replicates with persona\n",
    "prompt_llm(mms,model=\"gpt-4-turbo\", \n",
    "           csv_path=\"../Data/4_gpt4turbo_wp_20240605.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 replicates with persona\n",
    "prompt_llm(mms,model=\"gpt-4-turbo\", \n",
    "           csv_path=\"../Data/4_gpt4turbo_wp_20240606.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,000 predictions without persona\n",
    "prompt_llm(mms,model=\"gpt-4-turbo\", \n",
    "           csv_path=\"../Data/4_gpt4turbo_np_20240603.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## claude-3-5-sonnet-20241022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing responses: 7459\n",
      "Number of remaining prompts: 574522\n",
      "Prompt 1 out of 574522\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb Cell 39\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 22,315 predictions with persona\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m prompt_llm(mms_all,model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclaude-3-5-sonnet-20241022\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m            csv_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../Data/4_claude35sonnet20241022_wp_20241115.csv.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m            api_key\u001b[39m=\u001b[39;49mant_api_key, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m            include_persona\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/davidbroska/Library/Mobile Documents/com~apple~CloudDocs/Research Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb Cell 39\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39melif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclaude-3-5-sonnet-20241022\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     client \u001b[39m=\u001b[39m anthropic\u001b[39m.\u001b[39mAnthropic(api_key\u001b[39m=\u001b[39mapi_key)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     reply \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mmessages\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         system\u001b[39m=\u001b[39;49mprompt[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m,   \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt[\u001b[39m1\u001b[39;49m]}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m         ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     llm_response \u001b[39m=\u001b[39m reply\u001b[39m.\u001b[39mcontent[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/davidbroska/Library/Mobile%20Documents/com~apple~CloudDocs/Research%20Projects/IntegrativeExperimentsGAI/Code/4_PromptLLM.ipynb#X56sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anthropic/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anthropic/resources/messages.py:681\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    651\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    652\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m \u001b[39m600\u001b[39m,\n\u001b[1;32m    680\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Message \u001b[39m|\u001b[39m Stream[MessageStreamEvent]:\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    682\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/v1/messages\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    683\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    684\u001b[0m             {\n\u001b[1;32m    685\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    686\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    687\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    688\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    689\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop_sequences\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop_sequences,\n\u001b[1;32m    690\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    691\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m: system,\n\u001b[1;32m    692\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    693\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_k,\n\u001b[1;32m    694\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    695\u001b[0m             },\n\u001b[1;32m    696\u001b[0m             message_create_params\u001b[39m.\u001b[39;49mMessageCreateParams,\n\u001b[1;32m    697\u001b[0m         ),\n\u001b[1;32m    698\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    699\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    700\u001b[0m         ),\n\u001b[1;32m    701\u001b[0m         cast_to\u001b[39m=\u001b[39;49mMessage,\n\u001b[1;32m    702\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    703\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[MessageStreamEvent],\n\u001b[1;32m    704\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anthropic/_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1226\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1235\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1236\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1237\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1239\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anthropic/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anthropic/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    949\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mSending HTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl)\n\u001b[1;32m    951\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    953\u001b[0m         request,\n\u001b[1;32m    954\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    955\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    957\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    958\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered httpx.TimeoutException\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[39m=\u001b[39m pool_request\u001b[39m.\u001b[39mwait_for_connection(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[39m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[39m.\u001b[39;49mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[39m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[39m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    225\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1260\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1137\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1138\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 22,315 predictions with persona\n",
    "prompt_llm(mms_all,model=\"claude-3-5-sonnet-20241022\", \n",
    "           csv_path=\"../Data/4_claude35sonnet20241022_wp_20241115.csv.gz\", \n",
    "           api_key=ant_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22,315 predictions with persona\n",
    "prompt_llm(mms_all,model=\"o1-preview\", \n",
    "           csv_path=\"../Data/4_o1preview_wp_20241115.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22,315 predictions with persona\n",
    "prompt_llm(mms_all,model=\"o1-mini\", \n",
    "           csv_path=\"../Data/4_o1mini_wp_20241115.csv.gz\", \n",
    "           api_key=oai_api_key, \n",
    "           include_persona=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
