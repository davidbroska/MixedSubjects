\documentclass{article}
\input{preamble}

\title{\mytitle}
\date{\mydate}
\author{}



\begin{document}
\pagenumbering{gobble}
\maketitle


\doublespacing

\myabstract

\clearpage
\pagenumbering{arabic}
\setcounter{1}


\section{Introduction}

Large language models (LLMs)---neural networks with billions of parameters trained on massive amounts of text data---have been shown to mimic how humans respond to surveys and experimental treatments in various settings. Accurately predicting\endnote{We recognize that those deploying the methodology of ``silicon sampling'' prefer to describe LLMs used in this way as ``mimicing'' or ``modeling'' human behavior \citep{argyle_out_2023, horton2023large} instead of ``predicting'' it. We also acknowledge that there are nuanced differences between these uses for data (see, e.g., \citealt{breiman2001statistical}). However, exploring these complexities is beyond the scope of this article, and so we will stick with ``predict'' as our preferred term.} rather than observing human behavior could serve as a cost-effective and near-instantly available alternative to observing human behavior with the potential to transform the social sciences. This approach to learning about social phenomena, known as ``silicon sampling" \citep{argyle_out_2023}, might accelerate scientific progress, reduce inequities in access to costly evidence on hypotheses and research questions, and protect human subjects from deception and other risks associated with experimentation. 

However, there is scant guidance on how to leverage LLMs for conducting scientifically valid research. Researchers who currently use LLMs to predict human behavior have to rely implicitly or explicitly on what we term the \textit{interchangeability assumption}, a general premise that researchers adopt when drawing conclusions from predictive models of social processes rather than observations thereof (e.g. \citealp{friedman_methodology_1953}). When predicting human behavior, this assumption implies that the data extracted from LLMs closely correspond to human behavior or the responses given in a survey. The underlying logic of this approach is to treat silicon subjects \textit{as if} they were human participants. This assumption leads to valid inference only if the predicted responses approximate---at least on average---the same parameter estimate as from the human subjects.

Unfortunately, there is growing evidence that LLMs inaccurately portray human behavior (\citealp{Bisbee2024synthetic, park2024diminished, takemoto_moral_2024, abdurahman2024perils}). Even in settings where LLMs happen to accurately predict human behavior, there is a lack of generalizable procedures, metrics, and conventions to assess when this approximation is sufficiently accurate to be used in traditional null hypothesis testing. Currently, the interchangeability of predicted and observed behavior is assessed empirically on a case-by-case basis. As a result, silicon sampling is of minimal practical benefit since human subjects data must be collected alongside LLM predictions at a scale sufficient to validate the interchangeability assumption. Some have therefore suggested that predictions be confined to exploratory stages of research, such as LLM-powered pilot studies for anticipating effect sizes \citep{grossmann_ai_2023,ashokkumar_predicting_2024}.

To address this gap, we propose a \textit{mixed subjects} approach to designing research with LLMs. Rather than outright rejecting the assumption that human behavior and LLM predictions are interchangeable a priori, we argue that data from human subjects should inform inferences drawn from LLMs in a coherent statistical framework. We propose that observations from human subjects can be leveraged as a gold standard to correct for misrepresentations of human behavior through LLMs. We demonstrate how to implement this approach with prediction-powered inference (PPI) \citep{angelopoulos_prediction-powered_2023-1, angelopoulos2024ppi}, a recent statistical method that instantiates the mixed subjects approach. PPI allows researchers to combine observations of human behavior with predictions of that behavior generated by LLMs or other algorithms. So long as there is a correspondence between predicted and observed behavior, PPI produces valid point estimates with narrower confidence intervals than those derived solely from human subjects.

LLMs introduce a trade-off between predicted and observed behavior when estimating parameters such as causal effects. While obtaining predictions from LLMs is more cost-effective than recruiting human subjects, these predictions are less informative for estimating parameters than directly observed behavior. We derive a power analysis that formalizes and resolves this trade-off by balancing the costs of collecting these two types of data with the extent they inform inferences on parameters. Our power analysis allows researchers to allocate a fixed research budget to an optimal mix of human subjects and predictions that maximizes statistical power. Alternatively, researchers can minimize costs with an optimal combination of human subjects and predictions to achieve a given level of power. These functionalities will be integrated into the PPI Python library, available at \href{https://github.com/aangelopoulos/ppi_py}{https://github.com/aangelopoulos/ppi\_py}. With the extension of a power analysis, PPI becomes a fully usable methodology for conducting mixed subjects studies. 

This article demonstrates how researchers can use LLMs not only in exploratory but also in confirmatory research. The mixed subjects design with PPI ensures the validity of point estimates while allowing researchers to achieve higher precision at a lower cost than with human subjects alone. Therefore, the mixed subjects design may provide many of the benefits of silicon sampling while avoiding its drawbacks. 


\section{The Silicon Subjects Design}

Experiments in surveys, labs, and the field significantly enhanced our understanding of causal processes in the social sciences. However, experiments also face limitations, including the costs of conducting research, recruitment of harder-to-reach participants, and issues of measurement and generalizability. Below we outline how the silicon subjects approach promises to address these issues and highlight the potential pitfalls. 

\subsection{Promises of the Silicon Subjects Design}\label{sec:promises-silicon}

The silicon subjects design asserts that LLMs can mimic the behavior of human participants in empirical studies based on a prompt given by the researcher. In the context of experiments, the prompt contains the experimental manipulation, with silicon subjects being randomly assigned to a condition. The prompt may also include a profile of study participants with demographics, attitudes, and other information. While such a profile is necessarily an incomplete representation of a participant, it allows researchers to create a ``silicon sample" \citep{argyle_out_2023} that matches the demographic makeup of the population of interest. The additional context provided to the LLM may increase the diversity of responses one would expect in a human population or even increase the LLM's accuracy in predicting behavior \citep{gui2023challenge}. 
Based on successful replications of canonical experiments with the silicon subjects design, some scholars concluded that LLM predictions were interchangeable with human behavior under certain conditions:
\begin{displayquote}
\textit{These findings could indicate that—at least in some instances—GPT-3 is not just a stochastic parrot and could pass as a valid subject for some of the experiments we have administered.} (\citealp{binz2024using}: 9)
\end{displayquote}
\begin{displayquote}
\textit{Practically speaking, LLMs may be most useful as participants when studying specific topics, when using specific tasks, at specific research stages, and when simulating specific samples.} (\citealp{dillion2023can}: 597)
\end{displayquote}
If silicon subjects could substitute human participants, LLMs may help overcome the limitations of experiments that exclusively draw on responses from human subjects. The first set of issues relates to the cost of conducting experiments with human subjects. Depending on wages paid to survey participants and fees for using online survey panels, a single survey response can cost several dollars. Typical survey experiments in the social sciences require significant numbers of survey participants to identify an effect. For example, researchers need a sample size of $n=6,570$ to have a 90\% chance of detecting an effect of size $d=0.08$ with a two-sided $t$-test at $\alpha=0.05$ (Figure \ref{fig:gpower-power-analysis} in the Supporting Information). While $d=0.08$ represents the median effect size in a high-quality sample of online survey experiments in the social sciences \citep{rauf_audit_2024}, the required number of participants is even higher for smaller effects. Larger sample sizes are also required for experiments that systematically assess a broad range of hypotheses \citep{dellavigna_what_2018,milkman_megastudies_2021,voelkel_megastudy_2023, tappin_quantifying_2023} and those aimed at estimating interaction effects \citep{gelman_you_2018}. Across these cases, the costs of recruiting a sufficient number of human subjects may be prohibitive for researchers with more limited budgets. Silicon sampling offers a cost-effective alternative to human respondents. The cost of predicting a survey response with an LLM with currently available APIs can be as low as a fraction of a cent (Table \ref{tab:model-overview}). 

A second set of issues relates to challenges in finding suitable participants for a study. While researchers often go to significant lengths to create a sample representative of a target population, certain participants remain hard to reach on a typical panel for online research \citep{chandler_online_2019}. For instance, typical online panels for survey research consist of younger, more liberal, and more educated respondents who more are more likely to be White and who earn less on average than the American population \citep{berinsky_evaluating_2012, levay_demographic_2016, zack_can_2019}. Collecting samples that are representative across multiple dimensions---such as age, gender, income, and education---can be challenging since combinations of these characteristics may be rare among participants available on an online panel. If accurate, silicon sampling allows researchers to collect more observations on these otherwise hard-to-reach populations, with observations being nearly instantly available. Silicon subjects may even serve as alternative study populations if ethical concerns and risks limit the number of participants that can be recruited for experiments \citep{grossmann_ai_2023,bail_can_2024}.   

Finally, experimental research, like other quantitative scholarship, is only as good as the quality of measurements taken. Limited attention spans, insufficient effort, participant attrition, and non-compliance with research protocols are just a few examples of undesirable behaviors by study participants \citep{stantcheva_how_2023}. While these features characterize the \textit{typical} participant, silicon sampling envisions the \textit{ideal} participant---a prediction algorithm that exhibits human-like behavior but which allows researchers to control how much the responses randomly vary from one prompt to another, explicitly defining how erratic silicon subjects should behave. It may be unrealistic but advantageous to prompt LLMs to be inhumanely consistent in their responses and strictly abide by the researchers' directions \citep{grossmann_ai_2023}. Proponents of the silicon sampling approach could even argue that unrealistic distributions of LLM predictions help estimate parameters. While predictions of human responses exhibit less variability than human responses \citep{Bisbee2024synthetic,mei2024turing}, it may be precisely this misrepresentation that allows for more precise measurement of central tendencies such as the mean. This property does not imply that researchers obtain an accurate parameter estimate, but that this estimate exhibits less statistical uncertainty than an estimate obtained from a sample of human subjects.

\subsection{Perils of the Silicon Subjects Design}\label{sec:perils-silicon}

Empirical studies question whether silicon subjects alone will be sufficient to draw valid conclusions about human behavior. Predictions of human behavior have been shown to systematically diverge from observed behavior. For example, \cite{atari_xue_park_blasi_henrich_2023} find that LLMs respond to various tasks more like those from western, educated, industrialized democracies than those from other parts of the world. \cite{alvero2024large} find that LLMs, when compared to actual college applicants, write college admissions essays most similarly to those who are male and from neighborhoods with high socioeconomic status. When researchers are interested in populations or tasks that LLMs are less able to mimic, silicon sampling may lead them astray. 

Sources of prediction error are manifold and it remains unclear which ones can resolved. LLMs may inaccurately predict outcomes of certain groups of individuals because these have been misrepresented or underrepresented in the models' training data \citep{wang_large_2024,bail_can_2024}. While improving the representativeness and overall quality of the training data may enhance prediction accuracy, improving the inputs to LLMs may not be sufficient to rule out errors stemming from the prediction algorithm itself. LLMs have been shown to respond differently depending on the order of a question or give the same answer consistently \citep{park2024diminished}. Errors may also arise from the complexity of the research design. While LLMs achieved remarkable accuracy in predicted responses to social surveys \citep{kim_ai-augmented_2024}, predicting responses to experimental stimuli and more complex prediction tasks may result in higher error rates. At a more fundamental level, it remains unclear whether LLM predictions can be trusted without validation against human respondents. A treatment effect estimated based on LLM predictions could be a statistical fluke or an effect that would replicate in studies with human subjects  \citep{harding2023ai}.

Parameter estimates based on LLM predictions may not only be incorrect but also misleadingly precise. For example, using LLMs to generate many predicted values for an outcome $Y$ and regressing these on an independent variable $X$ results in narrow confidence intervals for a parameter simply because standard errors shrink with sample size. However, if LLMs inaccurately predict human behavior, the point estimates misrepresent the relationship between $Y$ and $X$ that would be observed in a sample of human subjects. Therefore, silicon samples may create a false sense of precision, leading to overly narrow confidence intervals with incorrect centers. This issue parallels the analysis of Big Data from non-representative samples, where researchers risk being ``precisely inaccurate" (\citealp{mcfarland_big_2015}). If biases are not adequately addressed, the availability of large and inexpensive data sources may do more harm than good \citep{meng_statistical_2018,bradley2021unrepresentative}. For example, silicon samples could further amplify doubts about the replicability of findings from experimental social science (c.f. \citealp{freese_emergence_2018}), not because studies lack sufficient statistical power to discern true effects from false positives, but because they are sufficiently powered to detect \textit{any} effect.


\section{The Mixed Subjects Design}

We propose the mixed subjects design, an alternative to silicon sampling and an umbrella term for statistical methods that provide valid inferences about human behavior while maintaining the benefits of employing silicon subjects. The mixed subjects approach treats silicon subjects as \textit{potentially} informative of human behavior, relying on the interchangeability assumption to an intermediate degree and in a way that is subject to disconfirmation via empirical evidence. Human respondents count as a gold standard to correct potentially flawed predictions from LLMs. The goal is to build confidence in LLMs as a research tool by combining human and silicon subjects with statistical methods that produce valid parameter estimates while maintaining the benefits of low costs of LLM predictions and increased statistical power to detect treatment effects. 
In the following, we present prediction-powered inference \citep{angelopoulos_prediction-powered_2023-1}, a recent statistical framework that instantiates the mixed subjects approach.
\clearpage

\begin{figure}[p]
    \centering
    \includegraphics[width=1\linewidth]{0_Infogram_v4.pdf}
    \caption{Comparison of experiments with human, silicon, and mixed subjects designs}
    \label{fig:concept}
\end{figure}

\clearpage

\subsection{The Mixed Subjects Design with PPI}\label{sec:PPI}

Prediction-powered inference (PPI) is a statistical method that combines a dataset of ``gold-standard" observations with predictions from a machine learning algorithm to estimate a broad class of estimands, including population means, regression coefficients, and quantiles  \citep{angelopoulos_prediction-powered_2023-1,angelopoulos2024ppi}. PPI does not make assumptions about the accuracy of the machine learning algorithm used to predict the dependent variable. Instead, the predictions are treated as informative but imperfect proxies. PPI uses the gold-standard observations to estimate prediction error and adjust parameter estimates accordingly. These corrected estimates target the same population parameters as a classical experiment (e.g. a regression coefficient estimated with a sample of responses from human subjects). Yet the PPI estimates are also more precise since increasing sample size with machine learning predictions leads to narrower confidence intervals. In the PPI framework, predictions and gold-standard observations thus complement each other in obtaining valid and precise point estimates.

To explain PPI in more detail we will follow the notation in \citet{angelopoulos_prediction-powered_2023-1,angelopoulos2024ppi}. To estimate a parameter $\theta$, PPI requires three things---a gold-standard (or labeled) dataset $\{(X_i, Y_i)\}_{i=1}^n$, an unlabeled dataset $\{\widetilde{X}_i\}_{i=1}^N$, and a machine learning algorithm $f$ that maps $X$ to a prediction of $Y$. PPI applies the machine learning algorithm to both datasets---this gives $\{(X_i,Y_i,f(X_i))\}_{i=1}^n$ and $\{(\widetilde{X}_i, f(\widetilde{X}_i))\}_{i=1}^N$.  
To understand how PPI estimates a parameter $\theta$ with these two datasets, it is instructive to consider how PPI estimates the simple population mean, e.g. the average student test score, loan amount, or time spent on social media (Equation \ref{eq:ppi-mean}, see also Figure \ref{fig:concept}). 


\begin{equation}\label{eq:ppi-mean}
    \hat{\theta}^\PPI = \underbrace{\frac{1}{N} \sum_{i=1}^N \hat \lambda f(\widetilde{X}_i)}_{\displaystyle \huge \hat \lambda \hat \theta_S} - \underbrace{\left( \frac{1}{n} \sum_{i=1}^n\hat \lambda f(X_i)- \frac{1}{n} \sum_{i=1}^n Y_i\right)}_{\displaystyle \huge \hat \Delta_{\hat \lambda} = \hat \lambda \hat  \theta_{f(X_i)}- \hat \theta_H} 
\end{equation}

The estimand $\hat{\theta}^\PPI$ comprises two parts: the estimand based on the algorithm's predictions $\hat \theta_S$ and a rectifier $\hat \Delta_{\hat \lambda}$. The rectifier quantifies the difference between the predicted and observed values from the gold-standard dataset and uses this information to adjust the estimate obtained from the prediction algorithm. If the algorithm is very accurate, the rectifier will be close to zero and the estimate is largely based on predictions. To optimize statistical precision, PPI estimates $\lambda$, an additional tuning parameter ranging from $0$ to $1$. $\hat \lambda \approx 1$ implies that full weight is given to the predictions whereas $\hat \lambda \approx 0$ means that the estimate is mostly based on the gold-standard observations.

In a mixed subjects experiment, the human subjects represent the gold standard dataset $\{(X_i, Y_i)\}_{i=1}^n$. The variable $X_i$ encodes the demographic covariates and the treatment assignment of the $i$th human subject. The variable $Y_i$ is the response from the $i$th human subject. For the unlabeled dataset $\{\widetilde{X}_i\}_{i=1}^N$ we create $N$ silicon subjects, for example by obtaining a representative sample of the target population \citep{argyle_out_2023}. For a silicon sample with covariates $\widetilde{X}$, we turn the information in $\widetilde{X}$ into a prompt for an LLM, resulting in a predicted survey response $f(\widetilde{X})$. Prompting the LLM for both the human and silicon subjects gives the datasets $\{(X_i, Y_i, f(X_i))\}_{i=1}^n$ and $\{(\widetilde{X}_i, f(\widetilde{X}_i))\}_{i=1}^N$. Researchers can then compute point estimates and confidence intervals with these two datasets using the software provided by \citet{angelopoulos_prediction-powered_2023-1, angelopoulos2024ppi}.

To apply PPI to a mixed subjects experiment, we need to verify several assumptions. First, PPI requires the classical assumption that $\{(X_i,Y_i)\}_{i=1}^n$ are independent and identically distributed (i.i.d). In addition, PPI requires that $\{\widetilde{X}_i\}_{i=1}^N$ are i.i.d. and that $\{ \widetilde{X}_i\}_{i=1}^N$ are drawn from the same distribution as $\{X_i\}_{i=1}^n$. That is, the hypothetical demographics of the silicon subject population must match the demographics of the human subject population.  Likewise, the treatment assignment mechanism must be the same for both groups. Ideally, each silicon subject should correspond to a human subject who would have been surveyed had the sample size $n$ been larger.
Second, PPI requires that the training of the machine learning algorithm $f$ is independent of both datasets. This assumption may be violated when the data from human subjects has been previously published and included in the LLM's training data. Finally, the procedure of prompting the LLM must be the same for the gold standard and the unlabeled dataset. This means that the same parameters and model should be used on both datasets. Likewise, the method used to turn the demographic and treatment information into a prompt must be the same for both datasets.

\subsection{The PPI Correlation}

In section \ref{sec:perils-silicon}, we argued that using classical inference methods---such as regression---to estimate parameters with a large number of predictions of human behavior gives a false sense of precision. Even if LLMs inaccurately portray human behavior, point estimates derived from their predictions exhibit too little uncertainty.
Therefore, a study design that treats LLM predictions not as interchangeable with human subjects must account for how closely LLMs can predict behavior. Therefore, we derived the \emph{PPI correlation} $\tilde{\rho} \in [-1,1]$ as an empirical measure of the interchangeability assumption. The PPI correlation $\widetilde{\rho}$ measures the correlation between the classical estimator $\hat{\theta}_{H}$ based on human subjects and the estimator $\hat{\theta}_S$ based on the LLM predictions.\endnote{Hence, the PPI correlation $\tilde{\rho}$ does not directly refer to the correlation between predicted and observed values of the dependent variable. The PPI correlation $\widetilde{\rho}$ is defined mathematically in equation~\eqref{eq:PPI_corr} in the Supporting Information.} To demonstrate that $\tilde{\rho}$ measures interchangeability, we derived an \textit{effective sample size}. The effective sample size in a mixed subjects experiment is the sample size required to achieve the same standard error for a parameter as in a human subjects experiment, allowing for a direct comparison of the two types of data. In section \ref{sec:effective-sample-size} of the Supporting Information, we show that the effective sample size when using PPI is given by
\begin{equation}\label{eq:n0}
    n_0 = n\cdot\frac{n+N}{n+N-N\tilde{\rho}^2},
\end{equation}
where $n$ and $N$ are the number of human and silicon subjects in the mixed subjects experiment and $n_0$ is the sample size in an equivalent human subjects experiment. 
When $\tilde{\rho}=1$, the effective sample size is $n+N$ and human and silicon subjects are treated as equally informative. When $\tilde{\rho}=0$, the effective sample size is $n$ and only human subjects are used. In empirical applications, $\tilde{\rho}$ will likely be between $0$ and $1$, indicating that the prediction algorithm is informative but not as informative as human subjects for estimating a parameter.
For example, if the PPI correlation is $\tilde{\rho}=0.75$ and $N/n=5$ so that for every human subject there are 5 silicon subjects, conducting a mixed subjects experiment PPI is equivalent to a human subjects experiment with 88\% more participants. As such, $\tilde{\rho}$ quantifies how informative predictions are in estimating $\theta$ and measures the extent to which predictions are interchangeable with gold standard data. 

In the following, we show that higher values of the PPI correlation are crucial for obtaining smaller standard errors for parameters, implying narrower confidence intervals, higher statistical power, and lower costs of conducting mixed subjects experiments relative to human subjects experiments. 
%$\tilde{\rho}$ does not directly refer to the correlation between observed and predictions but the quality of the predictions in estimating $\theta$. 
As shown in section \ref{sec:ppi-se-and-corr} of the Supporting Information, the PPI standard error can be written as 

\begin{equation}\label{eq:PPI-se}
    \mathrm{SE}(\hat{\theta}^{\PPI}) = \frac{\sigma}{\sqrt{n}}\sqrt{1 - \frac{N}{N+n}\tilde{\rho}^2},
\end{equation}

where $\sigma/\sqrt{n}$ is the standard error of $\hat{\theta}_H$ in an experiment with $n$ human subjects. As $\tilde{\rho}$ is always between $-1$ and $1$, the standard error of $\hat{\theta}^{\PPI}$ is always less than the classical standard error $\sigma/\sqrt{n}$. A smaller standard error for $\hat \theta^\PPI$ implies that a mixed subjects design with PPI produces narrower confidence intervals and higher statistical power than a human subjects experiment. Figure \ref{fig:theo-ppi-classic-ci} illustrates how achieving this higher statistical precision depends on $\tilde{\rho}$ and on the ratio $N/n$. The PPI standard error becomes narrower than the classical standard error for larger $N$ relative to $n,$ and this benefit is greatest for higher values of the PPI correlation. For example, if $N/n = 5$ and $\tilde{\rho} = 0.5$, then the PPI standard error will be approximately $11\%$ smaller than the classical standard error since $\sqrt{1-(N/(N+n))\tilde{\rho}^2} \approx 0.89$. If $\tilde{\rho}$ increased to $0.75$ and the same sample sizes were used, the PPI standard error would be approximately $27\%$ smaller than the classic standard error. The same consideration applies to confidence intervals since the ratio of standard errors is equivalent to the ratio of the width of confidence intervals. Finally, higher values of $\tilde{\rho}$, implying a smaller standard error, result in a non-linear increase in statistical power.

\clearpage

\begin{figure}[p]
    \centering
    \includegraphics[width=0.8\linewidth]{3_SeAsPercentageOfShareOfClassicSe.pdf}
    \caption{The x-axis shows the ratio $N/n$ of samples sizes with $N$ predictions $f(X_i)$ and $n$ gold standard observations $Y_i$. The y-axis shows the ratio of the PPI standard error to the classical standard error, defined by $\sqrt{1-(N/(N+n))\tilde{\rho}^2}.$ 
    }
    \label{fig:theo-ppi-classic-ci}
\end{figure}
\clearpage

The PPI standard error directly depends on $\tilde{\rho}$, the ratio of silicon subjects to the total sample size $N/(N+n)$, and the standard error of the classical estimator $\sigma/\sqrt{n}$. As shown in Figure \ref{fig:theo-ppi-classic-ci}, including more silicon subjects is most effective in reducing standard errors when the PPI correlation $\tilde{\rho}$ is closer to $1$. Hence, researchers can maximize returns on predictions by using algorithms that accurately portray human behavior. Researchers could increase $\tilde{\rho}$ by choosing more accurate prediction algorithms. These may include LLMs with more parameters, those trained on higher-quality data, models fine-tuned for specific prediction tasks, or models with retrieval-augmented generation \citep{lewis_retrieval-augmented_2021}. Prompt engineering---such as providing more context, examples, or specific instructions---may also enhance prediction accuracy. 

\subsection{PPI Power Analysis}\label{sec:PPI power}

Power analyses allow researchers to determine the necessary sample size for a desired level of power---i.e., the probability of correctly rejecting the null hypothesis when there is an effect \citep{cohen_statistical_1988}.  A power analysis not only addresses the practical question of how many resources researchers need to invest to find a significant effect but is also instrumental in advancing science. True treatment effects, particularly small ones, may go unnoticed if the sample size is too small. Reporting false negatives impedes researchers in discerning sound from flawed explanations for social phenomena, thwarts the accumulation of knowledge, and may explain the existence of inconsistent findings on core concepts in the social sciences \citep{thye_reliability_2000,stadtfeld_statistical_2020}. Power analyses are therefore crucial for testing and advancing theory. Yet no such method has been developed for PPI. To address this gap, we derive a power analysis, completing the toolkit necessary for conducting mixed subjects experiments.

Our power analysis for the mixed subjects design is based on the trade-off between human and silicon subjects. Obtaining a silicon sample is much more affordable than recruiting human subjects. However, silicon subjects are generally less informative than humans when estimating a parameter, corresponding to a PPI correlation of $\tilde{\rho} < 1$ when human and silicon subjects are not interchangeable. 
Researchers can make an optimal choice for combining a sample size of human subjects $n$ with $N$ silicon subjects, and this combination depends on the PPI correlation $\tilde{\rho}$, a hypothesized effect size, a desired level of power, the cost of recruiting human and silicon subjects, and the available research budget. Given these parameters, our power analysis allows researchers to optimally decide between recruiting \textit{costly but informative} human subjects or \textit{less informative but cheap} silicon subjects.
This multidimensional choice problem can be solved with constraint optimization \citep{apostol_calculus_1969}, allowing researchers to answer the following two questions.

First, which pair of sample sizes $(n, N)$ yields the highest power given a hypothesized effect size, fixed research budget, the cost of recruiting silicon subjects relative to human subjects, and how informative LLMs are in predicting human behavior $\tilde{\rho}$? Researchers may be particularly interested in finding the \textit{most powerful pair} $(n, N)$ if a limited research budget is the main constraint and resources should be allocated most effectively to maximize power. Figure \ref{fig:powerful-cheapest-pair}a illustrates this optimization problem: Finding the most powerful pair involves selecting combinations of $n$ and $N$ that satisfy the budget constraint and identifying the point where statistical power is highest.
Second, which combination of sample sizes $(n, N)$ is the cheapest to sample for a desired level of power, an effect size, the costs of silicon relative to human subjects, and the PPI correlation $\tilde{\rho}$? Researchers might be more interested in this question if budget constraints are less salient but resource allocation should still be as efficient as possible. Identifying the \textit{cheapest pair} means selecting the combination $(n, N)$ that gives a desired level of power and identifying the point where the cost is lowest (Figure \ref{fig:powerful-cheapest-pair}b). 

\clearpage
\begin{figure}[p]
    \includegraphics[width=1\linewidth]{9_MostPowerfulAndCheapestPair.pdf}
    \caption{Illustration of the constraint optimization with $\tilde{\rho}=0.75$, effect size $\delta = 0.2$, classical standard error $\hat \sigma/ \sqrt{n} = 1$, and a ratio of the costs of sampling silicon subjects to human subjects $\gamma=0.05$ (\textbf{a}) Given a fixed budget, the PPI power analysis identifies the combination of sample sizes $(n, N)$ with the highest statistical power. (\textbf{b}) Given a desired level of statistical power for detecting an effect, the PPI power analysis identifies the combination of sample sizes $(n, N)$ that minimizes budget expenditure.}
    \label{fig:powerful-cheapest-pair}
\end{figure}
\clearpage

Statistical software published alongside this article offers user-friendly tools for conducting power analyses in mixed subjects studies with PPI. 
This power analysis is \textit{data-driven} in that $\tilde{\rho}$ needs to be estimated from a small dataset $\{(X_i, Y_i, f(X_i))\}_{i=1}^n$. Researchers could also test hypothetical values for $\tilde{\rho},$ reflecting more or less accurate prediction algorithms. However, applying the PPI software is required to obtain precise estimates. Details on the derivation of the PPI power analysis are given in sections \ref{sec:most-powerful-pair} and \ref{sec:cheapest-pair} of the Supporting Information. 

\subsection{Lowering Costs of Data Collection}

PPI produces narrower confidence intervals and higher statistical power than classical inference. Whether PPI is also more cost-effective depends on the PPI correlation $\tilde{\rho}$ and $\gamma$ --- the ratio of the costs of surveying silicon and human subjects. That is,
\begin{equation}
    \gamma = \frac{c_f}{c_Y},
\end{equation}
where  $c_f$ is the cost of prompting an LLM to give a prediction, and $c_Y$ is the cost of surveying a human subject. In section \ref{sec:power} of the Supporting Information, we show that PPI is more cost-effective than classic inference with human subjects if and only if 
\begin{equation}\label{eq:cost-saving-condition}
\tilde{\rho}^2 > \frac{4 \gamma}{(1+\gamma)^2}.
\end{equation}

For example, we could assume a cost of \$3.20 for a participant to fill out a 12-minute survey, paying participants the 2024 California minimum wage of \$16.00/hour. Based on the costs of \$0.003 for prompting an LLM for this study, we calculated that any $\tilde{\rho}>0.06$ would be sufficient for PPI to save costs when compared to classical inference with human subjects only. More generally, if condition \eqref{eq:cost-saving-condition} is satisfied and researchers use the optimal sample size from the PPI power analysis, conducting mixed subjects can be substantially more cost-effective than experiments with human subjects only. Figure \ref{fig:perc-cost-reduction} shows that PPI experiments become less expensive than classical experiments as predictions become more affordable, with substantial savings at higher values of the PPI correlation. This theoretical result carries important implications as the field of generative AI continues to advance. The costs of conducting mixed subjects experiments will further decrease relative to classical experiments as costs for prompting LLMs decrease (e.g.  API fees) and LLMs become more capable of predicting human behavior. 

\clearpage

\begin{figure}[p]
    \centering
    \includegraphics[width=0.8\linewidth]{3_PercentCostOfHumanSubjectsExperiment.pdf}
    \caption{The y-axis shows the costs of conducting a mixed subjects experiments with PPI as a percentage of the costs of conducting a classical experiment with human subjects only, given by $1 - \tilde{\rho}^2(1 - \gamma) + 2 \sqrt{\gamma \tilde{\rho}^2(1-\tilde{\rho}^2)}$, as a function of the PPI correlation $\tilde{\rho}$ and the number of silicon subjects that researchers can afford for the costs of recruiting a human subject $1/\gamma = c_Y/c_f.$}
    \label{fig:perc-cost-reduction}
\end{figure}

\clearpage


\section{Application to the Moral Machine Experiment}

The Moral Machine experiment \citep{awad_moral_2018} sought to better understand the factors influencing people's decisions in moral dilemmas that self-driving cars might face on the road. In this conjoint experiment, participants were presented with hypothetical scenarios where a sudden brake failure would result in harm to either passengers or pedestrians. Participants could only spare one of the two groups. If participants choose to save the passengers, the autonomous vehicle would drive through a crosswalk where pedestrians are crossing the street. If participants chose to spare the pedestrians instead, the car would crash into a concrete barrier. The experiment measured how attributes such as age, gender, social status, and the number of individuals influenced the probability of participants choosing to save one group over the other. Using a weighted simple linear regression, \citet{awad_moral_2018} estimate the Average Marginal Component Effect (AMCE). The AMCE represents the causal effect of an attribute of a moral dilemma on a respondent's decision to spare passengers or pedestrians.
 
Our main interests in this application of PPI with LLMs are (\textbf{a}) to assess the extent to which including LLM predictions increases statistical precision and (\textbf{b}) to compare the validity of point estimates from PPI to those derived from LLM predictions. Analogous to Figure \ref{fig:theo-ppi-classic-ci}, we define increases in precision as the percent reduction in the width of the PPI confidence intervals relative to the width of the confidence interval obtained from human subjects only. We define the validity of a point estimate by the percent of confidence intervals that cover the true causal effect in a specific population. While such population parameters remain of course unknown, we use a quota sample of Americans who responded to the Moral Machine experiment to obtain best possible estimates of the true AMCEs (Figure \ref{fig:amce-estimates}). These AMCEs serve as a benchmark for comparing the validity of silicon sampling and the mixed subjects design with PPI. 

\subsection{Methods}

\citet{awad_moral_2018} obtained a convenience sample with millions of decisions on moral dilemmas from participants worldwide. We used the subset of 492,921 participants who completed an optional demographic survey to obtain a sample of the American population. Using quotas on age, education, gender, and income from the 2016 American Community Survey \citep{acs_2016}, we randomly sampled 2,097 Americans who evaluated a total of $22,315$ moral dilemmas.
Our sample closely resembles the demographics of the United States, except for older individuals, who could not be sufficiently sampled due to their minimal presence in the Moral Machine experiment (Figure \ref{fig:demographic-distribution}).

Next, we created the prompts for the LLMs based on the replication data from the Moral Machine experiment. The replication data records the attributes of the moral dilemmas evaluated by survey participants, such as the number of passengers in the car. We converted this numerical representation into a text description of the dilemmas with computer code adapted from a related study \citep{takemoto_moral_2024}. We also added a demographic profile to the prompt, including the age, education level, gender, and income of the survey respondent who evaluated the dilemma. Please refer to section \ref{sec:appendix-moral-machine} in the Supporting Information for an example. We then used the OpenAI API to prompt four LLMs---GPT4 Turbo (gpt-4-turbo), GPT4o (gpt-4o), and GPT3.5 Turbo (gpt-3.5-turbo-0125)---to predict the decisions of these survey respondents for the moral dilemmas.

We then compared the validity and statistical precision of point estimates derived from PPI against a naive approach that pools human and silicon subjects by incrementally increasing the number of silicon subjects. From the quota sample of 2,097 Americans, we randomly selected $n=500$ human subjects and added $n\times k =N$ silicon subjects for $k=(.25, .5, .75, 1, 1.5, \dots,4.5,5)$. For each combination of sample sizes $n$ and $N=(125,250,...,2500)$, we repeated the sampling 300 times and calculated the mean width and coverage of the confidence intervals. We estimated the AMCE for each scenario attribute with a weighted simple linear regression \citep{hainmueller_causal_2014}. To assess the validity and precision of the naive approach, we applied the weighted simple linear regression to the pooled sample of size $n+N$. We used the Python library created by \citet{angelopoulos_prediction-powered_2023-1, angelopoulos2024ppi} to obtain the corresponding PPI estimates of the AMCE.

\subsection{Results}

Prompting LLMs to predict 22,315 survey responses resulted in modest correlations, ranging from $r=0.36$ for GPT4 Turbo to $r=0.11$ for GPT3.5 Turbo. To explore potential ways of improving accuracy, we conducted two supplemental analyses (Table \ref{tab:corr-tab}). First, we prompted each LLM twice to give 5,000 additional predictions using the same prompts. We then created a composite by taking the mode of the three predictions. If anything, taking the modal prediction only minimally increases the correlation. Second, for a separate set of 5,000 predictions, we omit the demographic persona from the prompt. Excluding the persona minimally decreases the correlation, except for GPT3.5 Turbo where the correlation increases from $r=0.11$ to $r=0.17$. Overall, these supplemental analyses yielded very similar correlation coefficients within each LLM. For all subsequent analyses, we focus on the 22,315 predicted survey responses generated by GPT-4 Turbo.

\clearpage

\begin{figure}[p]
    \includegraphics[width=1\linewidth]{8_SimulationResults_gpt4turbo_wp_Saved_Nn_n500.pdf}
    \caption{The x-axes show the ratio of sample sizes $N/n$ with $N$ silicon subjects and $n$ gold standard observations. (\textbf{a}) The y-axis shows the width of the PPI confidence interval (CI) as a share of the CI from a regression on the pooled sample of size $n+N$. Values smaller than $100\%$ indicate the percent reduction in PPI CI width relative to the regression CI (cf. Figure \ref{fig:theo-ppi-classic-ci}). (\textbf{b}) The y-axis shows the percent of CIs calculated with PPI and regression that cover the AMCE estimates from the quota sample of 2,097 Americans; the estimates from this sample are used as the best available approximation of the AMCE parameters.}
    \label{fig:coverage-width}
\end{figure}

\clearpage

Figure \ref{fig:coverage-width} compares statistical precision (i.e., width of confidence intervals) and validity (i.e. percent of confidence intervals that cover the true parameter) in the mixed subjects approach with PPI versus the approach that naively pools human and silicon subjects.
Figure \ref{fig:coverage-width}a shows that adding an increasing number of LLM predictions reduces the width of confidence intervals more strongly for larger values of the PPI correlation $\tilde{\rho}$ (cf. Figure \ref{fig:theo-ppi-classic-ci}).
Figure \ref{fig:coverage-width}b shows that the percent of PPI confidence intervals that cover the parameter remains stable at high levels. In contrast, the coverage of the classical intervals computed for the naive approach remains stable only for some independent variables.
In sum, this analysis illustrates that PPI produces valid point estimates, with increases in statistical precision depending on the 
PPI correlation. The silicon sampling approach may also produce valid point estimates but this is impossible to ascertain without validation on human subjects. PPI automatically handles this validation by introducing a statistical correction to the point estimates derived from LLM predictions.

\section{Conclusion}

Large language models, and generative AI more generally, offer new data sources that may transform the social sciences. Predictions of human behavior---often called ``silicon subjects"---provide a cost-effective and near-instantly available alternative to observing behavior in human subjects studies. However, like novel data sources that have come before \citep{lazer2009css,lazer_computational_2020}, computational social scientists must critically assess the limitations of LLMs and develop robust methods to ensure sound conclusions from this emerging data source. We argue that researchers risk drawing incorrect conclusions when treating LLM predictions as interchangeable with observed human behavior. Estimating parameters based on large numbers of predictions can give a false sense of precision because the confidence intervals will be overly narrow, while the point estimates may systematically diverge from those estimated on a sample of human subjects. Even if LLMs become more accurate in predicting human behavior, these predictions remain of minimal benefit because researchers still need to validate the assumption of interchangeability with an appropriately large sample of human subjects.

We propose that LLM predictions be integrated with, rather than replace, human subjects in what we call a mixed subjects design. We demonstrate and extend prediction-powered inference (PPI), a statistical method that adjusts possibly invalid point estimates derived from LLM predictions to produce valid estimates. Mixed subjects studies with PPI also allow researchers to obtain narrower confidence intervals and higher statistical power than studies with human subjects only. Therefore, the mixed-subjects design with PPI allows researchers to combine the strengths of the human and silicon subjects approach.

Our statistical contributions to PPI are two-fold. First, we derive the PPI correlation $\tilde{\rho}$ as an empirical measure of the extent to which human subjects and LLM predictions are interchangeable. We show that high values of the PPI correlation produce small standard errors for parameters, implying narrower confidence intervals, higher statistical power, and lower costs of conducting mixed subjects experiments relative to human subjects experiments. If LLMs and other algorithms become more capable of predicting human behavior in the future, this improvement will be reflected in higher values for the PPI correlation. More capable algorithms will result in higher statistical precision of PPI estimates and the cost of conducting mixed subjects experiments will further decrease relative to human subjects experiments. 

Our second statistical contribution is a power analysis for PPI that addresses the trade-off between silicon and human subjects if they are not fully interchangeable (i.e., $\tilde{\rho} <1$). The PPI power analysis allows researchers to optimally choose between recruiting \textit{informative but costly} human subjects and \textit{less informative but cheap} silicon subjects. Researchers can allocate a given budget to maximize power or minimize budget expenditure to achieve a desired level of power.  
Statistical software published alongside this article completes the toolkit necessary to conduct mixed subjects studies with PPI.

Our work points to immediate next steps for mixed-subject experimental design. While we leverage PPI to implement these designs, we note that other methods are also well-suited for this purpose. For instance, researchers could combine samples of silicon and human subjects in a Bayesian regression framework (e.g. \citealp{jones2011bayesian}). Here, priors on the parameter values correspond to the degree to which researchers want to treat silicon subjects as interchangeable with human subjects. 
We also want to emphasize that the literature on doubly robust machine learning offers other promising routes for implementing mixed-subjects designs \citep{egami_using_2024-1,kallus_role_2024}. More generally, the development of a robust toolkit of mixed-subject methodologies will allow researchers to leverage LLMs and other forms of generative AI to pursue their research questions.

In a second future direction, the possibility of obtaining valid and precise estimates at low costs from a mixed-subjects design could be leveraged to conduct studies that would otherwise be prohibitively expensive.
For instance, identifying small treatment effects or interactions with sufficient statistical power requires thousands of observations, implying costs that may be too high when estimating these effects with human subjects alone. 
Moreover, studies aimed at systematically exploring a larger number of hypotheses and possible experimental designs have important practical and theoretical implications \citep{almaatooq2024beyond}, but often require an inordinate number of human subjects. The mixed subjects design could be integrated with existing research infrastructure to facilitate such large-scale experiments \citep{almaatouq_empirica_2021}. By reducing the cost of data collection, coupled with valid inferences about parameters, the mixed subjects design could increase scientific productivity and reduce inequality in access to otherwise costly data for research questions and hypotheses. The resulting savings could also be allocated to other research projects or used to pay higher wages to survey participants.

\section*{Data Availability}

A replication package is available at [URL blinded for review].

\newpage
\theendnotes

\newpage
\bibliographystyle{apa}
\bibliography{bibliography.bib}

\begin{appendix}

\include{SupportingInformation}
\end{appendix}

\end{document}